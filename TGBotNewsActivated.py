import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile, InputMediaPhoto, InputMediaVideo
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# ================== CONFIG ==================
BOT_TOKEN = '7885944156:AAHrh2o1UPzJ67jviCULfOmP_BGPExdh6l8'
GROQ_API_KEY = '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'
OPENROUTER_API_KEY = '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'
CHANNEL_ID = '@bulmyash'
TIMEZONE = "Europe/Moscow"

RSS_SOURCES = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "bbc_ru": "https://feeds.bbci.co.uk/russian/rss.xml",
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "rt": "https://www.rt.com/rss/",
    "lenta": "https://lenta.ru/rss",
    "meduza": "https://meduza.io/rss/all",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rosbalt": "https://www.rosbalt.ru/feed/",
}

KEYWORDS = [
    '—Å–∞–Ω–∫—Ü', '—Ç—Ä–∞–º–ø', '–ø—É—Ç–∏–Ω', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '–ª—É–∫–∞—à–µ–Ω–∫', 
    '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–¥—É–º–∞', '–º–∏–Ω–∏—Å—Ç', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–∫—É—Ä—Å', '—Ü–±', '–±–∞–Ω–∫',
    '–∏–Ω—Ñ–ª—è—Ü', '—Ä—ã–Ω–æ–∫', '–±–∏—Ä–∂–∞', '–∞–∫—Ü–∏–∏', '–∫—Ä–∏–ø—Ç', '–±–∏—Ç–∫–æ–∏–Ω',
    '—Å—à–∞', '–∫–∏—Ç–∞–π', '–µ—Å', '–µ–≤—Ä–æ—Å–æ—é–∑', '–Ω–∞—Ç–æ', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç',
    '–æ–ø–µ—Ä–∞—Ü', '–≤–æ–π—Å–∫', '–∞—Ä–º–∏—è', '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫',
    '–∞–≤–∞—Ä', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤', '–æ–±—Ä—É—à–µ–Ω', '–∫—Ä—É—à–µ–Ω',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '—Ä–∞–Ω–µ–Ω', '—Å–ø–∞—Å', '—ç–≤–∞–∫—É–∞—Ü', '–º—á—Å',
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'openai', 'google',
    'apple', 'microsoft', 'tesla', 'spacex', '–º–∞—Å–∫',
    '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '–æ–±—ã—Å–∫', '—Å–ª–µ–¥—Å—Ç–≤', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '—É—á–µ–Ω', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–∏–∑–æ–±—Ä–µ—Ç', '–∫–æ—Å–º–æ—Å',
    '—Ä–∞–∫–µ—Ç', '—Å–ø—É—Ç–Ω–∏–∫', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç'
]

BORING_KEYWORDS = ['–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', 
                   '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã', '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π', '—Å–æ–Ω–Ω–∏–∫']

FALLBACK_IMAGES = [
    "https://images.unsplash.com/photo-1504711434969-e33886168f5c",
    "https://images.unsplash.com/photo-1495020689067-958852a7765e",
    "https://images.unsplash.com/photo-1523995462485-3d171b5c8fa9"
]

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

try:
    c.execute("ALTER TABLE posted ADD COLUMN title TEXT")
    conn.commit()
except:
    pass

try:
    c.execute("ALTER TABLE posted ADD COLUMN url TEXT")
    conn.commit()
except:
    pass

c.execute('''CREATE TABLE IF NOT EXISTS posted (
    hash TEXT UNIQUE, 
    posted_at TEXT, 
    title TEXT,
    url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted (
    video_id TEXT UNIQUE, 
    posted_at TEXT, 
    type TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats (
    date TEXT UNIQUE, 
    normal_count INT
)''')
conn.commit()

def get_today_stats():
    today = datetime.now().date().isoformat()
    c.execute("SELECT normal_count FROM daily_stats WHERE date = ?", (today,))
    result = c.fetchone()
    return {"normal": result[0]} if result else {"normal": 0}

def increment_stat():
    today = datetime.now().date().isoformat()
    stats = get_today_stats()
    stats["normal"] += 1
    c.execute("INSERT OR REPLACE INTO daily_stats (date, normal_count) VALUES (?, ?)", 
              (today, stats["normal"]))
    conn.commit()

def is_duplicate(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("SELECT 1 FROM posted WHERE hash = ?", (h,))
    return c.fetchone() is not None

def save_posted(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("INSERT OR IGNORE INTO posted (hash, posted_at, title, url) VALUES (?, ?, ?, ?)", 
              (h, datetime.now().isoformat(), title, url))
    conn.commit()

def is_youtube_posted_today(video_id):
    today = datetime.now().date().isoformat()
    c.execute("SELECT 1 FROM youtube_posted WHERE video_id = ? AND DATE(posted_at) = ?", (video_id, today))
    return c.fetchone() is not None

def save_youtube_posted(video_id, video_type):
    c.execute("INSERT OR IGNORE INTO youtube_posted (video_id, posted_at, type) VALUES (?, ?, ?)", 
              (video_id, datetime.now().isoformat(), video_type))
    conn.commit()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI: –í–´–ë–û–† –¢–û–ü–û–í–û–ô –ù–û–í–û–°–¢–ò ==================
async def ai_select_and_summarize(news_list: list) -> dict:
    """
    –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ AI:
    1. –í—ã–±–∏—Ä–∞–µ—Ç –¢–û–ü-1 –Ω–æ–≤–æ—Å—Ç—å –∏–∑ —Å–ø–∏—Å–∫–∞
    2. –ü–∏—à–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑ –∏ —Ö–µ—à—Ç–µ–≥–∏
    """
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è AI
    news_text = "\n".join([f"{i+1}. {n['title']}" for i, n in enumerate(news_list[:25])])
    
    prompt = f"""–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞. –ò–∑ —Å–ø–∏—Å–∫–∞ –≤—ã–±–µ—Ä–∏ –û–î–ù–£ —Å–∞–º—É—é –≤–∞–∂–Ω—É—é/—à–æ–∫–∏—Ä—É—é—â—É—é/—Ç—Ä–µ–Ω–¥–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å.
–í–µ—Ä–Ω–∏ JSON:
{{
  "selected": –Ω–æ–º–µ—Ä –Ω–æ–≤–æ—Å—Ç–∏ (1-{len(news_list[:25])}),
  "summary": "–∫—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ—Å—Ç—Ä—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º",
  "hashtags": "#—Ç–µ–≥1 #—Ç–µ–≥2 #—Ç–µ–≥3"
}}

–ù–æ–≤–æ—Å—Ç–∏:
{news_text}"""
    
    # 1Ô∏è‚É£ GROQ
    try:
        async with aiohttp.ClientSession() as s:
            headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
            payload = {
                "model": "llama-3.1-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.8,
                "max_tokens": 300,
                "response_format": {"type": "json_object"}
            }
            async with s.post("https://api.groq.com/openai/v1/chat/completions", 
                            headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=20)) as r:
                if r.status == 200:
                    data = await r.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    content = re.sub(r'```json\s*|\s*```', '', content).strip()
                    result = json.loads(content)
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è
                    selected_idx = int(result.get("selected", 1)) - 1
                    if 0 <= selected_idx < len(news_list):
                        selected_news = news_list[selected_idx]
                        selected_news.update({
                            "summary": result.get("summary", ""),
                            "hashtags": result.get("hashtags", "")
                        })
                        log.info(f"‚úÖ AI: Groq –≤—ã–±—Ä–∞–ª #{selected_idx+1}")
                        return selected_news
                elif r.status == 429:
                    log.warning("‚ö†Ô∏è Groq rate limit")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è Groq failed: {e}")
    
    # 2Ô∏è‚É£ OPENROUTER
    try:
        async with aiohttp.ClientSession() as s:
            headers = {
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/news-bot",
                "X-Title": "News Bot"
            }
            payload = {
                "model": "meta-llama/llama-3.1-8b-instruct:free",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.8,
                "max_tokens": 400
            }
            async with s.post("https://openrouter.ai/api/v1/chat/completions",
                            headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=25)) as r:
                if r.status == 200:
                    data = await r.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    
                    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                    json_start = content.find('{')
                    json_end = content.rfind('}')
                    if json_start != -1 and json_end != -1:
                        content = content[json_start:json_end+1]
                    content = re.sub(r'```(?:json)?\s*|\s*```', '', content).strip()
                    
                    result = json.loads(content)
                    selected_idx = int(result.get("selected", 1)) - 1
                    
                    if 0 <= selected_idx < len(news_list):
                        selected_news = news_list[selected_idx]
                        selected_news.update({
                            "summary": result.get("summary", ""),
                            "hashtags": result.get("hashtags", "")
                        })
                        log.info(f"‚úÖ AI: OpenRouter –≤—ã–±—Ä–∞–ª #{selected_idx+1}")
                        return selected_news
                else:
                    log.error(f"OpenRouter HTTP {r.status}")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è OpenRouter failed: {e}")
    
    log.error("‚ùå –í—Å–µ AI –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    return None

# ================== –°–ë–û–† –ù–û–í–û–°–¢–ï–ô ==================
async def collect_fresh_news(limit=30):
    """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –±–µ–∑ AI-–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    candidates = []
    sources = list(RSS_SOURCES.items())
    random.shuffle(sources)
    
    for source_name, rss_url in sources:
        if len(candidates) >= limit:
            break
        
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                if len(candidates) >= limit:
                    break
                
                title = entry.title.strip()
                url = entry.link
                desc = entry.get("summary", "") or entry.get("description", "") or ""
                
                # –§–∏–ª—å—Ç—Ä—ã
                if len(title) < 20:
                    continue
                if is_duplicate(title, url):
                    continue
                if any(boring in title.lower() for boring in BORING_KEYWORDS):
                    continue
                if not any(k in title.lower() for k in KEYWORDS):
                    continue
                
                candidates.append({
                    "title": title,
                    "url": url,
                    "desc": desc,
                    "source": source_name,
                    "entry": entry
                })
        except Exception as e:
            log.error(f"RSS error {source_name}: {e}")
    
    return candidates

# ================== –ú–ï–î–ò–ê ==================
async def get_og_image(url: str):
    for attempt in range(2):
        try:
            connector = aiohttp.TCPConnector(ssl=False, force_close=True)
            async with aiohttp.ClientSession(connector=connector) as s:
                async with s.get(url, headers={"User-Agent": "Mozilla/5.0"}, 
                               timeout=aiohttp.ClientTimeout(total=5)) as r:
                    if r.status != 200:
                        return None
                    html = await r.text()
                    soup = BeautifulSoup(html, "html.parser")
                    og = soup.find("meta", property="og:image")
                    if og and og.get("content"):
                        img_url = og["content"]
                        if img_url.startswith("//"):
                            img_url = "https:" + img_url
                        return img_url
        except:
            pass
    return None

async def download_image(url: str):
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=8)) as r:
                if r.status == 200:
                    return await r.read()
    except:
        pass
    return None

# ================== –ü–û–°–¢–ò–ù–ì ==================
async def post_selected_news(news):
    """–ü–æ—Å—Ç–∏—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é AI –Ω–æ–≤–æ—Å—Ç—å"""
    title = news["title"]
    url = news["url"]
    summary = news.get("summary", "")
    hashtags = news.get("hashtags", "")
    source = news["source"]
    
    caption = f"**{title}**\n\n{summary}\n\n_{source}_\n\n{hashtags}"
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
    img_url = await get_og_image(url)
    if not img_url:
        img_url = random.choice(FALLBACK_IMAGES)
    
    img_data = await download_image(img_url)
    
    # –û—Ç–ø—Ä–∞–≤–∫–∞ —Å retry
    for attempt in range(3):
        try:
            if img_data:
                file = BufferedInputFile(img_data, filename="news.jpg")
                await bot.send_photo(CHANNEL_ID, file, caption=caption, parse_mode=ParseMode.MARKDOWN)
            else:
                await bot.send_message(CHANNEL_ID, caption, parse_mode=ParseMode.MARKDOWN)
            
            save_posted(title, url)
            increment_stat()
            log.info(f"‚úÖ –ü–æ—Å—Ç: {title[:50]}")
            return True
        except Exception as e:
            if attempt == 2:
                log.error(f"Post error: {e}")
                return False
            await asyncio.sleep(2)
    
    return False

# ================== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ==================
async def check_news():
    """1 –∑–∞–ø—Ä–æ—Å –∫ AI ‚Üí 1 –ø–æ—Å—Ç"""
    stats = get_today_stats()
    if stats["normal"] >= 25:
        log.info("üìä –õ–∏–º–∏—Ç 25 –ø–æ—Å—Ç–æ–≤ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç")
        return
    
    # –®–∞–≥ 1: –°–æ–±—Ä–∞—Ç—å 30 —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    log.info("üì• –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏...")
    candidates = await collect_fresh_news(30)
    
    if not candidates:
        log.info("‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return
    
    log.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    
    # –®–∞–≥ 2: AI –≤—ã–±–∏—Ä–∞–µ—Ç –¢–û–ü-1
    selected = await ai_select_and_summarize(candidates)
    
    if not selected:
        log.warning("‚ö†Ô∏è AI –Ω–µ —Å–º–æ–≥ –≤—ã–±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å")
        return
    
    # –®–∞–≥ 3: –ü–æ—Å—Ç–∏–º
    await post_selected_news(selected)

async def news_loop():
    """–ü–æ—Å—Ç–∏–Ω–≥ –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω"""
    log.info("‚è∞ –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 3 –º–∏–Ω...")
    await asyncio.sleep(3 * 60)
    
    while True:
        await check_news()
        next_interval = random.randint(20, 70)
        log.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ {next_interval} –º–∏–Ω")
        await asyncio.sleep(next_interval * 60)

# ================== YOUTUBE ==================
async def parse_youtube_trending():
    """–ü–∞—Ä—Å–∏—Ç —Ç–æ–ø-20 –∏–∑ YouTube Trending"""
    url = "https://www.youtube.com/feed/trending?gl=RU&hl=ru"
    
    try:
        log.info("üé¨ –ü–∞—Ä—Å–∏–Ω–≥ YouTube Trending...")
        
        async with aiohttp.ClientSession() as s:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
            }
            
            async with s.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as r:
                if r.status != 200:
                    log.error(f"‚ùå YouTube HTTP {r.status}")
                    return []
                
                html = await r.text()
                
                # –ò—â–µ–º ytInitialData
                match = re.search(r'var ytInitialData = ({.+?});', html, re.DOTALL)
                if not match:
                    log.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω ytInitialData –≤ HTML")
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±
                    match = re.search(r'window\["ytInitialData"\] = ({.+?});', html, re.DOTALL)
                    if not match:
                        log.error("‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–∞–∫–∂–µ –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è")
                        return []
                
                try:
                    data = json.loads(match.group(1))
                except json.JSONDecodeError as e:
                    log.error(f"‚ùå JSON parse error: {e}")
                    return []
                
                videos = []
                
                try:
                    # –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
                    tabs = data["contents"]["twoColumnBrowseResultsRenderer"]["tabs"]
                    
                    for tab in tabs:
                        if "tabRenderer" not in tab:
                            continue
                        
                        content = tab["tabRenderer"].get("content", {})
                        
                        # –ò—â–µ–º richGridRenderer –∏–ª–∏ sectionListRenderer
                        if "richGridRenderer" in content:
                            items = content["richGridRenderer"].get("contents", [])
                        elif "sectionListRenderer" in content:
                            sections = content["sectionListRenderer"].get("contents", [])
                            items = []
                            for section in sections:
                                if "itemSectionRenderer" in section:
                                    items.extend(section["itemSectionRenderer"].get("contents", []))
                        else:
                            continue
                        
                        for item in items:
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∫–ª–∞–º—É –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
                            if "richItemRenderer" not in item:
                                continue
                            
                            try:
                                video_data = item["richItemRenderer"]["content"]["videoRenderer"]
                                
                                video_id = video_data.get("videoId")
                                if not video_id:
                                    continue
                                
                                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                                title_data = video_data.get("title", {})
                                if "runs" in title_data:
                                    title = title_data["runs"][0].get("text", "")
                                else:
                                    title = title_data.get("simpleText", "")
                                
                                if not title:
                                    continue
                                
                                # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                                views_data = video_data.get("viewCountText", {})
                                views = views_data.get("simpleText", "0")
                                
                                # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                                length_data = video_data.get("lengthText", {})
                                length = length_data.get("simpleText", "")
                                
                                videos.append({
                                    "id": video_id,
                                    "title": title,
                                    "views": views,
                                    "length": length,
                                    "url": f"https://www.youtube.com/watch?v={video_id}"
                                })
                                
                            except KeyError as e:
                                log.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞—é item: {e}")
                                continue
                
                except KeyError as e:
                    log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
                    return []
                
                log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ")
                return videos[:20]
                
    except asyncio.TimeoutError:
        log.error("‚ùå YouTube timeout")
        return []
    except Exception as e:
        log.error(f"‚ùå YouTube error: {type(e).__name__}: {e}")
        return []


def is_short_video(length_str: str):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–∏–¥–µ–æ Shorts (<60 —Å–µ–∫)"""
    if not length_str:
        return False
    
    try:
        parts = length_str.split(":")
        
        if len(parts) == 1:
            # –§–æ—Ä–º–∞—Ç "45" (—Å–µ–∫—É–Ω–¥—ã)
            return int(parts[0]) < 60
        elif len(parts) == 2:
            # –§–æ—Ä–º–∞—Ç "0:45" (–º–∏–Ω—É—Ç—ã:—Å–µ–∫—É–Ω–¥—ã)
            m, s = map(int, parts)
            return m == 0 and s < 60
        elif len(parts) == 3:
            # –§–æ—Ä–º–∞—Ç "0:00:45" (—á–∞—Å—ã:–º–∏–Ω—É—Ç—ã:—Å–µ–∫—É–Ω–¥—ã)
            h, m, s = map(int, parts)
            return h == 0 and m == 0 and s < 60
    except ValueError:
        pass
    
    return False


async def post_youtube_tops():
    """–ü–æ—Å—Ç–∏—Ç —Ç–æ–ø –ø–æ–ª–Ω–æ–µ –≤–∏–¥–µ–æ + —Ç–æ–ø Shorts"""
    log.info("üé¨ –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ YouTube —Ç–æ–ø–æ–≤...")
    
    # –ü–∞—Ä—Å–∏–º trending
    videos = await parse_youtube_trending()
    
    if not videos:
        log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∏–¥–µ–æ –∏–∑ YouTube")
        return
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø–æ–ª–Ω—ã–µ –≤–∏–¥–µ–æ –∏ Shorts
    full_videos = [v for v in videos if not is_short_video(v.get("length", ""))]
    short_videos = [v for v in videos if is_short_video(v.get("length", ""))]
    
    log.info(f"üìä –ü–æ–ª–Ω—ã—Ö –≤–∏–¥–µ–æ: {len(full_videos)}, Shorts: {len(short_videos)}")
    
    # –ò—â–µ–º –Ω–µ–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ
    top_full = None
    for v in full_videos:
        if not is_youtube_posted_today(v["id"]):
            top_full = v
            break
    
    top_short = None
    for v in short_videos:
        if not is_youtube_posted_today(v["id"]):
            top_short = v
            break
    
    # –ü–æ—Å—Ç–∏–º –ø–æ–ª–Ω–æ–µ –≤–∏–¥–µ–æ
    if top_full:
        try:
            log.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è—é —Ç–æ–ø –≤–∏–¥–µ–æ: {top_full['title'][:50]}...")
            
            caption = (
                f"üî• **–°–∞–º–æ–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ –≤–∏–¥–µ–æ —Å–µ–≥–æ–¥–Ω—è –≤ –†–§**\n\n"
                f"{top_full['title']}\n\n"
                f"üëÄ {top_full['views']}\n\n"
                f"{top_full['url']}"
            )
            
            await bot.send_message(
                CHANNEL_ID, 
                caption, 
                parse_mode=ParseMode.MARKDOWN, 
                disable_web_page_preview=False
            )
            
            save_youtube_posted(top_full['id'], 'full')
            log.info("‚úÖ YouTube —Ç–æ–ø –≤–∏–¥–µ–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ")
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–æ–ø –≤–∏–¥–µ–æ: {e}")
    else:
        log.info("‚ÑπÔ∏è –¢–æ–ø –≤–∏–¥–µ–æ —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å–µ–≥–æ–¥–Ω—è")
    
    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
    await asyncio.sleep(5)
    
    # –ü–æ—Å—Ç–∏–º Shorts
    if top_short:
        try:
            log.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è—é —Ç–æ–ø Shorts: {top_short['title'][:50]}...")
            
            caption = (
                f"‚ö° **–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π Shorts —Å–µ–≥–æ–¥–Ω—è**\n\n"
                f"{top_short['title']}\n\n"
                f"üëÄ {top_short['views']}\n\n"
                f"{top_short['url']}"
            )
            
            await bot.send_message(
                CHANNEL_ID, 
                caption, 
                parse_mode=ParseMode.MARKDOWN, 
                disable_web_page_preview=False
            )
            
            save_youtube_posted(top_short['id'], 'shorts')
            log.info("‚úÖ YouTube Shorts –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω")
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ Shorts: {e}")
    else:
        log.info("‚ÑπÔ∏è –¢–æ–ø Shorts —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω —Å–µ–≥–æ–¥–Ω—è")
    
    log.info("üé¨ –ó–∞–¥–∞—á–∞ YouTube —Ç–æ–ø–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
async def main():
    scheduler = AsyncIOScheduler(timezone=TIMEZONE)
    scheduler.add_job(post_youtube_tops, "cron", hour=19, minute=0)
    scheduler.start()
    
    log.info("ü§ñ –ë–û–¢ –ó–ê–ü–£–©–ï–ù")
    log.info("üì∞ –ü–æ—Å—Ç—ã –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å)")
    log.info("üé¨ YouTube: 19:00 (—Ç–æ–ø –≤–∏–¥–µ–æ + shorts)")
    log.info("ü§ñ AI: 1 –∑–∞–ø—Ä–æ—Å = 1 –ø–æ—Å—Ç (–≤—ã–±–æ—Ä –∏–∑ 30 –Ω–æ–≤–æ—Å—Ç–µ–π)")
    log.info(f"üì° –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(RSS_SOURCES)}")
    
    await news_loop()

if __name__ == "__main__":
    asyncio.run(main())
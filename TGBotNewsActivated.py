import io
import os
import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
import sys
from dotenv import load_dotenv
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot, Dispatcher, Router
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pytz

load_dotenv()

# ================== CONFIG ==================
BOT_TOKEN = os.getenv("BOT_TOKEN")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
PEXELS_API_KEY = os.getenv("PEXELS_API_KEY")

TWITTER_API_KEY = os.getenv("TWITTER_API_KEY")
TWITTER_API_SECRET = os.getenv("TWITTER_API_SECRET")
TWITTER_ACCESS_TOKEN = os.getenv("TWITTER_ACCESS_TOKEN")
TWITTER_ACCESS_SECRET = os.getenv("TWITTER_ACCESS_SECRET")

CHANNEL_RU = '@bulmyash'
CHANNEL_EN = '@WORLD_ALERT_NEWS'

TIMEZONE = "Europe/Moscow"

if sys.platform == "win32":
    TEMP_DIR = "C:/temp/shorts"
else:
    TEMP_DIR = "/tmp/shorts"
os.makedirs(TEMP_DIR, exist_ok=True)

# ================== RSS –ò–°–¢–û–ß–ù–ò–ö–ò ==================
RSS_SOURCES_RU = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "lenta": "https://lenta.ru/rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rt_ru": "https://russian.rt.com/rss",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "rosbalt": "https://www.rosbalt.ru/feed/",
    "forbes_ru": "https://www.forbes.ru/newrss.xml",
    "cnews": "https://www.cnews.ru/inc/rss/news.xml",
    "habr": "https://habr.com/ru/rss/all/all/",
    "meduza": "https://meduza.io/rss/all",
}

RSS_SOURCES_EN = {
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "bbc": "https://feeds.bbci.co.uk/news/world/rss.xml",
    "cnn": "http://rss.cnn.com/rss/edition_world.rss",
    "ap": "https://rsshub.app/apnews/topics/world-news",
    "guardian": "https://www.theguardian.com/world/rss",
    "nyt": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "aljazeera": "https://www.aljazeera.com/xml/rss/all.xml",
    "france24": "https://www.france24.com/en/rss",
    "dw": "https://rss.dw.com/rdf/rss-en-all",
    "rt_en": "https://www.rt.com/rss/news/",
    "politico": "https://www.politico.com/rss/politicopicks.xml",
    "thehill": "https://thehill.com/feed/",
    "npr": "https://feeds.npr.org/1001/rss.xml",
    "abc": "https://abcnews.go.com/abcnews/internationalheadlines",
    "sky": "https://feeds.skynews.com/feeds/rss/world.xml",
}

# ================== BREAKING KEYWORDS (–ù–û–í–û–ï!) ==================
BREAKING_KEYWORDS_RU = [
    '—Å—Ä–æ—á–Ω–æ', '–º–æ–ª–Ω–∏—è', '—ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ', '–≤–∑—Ä—ã–≤', '—Ç–µ—Ä–∞–∫—Ç', '–≤–æ–π–Ω–∞',
    '—è–¥–µ—Ä–Ω', '—Ä–∞–∫–µ—Ç–Ω', '–≤—Ç–æ—Ä–∂–µ–Ω', '–ø–µ—Ä–µ–≤–æ—Ä–æ—Ç', '—É–±–∏—Ç', '–ø–æ–≥–∏–±',
    '–∫—Ä—É—à–µ–Ω–∏', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ', '–æ–±–≤–∞–ª', '–¥–µ—Ñ–æ–ª—Ç', '–∏–º–ø–∏—á–º–µ–Ω—Ç',
    '–æ–±—ä—è–≤–∏–ª –≤–æ–π–Ω—É', '–≤–≤—ë–ª –≤–æ–π—Å–∫–∞', '—á—Ä–µ–∑–≤—ã—á–∞–π–Ω', '—ç–≤–∞–∫—É–∞—Ü',
]

BREAKING_KEYWORDS_EN = [
    'breaking', 'urgent', 'explosion', 'terror', 'war declared',
    'nuclear', 'missile', 'invasion', 'coup', 'killed', 'dead',
    'crash', 'catastrophe', 'collapse', 'default', 'impeach',
    'troops deployed', 'emergency', 'evacuation', 'assassination',
    'martial law', 'airspace closed',
]

# ================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ==================
KEYWORDS_RU = [
    '–ø—É—Ç–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–∫—Ä–µ–º–ª', '–≥–æ—Å–¥—É–º', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',
    '–º–∏–Ω–∏—Å—Ç—Ä', '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '—Å—à–∞', '–∫–∏—Ç–∞–π',
    '–±–∏—Ç–∫–æ–∏–Ω', 'bitcoin', 'btc', '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç', '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä',
    '–µ–≤—Ä–æ', '–∫—É—Ä—Å –≤–∞–ª—é—Ç', '—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–∏–Ω—Ñ–ª—è—Ü',
    '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–∑–æ–ª–æ—Ç–æ', '–¥—Ä–∞–≥–º–µ—Ç–∞–ª–ª', 'brent', 'urals',
    '—Å–∞–Ω–∫—Ü', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–∞—Ä–º–∏—è', '–≤—Å—É',
    '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫', '–∞–≤–∞—Ä', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'google', 'apple',
    '—É—á–µ–Ω', '–∫–æ—Å–º–æ—Å', '–≤—ã–±–æ—Ä', '–∑–∞–∫–æ–Ω', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç',
    '—Å–∫–∞–Ω–¥–∞–ª', '–∫–æ—Ä—Ä—É–ø—Ü', '—Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω'
]

KEYWORDS_EN = [
    'putin', 'kremlin', 'russia', 'president', 'government',
    'trump', 'biden', 'zelensky', 'usa', 'china', 'nato',
    'bitcoin', 'btc', 'crypto', 'cryptocurrency', 'ethereum',
    'dollar', 'euro', 'pound', 'fed', 'federal reserve',
    'stock market', 'wall street', 'dow jones', 'nasdaq',
    'inflation', 'economy', 'recession', 'gdp',
    'oil', 'crude', 'brent', 'gas', 'gold', 'silver',
    'sanctions', 'war', 'conflict', 'military',
    'attack', 'strike', 'explosion', 'fire', 'crash',
    'killed', 'death', 'arrest', 'court', 'verdict',
    'ai', 'chatgpt', 'google', 'apple', 'tesla', 'musk',
    'science', 'space', 'election', 'law', 'breaking',
    'scandal', 'corruption', 'investigation'
]

# ================== –ß–Å–†–ù–´–ï –°–ü–ò–°–ö–ò ==================
BORING_KEYWORDS_RU = [
    '–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã',
    '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π', '—Å–æ–Ω–Ω–∏–∫', '–ø—Ä–∏–º–µ—Ç—ã', '–∏–º–µ–Ω–∏–Ω—ã',
    '—Å—Ç–∞–∂–∏—Ä–æ–≤–∫', '–æ–±–µ—Å–ø–µ—á–∏—Ç—å', '–ø–æ—Ä—É—á–∏–ª', '—Å–æ–≤–µ—â–∞–Ω–∏', '–∑–∞—Å–µ–¥–∞–Ω–∏',
    '–≤—Ä—É—á–∏–ª', '–Ω–∞–≥—Ä–∞–¥–∏–ª', '–ø–æ–∑–¥—Ä–∞–≤–∏–ª', '–≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è',
    '—Ç—É–±–µ—Ä–∫—É–ª–µ–∑', '–≥—Ä–∏–ø–ø', '–æ—Ä–≤–∏', '–ø—Ä–æ—Å—Ç—É–¥', '–≤–∞–∫—Ü–∏–Ω–∞—Ü',
    '–ø—Ä–∏–≤–∏–≤–∫', '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫', '–±–æ–ª—å–Ω–∏—Ü',
    '—à–∫–æ–ª—å–Ω–∏–∫', '—É—á–µ–Ω–∏–∫', '—É—á–∏—Ç–µ–ª', '—É—Ä–æ–∫', '–¥–æ–º–∞—à–Ω',
    '—ç–∫–∑–∞–º–µ–Ω', '–µ–≥—ç', '–æ–ª–∏–º–ø–∏–∞–¥',
    '–≤—ã—Å—Ç–∞–≤–∫', '–∫–æ–Ω—Ü–µ—Ä—Ç', '—Ñ–µ—Å—Ç–∏–≤–∞–ª', '–ø—Ä–µ–º—å–µ—Ä', '—Å–ø–µ–∫—Ç–∞–∫–ª',
    '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '—Ç—É—Ä–Ω–∏—Ä', '–º–∞—Ç—á', '–∏–≥—Ä–∞', '—Ç—Ä–µ–Ω–µ—Ä',
]

BORING_KEYWORDS_EN = [
    'weather', 'forecast', 'temperature', 'rain', 'snow',
    'horoscope', 'zodiac', 'lottery', 'astrology',
    'meeting', 'conference', 'seminar', 'workshop',
    'awarded', 'honored', 'congratulated',
    'flu', 'cold', 'vaccine', 'vaccination', 'clinic',
    'kardashian', 'royal family', 'celebrity baby',
    'engagement', 'wedding', 'divorce',
    'recipe', 'cooking tips', 'lifestyle',
]

BLACKLIST_CHANNELS = [
    'kids', 'children', 'cartoon', 'animation', 'nursery',
    'minecraft', '–º–∞–π–Ω–∫—Ä–∞—Ñ—Ç', 'roblox', 'fortnite', 'gaming', '–≥–µ–π–º–µ—Ä',
    'asmr', '–∞—Å–º—Ä', 'mukbang', '–º—É–∫–±–∞–Ω–≥', 'prank', '–ø—Ä–∞–Ω–∫',
    'tiktok compilation', 'shorts compilation',
    # –ò–ù–î–ò–Ø ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    'rankers gurukul', 'study iq', 'dhruv rathee', 'technical guruji',
    'total gaming', 'carryminati', 'bb ki vines', 'ashish chanchlani',
    'round2hell', 'harsh beniwal', 'elvish yadav', 'physics wallah',
    'unacademy', 'byju', 'vedantu', 'khan sir', 'alakh pandey',
    'sandeep maheshwari', 'vivek bindra', 'beer biceps',
    'amit bhadana', 'triggered insaan', 'lakshay chaudhary',
    'flying beast', 'sourav joshi', 'manoj dey', 'techno gamerz',
    'gyan therapy', 'facts mine', 'top 10 hindi', 'abhi and niyu',
]

BLACKLIST_TOPICS = [
    '–º–∞–π–Ω–∫—Ä–∞—Ñ—Ç', 'minecraft', 'roblox', 'fortnite',
    'asmr', '–∞—Å–º—Ä', '–º—É–∫–±–∞–Ω–≥', 'mukbang',
    '–¥–µ—Ç—Å–∫–∏–π', 'kids', 'children', 'cartoon',
    '–ø—Ä–∞–Ω–∫', 'prank', '—á–µ–ª–ª–µ–Ω–¥–∂', 'challenge',
    '–≥–∞–¥–∞–Ω–∏–µ', 'tarot', '–∞—Å—Ç—Ä–æ–ª–æ–≥',
]

# ================== YOUTUBE –ö–ê–ù–ê–õ–´ ==================
RU_NEWS_CHANNELS = [
    "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏", "–¢–ê–°–°", "–ò–∑–≤–µ—Å—Ç–∏—è", "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å", "–†–ë–ö",
    "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä", "–í–µ–¥–æ–º–æ—Å—Ç–∏", "–ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª", "–†–æ—Å—Å–∏—è 24",
    "–ù–¢–í", "RT", "–î–ï–ù–¨ –¢–í", "–ö—Ä–µ–º–ª—å",
    "–î–æ–∂–¥—å", "–ú–µ–¥—É–∑–∞", "–ù–æ–≤–∞—è –≥–∞–∑–µ—Ç–∞",
    "–≤–î—É–¥—å", "–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞", "–§–ï–ô–ì–ò–ù LIVE",
    "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ", "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ Shorts",
    "–†–µ–¥–∞–∫—Ü–∏—è", "Varlamov", "Varlamov News",
    "Soloviev LIVE", "–°–æ–ª–æ–≤—å—ë–≤ LIVE", "60 –º–∏–Ω—É—Ç",
    "–¶–∞—Ä—å–≥—Ä–∞–¥ –¢–í", "–°–ø—É—Ç–Ω–∏–∫", "Life", "–õ–∞–π—Ñ",
    "Mash", "Shot", "112", "Baza", "–ë–∞–∑–∞",
    "Readovka", "WarGonzo", "Rybar", "–†—ã–±–∞—Ä—å",
    "BRIEF", "–ù–µ–∑—ã–≥–∞—Ä—å", "–ü–æ–¥—ä—ë–º", "–ù–æ–≤–æ—Å—Ç–∏",
    "–ü–æ–ª–∏—Ç–∏–∫–∞ —Å–µ–≥–æ–¥–Ω—è", "–†–æ—Å—Å–∏—è 1", "–û–¢–†",
    "–≠—Ö–æ", "The Insider", "–í–∞–∂–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏",
]

EN_NEWS_CHANNELS = [
    "BBC News", "CNN", "Reuters", "Al Jazeera English",
    "Sky News", "NBC News", "ABC News", "CBS News",
    "Fox News", "MSNBC", "Bloomberg", "CNBC",
    "The Guardian", "The New York Times", "Washington Post",
    "AP Archive", "AFP News Agency", "DW News",
    "France 24 English", "Euronews", "WION",
    "Times Radio", "Channel 4 News", "ITV News",
    "Global News", "CTV News", "PBS NewsHour",
    "Vice News", "Vox", "The Economist",
]

RU_ENTERTAINMENT_CHANNELS = [
    "–ß–ë–î", "Labelcom", "Stand-Up Club #1", "Roast Battle",
    "–ò–º–ø—Ä–æ–≤–∏–∑–∞—Ü–∏—è", "–ì–¥–µ –ª–æ–≥–∏–∫–∞", "–ß—Ç–æ –±—ã–ª–æ –¥–∞–ª—å—à–µ",
    "SciOne", "–ù–∞—É—á–ø–æ–∫", "–ê—Ä–∑–∞–º–∞—Å", "–ü—Ä–∞–≤–¥–∞ –ì–ª–∞–∑–∞ –ö–æ–ª–µ—Ç",
    "–¢–æ–ø–ª–µ—Å", "Utopia Show", "Droider", "Wylsacom",
    "AdMe", "5-Minute Crafts LIKE",
]

EN_ENTERTAINMENT_CHANNELS = [
    "Veritasium", "Vsauce", "Kurzgesagt", "SmarterEveryDay",
    "Mark Rober", "Tom Scott", "CGP Grey",
    "MKBHD", "Linus Tech Tips", "JerryRigEverything",
    "Johnny Harris", "Wendover Productions", "RealLifeLore",
    "Half as Interesting", "PolyMatter",
]

RU_COMMENTARY_CHANNELS = [
    "–î–º–∏—Ç—Ä–∏–π –ì–æ—Ä–¥–æ–Ω", "–ù–µ–≤–∑–æ—Ä–æ–≤", "–ö–∞—Ü", "–®—É–ª—å–º–∞–Ω",
    "–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞", "–ê –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å",
    "KamikadzeDead", "ThisIs–•–æ—Ä–æ—à–æ", "–ë–æ—Ä–æ–¥–∞—á",
    "–ñ–∏–≤–æ–π –ì–≤–æ–∑–¥—å", "Cynicmansion",
    "–§–ï–ô–ì–ò–ù LIVE", "–ù–∞–≤–∞–ª—å–Ω—ã–π LIVE",
]

EN_COMMENTARY_CHANNELS = [
    "Shawn Ryan Show", "Joe Rogan Experience",
    "Ben Shapiro", "Tucker Carlson",
    "The Daily Show", "Last Week Tonight",
    "Late Night with Seth Meyers",
    "Breaking Points", "Russell Brand",
    "Tim Pool", "Jordan Peterson",
    "The Young Turks", "TYT",
    "The Jimmy Dore Show",
]

# ================== –ö–ê–¢–ï–ì–û–†–ò–ò –ö–û–ù–¢–ï–ù–¢–ê ==================
CONTENT_CATEGORIES = {
    "news": {
        "weight": 35,
        "queries_ru": [
            "–Ω–æ–≤–æ—Å—Ç–∏ —Ä–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è", "–ø—É—Ç–∏–Ω –∑–∞—è–≤–∏–ª", "—Ç—Ä–∞–º–ø –Ω–æ–≤–æ—Å—Ç–∏",
            "–º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏", "—Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏", "–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å",
        ],
        "queries_en": [
            "breaking news today", "world news", "trump news",
            "biden news", "russia news", "china news",
        ]
    },
    "politics": {
        "weight": 15,
        "queries_ru": [
            "–ø–æ–ª–∏—Ç–∏–∫–∞ —Ä–æ—Å—Å–∏—è", "–∫—Ä–µ–º–ª—å –Ω–æ–≤–æ—Å—Ç–∏", "–≥–æ—Å–¥—É–º–∞",
            "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è", "—Å–∞–Ω–∫—Ü–∏–∏",
        ],
        "queries_en": [
            "politics news", "white house", "congress",
            "european union", "nato news",
        ]
    },
    "economy": {
        "weight": 10,
        "queries_ru": [
            "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "—ç–∫–æ–Ω–æ–º–∏–∫–∞ —Ä–æ—Å—Å–∏–∏", "—Ä—É–±–ª—å —Å–µ–≥–æ–¥–Ω—è",
            "–Ω–µ—Ñ—Ç—å –≥–∞–∑", "–±–∏—Ä–∂–∞",
        ],
        "queries_en": [
            "stock market", "economy news", "bitcoin",
            "inflation", "fed rates",
        ]
    },
    "science_tech": {
        "weight": 15,
        "queries_ru": [
            "–Ω–∞—É–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏", "–∫–æ—Å–º–æ—Å",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        ],
        "queries_en": [
            "science news", "tech news", "ai news",
            "space news", "innovation",
        ]
    },
    "entertainment": {
        "weight": 10,
        "queries_ru": [
            "–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏", "—Ç–æ–ø —Ñ–∞–∫—Ç–æ–≤",
            "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ —Ä—è–¥–æ–º", "–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ–µ",
        ],
        "queries_en": [
            "amazing facts", "interesting facts", "mind blowing",
            "did you know", "incredible stories",
        ]
    },
    "commentary": {
        "weight": 15,
        "queries_ru": [
            "–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —é–º–æ—Ä", "—Å–∞—Ç–∏—Ä–∞ –Ω–æ–≤–æ—Å—Ç–∏", "—Å–º–µ—à–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
            "–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–µ–º—ã", "—Ä–∞–∑–±–æ—Ä –ø–æ–ª–∏—Ç–∏–∫–∏", "–º–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞",
        ],
        "queries_en": [
            "political satire", "news commentary", "political memes",
            "political humor", "expert opinion", "political reaction",
        ]
    },
}

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

# –¢–∞–±–ª–∏—Ü—ã –¥–ª—è RU
c.execute('''CREATE TABLE IF NOT EXISTS posted_ru (
    hash TEXT UNIQUE, posted_at TEXT, title TEXT, url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted_ru (
    video_id TEXT UNIQUE, posted_at TEXT, type TEXT, category TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats_ru (
    date TEXT UNIQUE, news_count INT DEFAULT 0, shorts_count INT DEFAULT 0
)''')

# –¢–∞–±–ª–∏—Ü—ã –¥–ª—è EN
c.execute('''CREATE TABLE IF NOT EXISTS posted_en (
    hash TEXT UNIQUE, posted_at TEXT, title TEXT, url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted_en (
    video_id TEXT UNIQUE, posted_at TEXT, type TEXT, category TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats_en (
    date TEXT UNIQUE, news_count INT DEFAULT 0, shorts_count INT DEFAULT 0
)''')

# –û–±—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
c.execute('''CREATE TABLE IF NOT EXISTS youtube_channels_used (
    channel_name TEXT, used_at TEXT, lang TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS used_images (
    url TEXT, used_at TEXT
)''')

# –ê–ù–ê–õ–ò–¢–ò–ö–ê
c.execute('''CREATE TABLE IF NOT EXISTS analytics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT, lang TEXT, content_type TEXT, category TEXT,
    title TEXT, channel TEXT, views INT, likes INT, success BOOLEAN
)''')

# –ù–û–í–û–ï! –¢–∞–±–ª–∏—Ü–∞ –≥–æ—Ä—è—á–∏—Ö —Ç–µ–º –¥–ª—è —Ç—Ä–µ–¥–æ–≤
c.execute('''CREATE TABLE IF NOT EXISTS hot_topics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    topic_hash TEXT,
    keywords TEXT,
    first_message_id INT,
    channel_id TEXT,
    lang TEXT,
    created_at TEXT,
    last_update TEXT,
    update_count INT DEFAULT 1
)''')

# –ù–û–í–û–ï! –¢–∞–±–ª–∏—Ü–∞ breaking-—Å–æ–±—ã—Ç–∏–π (–∞–Ω—Ç–∏—Ñ–ª—É–¥)
c.execute('''CREATE TABLE IF NOT EXISTS breaking_events (
    hash TEXT UNIQUE,
    title TEXT,
    lang TEXT,
    posted_at TEXT
)''')

conn.commit()


def migrate_database():
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å—Ç–∞—Ä—É—é –±–∞–∑—É"""
    try:
        c.execute("SELECT lang FROM youtube_channels_used LIMIT 1")
    except sqlite3.OperationalError:
        log.info("üîß –ú–∏–≥—Ä–∞—Ü–∏—è –±–∞–∑—ã: –¥–æ–±–∞–≤–ª—è—é –∫–æ–ª–æ–Ω–∫—É 'lang'...")
        c.execute("ALTER TABLE youtube_channels_used ADD COLUMN lang TEXT DEFAULT 'ru'")
        c.execute("UPDATE youtube_channels_used SET lang = 'ru' WHERE lang IS NULL")
        conn.commit()
        log.info("‚úÖ –ë–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

    c.execute("SELECT COUNT(*) FROM youtube_channels_used WHERE lang IS NULL")
    null_count = c.fetchone()[0]
    if null_count > 0:
        c.execute("UPDATE youtube_channels_used SET lang = 'ru' WHERE lang IS NULL")
        conn.commit()


# ================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ==================
def get_today_stats(lang: str):
    today = datetime.now().date().isoformat()
    table = f"daily_stats_{lang}"
    c.execute(f"SELECT news_count, shorts_count FROM {table} WHERE date = ?", (today,))
    result = c.fetchone()
    return {"news": result[0], "shorts": result[1]} if result else {"news": 0, "shorts": 0}


def increment_stat(lang: str, stat_type: str):
    today = datetime.now().date().isoformat()
    table = f"daily_stats_{lang}"
    stats = get_today_stats(lang)
    if stat_type == "news":
        stats["news"] += 1
    else:
        stats["shorts"] += 1
    c.execute(f"INSERT OR REPLACE INTO {table} (date, news_count, shorts_count) VALUES (?, ?, ?)",
              (today, stats["news"], stats["shorts"]))
    conn.commit()


def is_duplicate(title: str, url: str, lang: str):
    h = hashlib.md5((title + url).encode()).hexdigest()
    table = f"posted_{lang}"
    c.execute(f"SELECT 1 FROM {table} WHERE hash = ?", (h,))
    return c.fetchone() is not None


def save_posted(title: str, url: str, lang: str):
    h = hashlib.md5((title + url).encode()).hexdigest()
    table = f"posted_{lang}"
    c.execute(f"INSERT OR IGNORE INTO {table} (hash, posted_at, title, url) VALUES (?, ?, ?, ?)",
              (h, datetime.now().isoformat(), title, url))
    conn.commit()


def is_youtube_posted(video_id: str, lang: str):
    table = f"youtube_posted_{lang}"
    c.execute(f"SELECT 1 FROM {table} WHERE video_id = ?", (video_id,))
    return c.fetchone() is not None


def save_youtube_posted(video_id: str, video_type: str, category: str, lang: str):
    table = f"youtube_posted_{lang}"
    c.execute(f"INSERT OR IGNORE INTO {table} (video_id, posted_at, type, category) VALUES (?, ?, ?, ?)",
              (video_id, datetime.now().isoformat(), video_type, category))
    conn.commit()


def track_youtube_channel(channel_name: str, lang: str):
    c.execute("INSERT INTO youtube_channels_used (channel_name, used_at, lang) VALUES (?, ?, ?)",
              (channel_name.lower(), datetime.now().isoformat(), lang))
    conn.commit()
    three_days_ago = (datetime.now() - timedelta(days=3)).isoformat()
    c.execute("DELETE FROM youtube_channels_used WHERE used_at < ?", (three_days_ago,))
    conn.commit()


def get_recent_channels(hours: int, lang: str) -> list:
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT DISTINCT channel_name FROM youtube_channels_used WHERE used_at > ? AND lang = ?",
              (cutoff, lang))
    return [row[0] for row in c.fetchall()]


def get_channel_usage_count(channel_name: str, hours: int, lang: str) -> int:
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT COUNT(*) FROM youtube_channels_used WHERE channel_name = ? AND used_at > ? AND lang = ?",
              (channel_name.lower(), cutoff, lang))
    result = c.fetchone()
    return result[0] if result else 0


def track_used_image(url: str):
    c.execute("INSERT INTO used_images (url, used_at) VALUES (?, ?)",
              (url, datetime.now().isoformat()))
    conn.commit()
    month_ago = (datetime.now() - timedelta(days=30)).isoformat()
    c.execute("DELETE FROM used_images WHERE used_at < ?", (month_ago,))
    conn.commit()


def log_analytics(lang: str, content_type: str, category: str, title: str,
                  channel: str = "", views: int = 0, likes: int = 0, success: bool = True):
    c.execute("""INSERT INTO analytics
                 (timestamp, lang, content_type, category, title, channel, views, likes, success)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
              (datetime.now().isoformat(), lang, content_type, category,
               title[:200], channel[:100], views, likes, success))
    conn.commit()


def get_analytics_summary(days: int = 7):
    cutoff = (datetime.now() - timedelta(days=days)).isoformat()
    summary = {}
    c.execute("""SELECT lang, COUNT(*), SUM(CASE WHEN success THEN 1 ELSE 0 END)
                 FROM analytics WHERE timestamp > ? GROUP BY lang""", (cutoff,))
    summary["by_lang"] = {row[0]: {"total": row[1], "success": row[2]} for row in c.fetchall()}
    c.execute("""SELECT category, COUNT(*), AVG(views)
                 FROM analytics WHERE timestamp > ? AND content_type = 'shorts'
                 GROUP BY category ORDER BY COUNT(*) DESC""", (cutoff,))
    summary["by_category"] = {row[0]: {"count": row[1], "avg_views": row[2]} for row in c.fetchall()}
    c.execute("""SELECT channel, COUNT(*), AVG(views)
                 FROM analytics WHERE timestamp > ? AND channel != ''
                 GROUP BY channel ORDER BY COUNT(*) DESC LIMIT 10""", (cutoff,))
    summary["top_channels"] = [(row[0], row[1], row[2]) for row in c.fetchall()]
    return summary


# ================== –ù–û–í–û–ï! –ì–û–†–Ø–ß–ò–ï –¢–ï–ú–´ (—Ç—Ä–µ–¥—ã) ==================
def extract_topic_keywords(title: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –º–∞—Ç—á–∏–Ω–≥–∞ —Ç–µ–º"""
    stop_words_ru = {'–≤', '–Ω–∞', '–∏', '–ø–æ', '—Å', '–∏–∑', '–∑–∞', '–∫', '–æ—Ç', '–¥–æ', '–æ', '–æ–±', '—á—Ç–æ', '–∫–∞–∫', '–Ω–µ', '–Ω–æ', '–∞'}
    stop_words_en = {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'has', 'had', 'do', 'did', 'not', 'and', 'or', 'but', 'if'}

    words = re.findall(r'[–∞-—è—ëa-z]{3,}', title.lower())
    stop = stop_words_ru | stop_words_en
    return [w for w in words if w not in stop]


def find_related_topic(title: str, lang: str) -> dict:
    """–ò—â–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—É—é –≥–æ—Ä—è—á—É—é —Ç–µ–º—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 —á–∞—Å–æ–≤"""
    keywords = extract_topic_keywords(title)
    if len(keywords) < 2:
        return None

    cutoff = (datetime.now() - timedelta(hours=6)).isoformat()
    c.execute("""SELECT id, topic_hash, keywords, first_message_id, channel_id, update_count
                 FROM hot_topics WHERE lang = ? AND created_at > ? ORDER BY created_at DESC""",
              (lang, cutoff))

    for row in c.fetchall():
        saved_keywords = json.loads(row[2])
        overlap = set(keywords) & set(saved_keywords)
        # –ï—Å–ª–∏ 3+ –æ–±—â–∏—Ö —Å–ª–æ–≤–∞ ‚Äî —ç—Ç–æ —Ç–∞ –∂–µ —Ç–µ–º–∞
        if len(overlap) >= 3:
            return {
                "id": row[0],
                "topic_hash": row[1],
                "keywords": saved_keywords,
                "first_message_id": row[3],
                "channel_id": row[4],
                "update_count": row[5]
            }
    return None


def save_hot_topic(title: str, message_id: int, channel_id: str, lang: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—É—é –≥–æ—Ä—è—á—É—é —Ç–µ–º—É"""
    keywords = extract_topic_keywords(title)
    topic_hash = hashlib.md5(' '.join(sorted(keywords[:5])).encode()).hexdigest()

    c.execute("""INSERT INTO hot_topics (topic_hash, keywords, first_message_id, channel_id, lang, created_at, last_update, update_count)
                 VALUES (?, ?, ?, ?, ?, ?, ?, 1)""",
              (topic_hash, json.dumps(keywords), message_id, channel_id, lang,
               datetime.now().isoformat(), datetime.now().isoformat()))
    conn.commit()


def update_hot_topic(topic_id: int):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—á—ë—Ç—á–∏–∫ –≥–æ—Ä—è—á–µ–π —Ç–µ–º—ã"""
    c.execute("""UPDATE hot_topics SET last_update = ?, update_count = update_count + 1 WHERE id = ?""",
              (datetime.now().isoformat(), topic_id))
    conn.commit()


# ================== –ù–û–í–û–ï! BREAKING –∞–Ω—Ç–∏—Ñ–ª—É–¥ ==================
def is_breaking_duplicate(title: str, lang: str) -> bool:
    """–ù–µ –ø–æ—Å—Ç–∏–º –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ breaking –¥–≤–∞–∂–¥—ã –∑–∞ 2 —á–∞—Å–∞"""
    keywords = extract_topic_keywords(title)
    h = hashlib.md5(' '.join(sorted(keywords[:5])).encode()).hexdigest()
    cutoff = (datetime.now() - timedelta(hours=2)).isoformat()
    c.execute("SELECT 1 FROM breaking_events WHERE hash = ? AND posted_at > ?", (h, cutoff))
    return c.fetchone() is not None


def save_breaking_event(title: str, lang: str):
    keywords = extract_topic_keywords(title)
    h = hashlib.md5(' '.join(sorted(keywords[:5])).encode()).hexdigest()
    c.execute("INSERT OR REPLACE INTO breaking_events (hash, title, lang, posted_at) VALUES (?, ?, ?, ?)",
              (h, title[:200], lang, datetime.now().isoformat()))
    conn.commit()
    # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ
    old = (datetime.now() - timedelta(days=1)).isoformat()
    c.execute("DELETE FROM breaking_events WHERE posted_at < ?", (old,))
    conn.commit()


# ================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ==================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI HELPER ==================
async def ask_ai(prompt: str, temperature=0.7) -> str:
    if not OPENROUTER_API_KEY:
        return None

    models = [
        "nousresearch/hermes-3-llama-3.1-405b:free",
        "meta-llama/llama-3.1-8b-instruct:free",
        "google/gemma-2-9b-it:free",
    ]

    for model in models:
        try:
            async with aiohttp.ClientSession() as s:
                headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 800
                }
                async with s.post("https://openrouter.ai/api/v1/chat/completions",
                                  headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as r:
                    if r.status == 200:
                        data = await r.json()
                        return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
            log.debug(f"AI error ({model}): {e}")
            continue
    return None


# ================== –ü–†–û–í–ï–†–ö–ê –ö–û–ù–¢–ï–ù–¢–ê ==================
def has_cyrillic(text):
    return bool(re.search('[–∞-—è–ê-–Ø—ë–Å]', text))


def has_ukrainian(text):
    return any(l in text for l in ['—î', '—ñ', '—ó', '“ë', '–Ñ', '–Ü', '–á', '“ê'])


def is_russian_content(title: str, channel: str, description: str = "") -> bool:
    full_text = f"{title} {channel} {description}".lower()
    if not has_cyrillic(title + channel):
        return False
    if has_ukrainian(title + channel + description):
        return False
    ua_keywords = ['—É–∫—Ä–∞—ó–Ω', 'ukrainian', '–∫–∏—ó–≤', '–∑–µ–ª–µ–Ω—Å—å–∫', '–∞–∑–æ–≤', '–≤—Å—É', '–∑—Å—É']
    if any(kw in full_text for kw in ua_keywords):
        return False
    return True


def is_english_content(title: str, channel: str, description: str = "") -> bool:
    """–ñ–Å–°–¢–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –¢–û–õ–¨–ö–û –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ‚Äî —É–±–∏–≤–∞–µ–º –∏–Ω–¥—É—Å–æ–≤"""
    full_text = f"{title} {channel} {description}"
    text_lower = full_text.lower()

    # 1. –ù–ï–¢ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    if has_cyrillic(full_text):
        return False

    # 2. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ª–∞—Ç–∏–Ω–∏—Ü–∞
    if not re.search('[a-zA-Z]', title):
        return False

    # 3. –ù–ï–¢ –¥—Ä—É–≥–∏—Ö –∞–ª—Ñ–∞–≤–∏—Ç–æ–≤
    non_english_scripts = [
        r'[\u0900-\u097F]',  # –•–∏–Ω–¥–∏ (–¥–µ–≤–∞–Ω–∞–≥–∞—Ä–∏)
        r'[\u0980-\u09FF]',  # –ë–µ–Ω–≥–∞–ª—å—Å–∫–∏–π
        r'[\u0A00-\u0A7F]',  # –ì—É—Ä–º—É–∫—Ö–∏
        r'[\u0600-\u06FF]',  # –ê—Ä–∞–±—Å–∫–∏–π
        r'[\u0750-\u077F]',  # –ê—Ä–∞–±—Å–∫–∏–π –¥–æ–ø
        r'[\u4E00-\u9FFF]',  # –ö–∏—Ç–∞–π—Å–∫–∏–π
        r'[\u3040-\u309F]',  # –•–∏—Ä–∞–≥–∞–Ω–∞
        r'[\u30A0-\u30FF]',  # –ö–∞—Ç–∞–∫–∞–Ω–∞
        r'[\uAC00-\uD7AF]',  # –ö–æ—Ä–µ–π—Å–∫–∏–π
        r'[\u0E00-\u0E7F]',  # –¢–∞–π—Å–∫–∏–π
        r'[\u1000-\u109F]',  # –ë–∏—Ä–º–∞–Ω—Å–∫–∏–π
        r'[\u0B80-\u0BFF]',  # –¢–∞–º–∏–ª—å—Å–∫–∏–π
        r'[\u0C00-\u0C7F]',  # –¢–µ–ª—É–≥—É
        r'[\u0C80-\u0CFF]',  # –ö–∞–Ω–Ω–∞–¥–∞
        r'[\u0D00-\u0D7F]',  # –ú–∞–ª–∞—è–ª–∞–º
        r'[\u0A80-\u0AFF]',  # –ì—É–¥–∂–∞—Ä–∞—Ç–∏
        r'[\u0B00-\u0B7F]',  # –û—Ä–∏—è
    ]
    for pattern in non_english_scripts:
        if re.search(pattern, full_text):
            return False

    # 4. –ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π/–∏—Å–ø–∞–Ω—Å–∫–∏–π/—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π
    non_english_chars = ['√±', '√ß', '√£', '√µ', '√º', '√∂', '√§', '√ü', '√®', '√©', '√†', '√¥', '√™', '√Æ', '√ª']
    if any(char in text_lower for char in non_english_chars):
        return False

    portuguese_words = [
        'voc√™', 'como', 'funciona', 'sabia', 'aqui', 'muito', 'mais',
        'esse', 'essa', 'quando', 'porque', 'ent√£o', 'agora', 'tamb√©m',
        'ainda', 'depois', 'antes', 'sempre', 'nunca', 'apenas',
    ]
    spanish_words = [
        'c√≥mo', 'qu√©', 'para', 'est√°', 'aqu√≠', 'm√°s', 'muy',
        'este', 'esta', 'cuando', 'porque', 'ahora', 'entonces',
        'tambi√©n', 'siempre', 'nunca', 'despu√©s', 'antes',
    ]
    if any(word in text_lower for word in portuguese_words + spanish_words):
        return False

    # 5. –ñ–Å–°–¢–ö–ò–ô –ò–ù–î–ò–ô–°–ö–ò–ô –§–ò–õ–¨–¢–†
    indian_channel_keywords = [
        'hindi', '‡§π‡§ø‡§®‡•ç‡§¶‡•Ä', 'bengali', '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ', 'sangbad', 'khabar',
        'tamil', 'telugu', 'urdu', 'punjabi', 'gujarati', 'marathi',
        'bollywood', 'zee', 'aaj tak', 'ndtv india', 'republic bharat',
        'tomazoli', 'curiosidades', 'curioso', 'increible',
        'brasileiro', 'portugu√™s', 'espa√±ol',
        'india today', 'india tv', 'abp news', 'tv9',
        'news18', 'news24', 'first india', 'good news today',
        'the lallantop', 'soch by mohak', 'satish ray',
        'drishti ias', 'pw', 'allen', 'motion',
    ]
    channel_lower = channel.lower()
    if any(kw in channel_lower for kw in indian_channel_keywords):
        return False

    # 6. –ù–û–í–û–ï! –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –Ω–∞ —Ö–∏–Ω–¥–∏-—Å–ª–æ–≤–∞ –≤ –ª–∞—Ç–∏–Ω–∏—Ü–µ
    hindi_transliterated = [
        'kya', 'hai', 'aur', 'yeh', 'woh', 'kaise', 'kyun', 'kab',
        'nahi', 'hoga', 'karo', 'dekho', 'bhai', 'yaar', 'dost',
        'bharat', 'desh', 'jaan', 'zindagi', 'pyar', 'dil',
        'samajh', 'padhai', 'paisa', 'sarkari', 'naukri',
    ]
    desc_lower = description.lower() if description else ""
    hindi_hits = sum(1 for w in hindi_transliterated if w in desc_lower)
    if hindi_hits >= 3:
        return False

    return True


def is_trusted_channel(channel: str, lang: str) -> bool:
    if lang == "ru":
        channels = RU_NEWS_CHANNELS + RU_ENTERTAINMENT_CHANNELS + RU_COMMENTARY_CHANNELS
    else:
        channels = EN_NEWS_CHANNELS + EN_ENTERTAINMENT_CHANNELS + EN_COMMENTARY_CHANNELS
    return any(t.lower() in channel.lower() for t in channels)


def is_blacklisted(title: str, channel: str) -> bool:
    text = f"{title} {channel}".lower()
    for banned in BLACKLIST_CHANNELS:
        if banned.lower() in text:
            return True
    for banned in BLACKLIST_TOPICS:
        if banned.lower() in text:
            return True
    return False


# ================== –í–´–ë–û–† –ö–ê–¢–ï–ì–û–†–ò–ò ==================
def select_category_by_time(lang: str) -> str:
    if lang == "ru":
        tz = pytz.timezone('Europe/Moscow')
    else:
        tz = pytz.timezone('America/New_York')

    local_time = datetime.now(tz)
    hour = local_time.hour

    if 6 <= hour < 10:
        weights = {"news": 60, "politics": 20, "economy": 10, "science_tech": 5, "entertainment": 3, "commentary": 2}
    elif 10 <= hour < 18:
        weights = {"news": 35, "politics": 20, "economy": 15, "science_tech": 15, "entertainment": 10, "commentary": 5}
    elif 18 <= hour < 23:
        weights = {"news": 25, "politics": 15, "economy": 5, "science_tech": 15, "entertainment": 25, "commentary": 15}
    else:
        weights = {"news": 20, "politics": 5, "economy": 5, "science_tech": 20, "entertainment": 35, "commentary": 15}

    categories = list(weights.keys())
    probs = list(weights.values())
    return random.choices(categories, weights=probs, k=1)[0]

# ================== –ù–û–í–û–ï! –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï BREAKING ==================
def calculate_breaking_score(title: str, lang: str) -> int:
    """–°—á–∏—Ç–∞–µ—Ç '–≥–æ—Ä—è—á–µ—Å—Ç—å' –Ω–æ–≤–æ—Å—Ç–∏. 0 = –æ–±—ã—á–Ω–∞—è, 3+ = BREAKING"""
    text = title.lower()
    breaking_kw = BREAKING_KEYWORDS_RU if lang == "ru" else BREAKING_KEYWORDS_EN
    score = sum(1 for kw in breaking_kw if kw in text)
    return score


# ================== AI: –í–´–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –° –ì–û–õ–û–°–û–ú ==================
async def ai_select_and_summarize(news_list: list, lang: str, is_breaking: bool = False) -> dict:
    news_text = "\n".join([f"{i+1}. {n['title']}" for i, n in enumerate(news_list[:25])])

    if lang == "ru":
        prompt = f"""–¢—ã ‚Äî –∑–ª–æ–π, —Ü–∏–Ω–∏—á–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –≤ –¥—É—Ö–µ –õ–µ–ø—Ä—ã –∏ –¢–æ–ø–æ—Ä–∞.
–¢–≤–æ–π —Å—Ç–∏–ª—å: –µ–¥–∫–∏–π —Å–∞—Ä–∫–∞–∑–º, —á—ë—Ä–Ω—ã–π —é–º–æ—Ä, –±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã –º—ã—Å–ª–µ–π (–Ω–æ –±–µ–∑ –º–∞—Ç–∞).
–¢—ã –Ω–µ —Å—é—Å—é–∫–∞–µ—à—å. –¢—ã –ø–∏—à–µ—à—å —Ç–∞–∫, –±—É–¥—Ç–æ —É—Å—Ç–∞–ª –æ—Ç —ç—Ç–æ–≥–æ –º–∏—Ä–∞, –Ω–æ –≤—Å—ë –µ—â—ë —Å–ª–µ–¥–∏—à—å –∑–∞ –Ω–∏–º.

{"üö® –≠–¢–û BREAKING! –ü–∏—à–∏ –∂—ë—Å—Ç–∫–æ –∏ —Å—Ä–æ—á–Ω–æ!" if is_breaking else ""}

‚ö†Ô∏è –ù–ï –í–´–ë–ò–†–ê–ô: –ø–æ–≥–æ–¥—É, —à–∫–æ–ª—ã, —Å–æ–≤–µ—â–∞–Ω–∏—è, —Ñ–∏–ª–æ—Å–æ—Ñ–∏—é, –±—é—Ä–æ–∫—Ä–∞—Ç–∏—é.

‚úÖ –í–´–ë–ò–†–ê–ô: –¥–µ–Ω—å–≥–∏, –≤–æ–π–Ω—É, —Å–∫–∞–Ω–¥–∞–ª—ã, –ø–æ–ª–∏—Ç–∏–∫—É, —à–æ–∫, –∫—Ä–∏–ø—Ç—É.

–§–û–†–ú–ê–¢:
- –ó–ê–ì–û–õ–û–í–û–ö: 30-50 —Å–∏–º–≤–æ–ª–æ–≤, —Ü–µ–ø–ª—è—é—â–∏–π, —Å –ø–æ–¥–∫–æ–ª–æ–º –∏–ª–∏ —Å–∞—Ä–∫–∞–∑–º–æ–º. –ë–ï–ó —ç–º–æ–¥–∑–∏.
- –ü–ï–†–ï–°–ö–ê–ó: 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –Ø–∑–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ. –ß–∏—Ç–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –∏ —É–∑–Ω–∞—Ç—å —Ñ–∞–∫—Ç, –∏ —É—Å–º–µ—Ö–Ω—É—Ç—å—Å—è.
  {"–ù–∞—á–Ω–∏ —Å ‚ö°Ô∏è BREAKING:" if is_breaking else ""}
- –•–ï–®–¢–ï–ì–ò: 3-4 —à—Ç—É–∫–∏

–ü–†–ò–ú–ï–†–´ –°–¢–ò–õ–Ø:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–ó–æ–ª–æ—Ç–æ –ø—Ä–æ–±–∏–ª–æ $5K, –∞ —Ç—ã –Ω–µ—Ç

–ü–æ–∫–∞ —Ç—ã —á–∏—Ç–∞–ª –º–µ–º—ã, –∑–æ–ª–æ—Ç–æ –æ–±–Ω–æ–≤–∏–ª–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π —Ä–µ–∫–æ—Ä–¥ ‚Äî $5,000 –∑–∞ —É–Ω—Ü–∏—é. –°–ø—Ä–æ—Å –Ω–∞ –±–ª–µ—Å—Ç—è—â–∏–µ –∫–∏—Ä–ø–∏—á–∏ —Ä–∞—Å—Ç—ë—Ç –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Ç–≤–æ—è —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å.

#–ó–æ–ª–æ—Ç–æ #–†–µ–∫–æ—Ä–¥ #–≠–∫–æ–Ω–æ–º–∏–∫–∞
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–¢—Ä–∞–º–ø –æ–ø—è—Ç—å —Å–∫–∞–∑–∞–ª. –ú–∏—Ä –æ–ø—è—Ç—å –æ—Ö–Ω—É–ª

–≠–∫—Å-–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –ø–æ–æ–±–µ—â–∞–ª "–∑–∞–∫–æ–Ω—á–∏—Ç—å –≤—Å–µ –≤–æ–π–Ω—ã –∑–∞ 24 —á–∞—Å–∞". –û—Å—Ç–∞–ª–æ—Å—å –ø–æ–Ω—è—Ç—å, –æ–Ω –ø—Ä–æ —á—É–∂–∏–µ –∏–ª–∏ –ø—Ä–æ —Å–≤–æ–∏.

#–¢—Ä–∞–º–ø #–ü–æ–ª–∏—Ç–∏–∫–∞ #–°–®–ê
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

–í–ê–ñ–ù–û: "summary" –ù–ï –î–û–õ–ñ–ï–ù –ë–´–¢–¨ –ü–£–°–¢–´–ú. –ú–∏–Ω–∏–º—É–º 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–í–µ—Ä–Ω–∏ JSON:
{{
  "selected": –Ω–æ–º–µ—Ä (1-{len(news_list[:25])}),
  "title": "–ï–¥–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ 30-50 —Å–∏–º–≤–æ–ª–æ–≤",
  "summary": "–¶–∏–Ω–∏—á–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "hashtags": "#–¢–µ–≥1 #–¢–µ–≥2 #–¢–µ–≥3"
}}

–ù–æ–≤–æ—Å—Ç–∏:
{news_text}"""

    else:
        prompt = f"""You are a sharp, cynical news editor. Think The Daily Show meets Reuters.
Your style: dry wit, dark humor, no sugarcoating. You're tired of the world but still watching it burn.

{"üö® THIS IS BREAKING! Write urgently and sharply!" if is_breaking else ""}

‚ö†Ô∏è DON'T PICK: weather, schools, meetings, celebrity gossip, recipes.
‚úÖ PICK: money, war, scandals, politics, shock, crypto, tech.

FORMAT:
- TITLE: 30-50 chars, catchy with a twist. NO emojis.
- SUMMARY: 2-3 sentences. Sarcastic but informative. Reader should learn the fact AND smirk.
  {"Start with ‚ö°Ô∏è BREAKING:" if is_breaking else ""}
- HASHTAGS: 3-4 single words

STYLE EXAMPLES:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Gold hits $5K. Your savings didn't.

Gold just smashed through $5,000/oz while your portfolio weeps quietly in the corner. Safe haven demand is up. So is everyone's anxiety.

#Gold #Record #Economy
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Trump promised again. World sighed again.

The former president vowed to "end all wars in 24 hours." Unclear if he means other people's wars or his own.

#Trump #Politics #USA
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

IMPORTANT: "summary" MUST NOT BE EMPTY. Minimum 2 sentences.

Return JSON:
{{
  "selected": number (1-{len(news_list[:25])}),
  "title": "Sharp witty title 30-50 chars",
  "summary": "Cynical summary 2-3 sentences",
  "hashtags": "#tag1 #tag2 #tag3"
}}

News:
{news_text}"""

    response = await ask_ai(prompt, temperature=0.9)

    if response:
        try:
            json_start = response.find('{')
            json_end = response.rfind('}')
            if json_start != -1 and json_end != -1:
                content = response[json_start:json_end + 1]
                result = json.loads(content)
                selected_idx = int(result.get("selected", 1)) - 1

                if 0 <= selected_idx < len(news_list):
                    selected = news_list[selected_idx]
                    selected["ai_title"] = result.get("title", selected["title"])
                    selected["summary"] = result.get("summary", "")
                    selected["hashtags"] = fix_hashtags(result.get("hashtags", ""), selected["title"], lang)
                    selected["is_breaking"] = is_breaking
                    return selected
        except Exception as e:
            log.warning(f"AI parse error: {e}")

    # Fallback
    selected = random.choice(news_list[:5])
    selected["ai_title"] = selected["title"]
    selected["summary"] = selected["desc"][:200] if selected["desc"] else ""
    selected["hashtags"] = generate_smart_hashtags(selected["title"], selected["desc"], lang)
    selected["is_breaking"] = is_breaking
    return selected


def fix_hashtags(raw_hashtags: str, title: str = "", lang: str = "ru") -> str:
    raw_hashtags = re.sub(r'@\w+', '', raw_hashtags).strip()
    tags = re.findall(r'#\w+', raw_hashtags)

    fixed_tags = []
    for tag in tags:
        word = tag[1:]
        parts = re.findall(r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]*|[–∞-—è—ëa-z]+|[A-Z][a-z]*|[a-z]+', word)
        if len(parts) > 1 and len(word) > 12:
            for part in parts:
                if len(part) > 2:
                    fixed_tags.append(f"#{part}")
        else:
            fixed_tags.append(tag)

    if len(fixed_tags) < 2:
        auto_tags = generate_smart_hashtags(title, "", lang)
        auto_list = re.findall(r'#\w+', auto_tags)
        for auto_tag in auto_list:
            if auto_tag.lower() not in [t.lower() for t in fixed_tags]:
                fixed_tags.append(auto_tag)
                if len(fixed_tags) >= 3:
                    break

    seen = set()
    unique = []
    for tag in fixed_tags:
        tag_lower = tag.lower()
        if tag_lower not in seen:
            seen.add(tag_lower)
            unique.append(tag)

    if len(unique) < 2:
        if lang == "ru":
            if "#–Ω–æ–≤–æ—Å—Ç–∏" not in [t.lower() for t in unique]:
                unique.append("#–ù–æ–≤–æ—Å—Ç–∏")
            if "#—Ä–æ—Å—Å–∏—è" not in [t.lower() for t in unique]:
                unique.append("#–†–æ—Å—Å–∏—è")
        else:
            if "#news" not in [t.lower() for t in unique]:
                unique.append("#News")
            if "#world" not in [t.lower() for t in unique]:
                unique.append("#World")

    return ' '.join(unique[:4])


def generate_smart_hashtags(title: str, description: str, lang: str) -> str:
    text = f"{title} {description}".lower()
    tags = []

    if lang == "ru":
        if '–ø—É—Ç–∏–Ω' in text: tags.append('#–ü—É—Ç–∏–Ω')
        if '—Ç—Ä–∞–º–ø' in text: tags.append('#–¢—Ä–∞–º–ø')
        if '–±–∞–π–¥–µ–Ω' in text: tags.append('#–ë–∞–π–¥–µ–Ω')
        if '—Å—à–∞' in text: tags.append('#–°–®–ê')
        if '—Ä–æ—Å—Å–∏' in text: tags.append('#–†–æ—Å—Å–∏—è')
        if '—É–∫—Ä–∞–∏–Ω' in text: tags.append('#–£–∫—Ä–∞–∏–Ω–∞')
        if '–¥–æ–ª–ª–∞—Ä' in text or '—Ä—É–±–ª—å' in text: tags.append('#–ö—É—Ä—Å')
        if '–≤–æ–π–Ω–∞' in text: tags.append('#–í–æ–π–Ω–∞')
        if not tags: tags.append('#–ù–æ–≤–æ—Å—Ç–∏')
    else:
        if 'putin' in text: tags.append('#Putin')
        if 'trump' in text: tags.append('#Trump')
        if 'biden' in text: tags.append('#Biden')
        if 'russia' in text: tags.append('#Russia')
        if 'usa' in text or 'america' in text: tags.append('#USA')
        if 'ukraine' in text: tags.append('#Ukraine')
        if 'war' in text: tags.append('#War')
        if not tags: tags.append('#News')

    return ' '.join(tags[:4])

# ================== –ö–ê–†–¢–ò–ù–ö–ò ==================
PERSON_SEARCH_QUERIES = {
    '—Ç—Ä–∞–º–ø': ['donald trump', 'trump president'],
    '–ø—É—Ç–∏–Ω': ['vladimir putin', 'putin russia'],
    '–±–∞–π–¥–µ–Ω': ['joe biden', 'biden president'],
    'trump': ['donald trump', 'trump president'],
    'putin': ['vladimir putin', 'putin russia'],
    'biden': ['joe biden', 'biden president'],
}


async def search_unsplash(query: str, count=10) -> list:
    if not UNSPLASH_ACCESS_KEY:
        return []
    try:
        url = "https://api.unsplash.com/search/photos"
        params = {"query": query, "per_page": count, "orientation": "landscape"}
        headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    data = await r.json()
                    return [{"url": p["urls"]["regular"], "source": "unsplash"}
                            for p in data.get("results", [])[:count]]
    except:
        pass
    return []


async def download_image(url: str):
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    return await r.read()
    except:
        pass
    return None


# ========== –ì–†–ê–§–ò–ö–ò ==========
async def get_bitcoin_data(days=30):
    url = "https://api.coingecko.com/api/v3/coins/bitcoin/market_chart"
    params = {"vs_currency": "usd", "days": days}
    try:
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, timeout=10) as r:
                if r.status == 200:
                    data = await r.json()
                    return [(datetime.fromtimestamp(p[0] / 1000), p[1]) for p in data["prices"]]
    except:
        pass
    return []


async def get_gold_data(days=30):
    url = "https://api.coingecko.com/api/v3/coins/pax-gold/market_chart"
    params = {"vs_currency": "usd", "days": days}
    try:
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, timeout=10) as r:
                if r.status == 200:
                    data = await r.json()
                    return [(datetime.fromtimestamp(p[0] / 1000), p[1]) for p in data["prices"]]
    except:
        pass
    return []


async def create_chart(data: list, title: str, ylabel: str, color: str = "#00ff00") -> bytes:
    if not data or len(data) < 2:
        return None
    dates = [d[0] for d in data]
    values = [d[1] for d in data]
    plt.style.use('dark_background')
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(dates, values, color=color, linewidth=2)
    ax.fill_between(dates, values, alpha=0.3, color=color)
    ax.set_title(title, fontsize=16, fontweight='bold', color='white')
    ax.set_ylabel(ylabel, fontsize=12, color='white')
    ax.grid(True, alpha=0.3)
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m'))
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=5))
    plt.xticks(rotation=45)
    current_price = values[-1]
    prev_price = values[-2]
    change = ((current_price - prev_price) / prev_price) * 100
    change_text = f"+{change:.2f}%" if change > 0 else f"{change:.2f}%"
    change_color = "#00ff00" if change > 0 else "#ff0000"
    ax.text(0.02, 0.98, f"${current_price:,.2f}",
            transform=ax.transAxes, fontsize=20,
            verticalalignment='top', color='white', fontweight='bold')
    ax.text(0.02, 0.90, change_text,
            transform=ax.transAxes, fontsize=14,
            verticalalignment='top', color=change_color, fontweight='bold')
    plt.tight_layout()
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, facecolor='#1a1a1a')
    buf.seek(0)
    plt.close()
    return buf.read()


async def get_economic_chart(title: str, lang: str) -> bytes:
    text_lower = title.lower()
    if any(kw in text_lower for kw in ['–±–∏—Ç–∫–æ–∏–Ω', 'btc', 'bitcoin', '–∫—Ä–∏–ø—Ç', 'crypto']):
        data = await get_bitcoin_data(30)
        if data:
            title_text = "Bitcoin (BTC) - Last 30 Days" if lang == "en" else "–ë–∏—Ç–∫–æ–∏–Ω (BTC) - 30 –¥–Ω–µ–π"
            return await create_chart(data, title_text, "USD", color="#f7931a")
    if any(kw in text_lower for kw in ['–∑–æ–ª–æ—Ç–æ', 'gold']):
        data = await get_gold_data(30)
        if data:
            title_text = "Gold - Last 30 Days" if lang == "en" else "–ó–æ–ª–æ—Ç–æ - 30 –¥–Ω–µ–π"
            return await create_chart(data, title_text, "USD", color="#ffd700")
    return None


async def get_perfect_image(title: str, description: str, rss_image: str = None, lang: str = "ru") -> str:
    text_lower = f"{title} {description}".lower()

    if rss_image:
        bad_domains = [
            'meduza.io/logo', 'meduza.io/images', 'meduza.io/css',
            'logo.png', 'placeholder', 'default.jpg', 'avatar',
            '1x1.png', 'pixel.gif', 'blank.jpg'
        ]
        if len(rss_image) >= 40 and not any(bad in rss_image.lower() for bad in bad_domains):
            img_data = await download_image(rss_image)
            if img_data and len(img_data) > 50000:
                if not (img_data[:100].startswith(b'<svg') or img_data[:100].startswith(b'<!DOCTYPE')):
                    track_used_image(rss_image)
                    return rss_image

    person_queries = []
    for person, queries in PERSON_SEARCH_QUERIES.items():
        if person in text_lower:
            person_queries.extend(queries[:1])
            break

    theme_queries = []
    if lang == "ru":
        themes = {
            '—Ç—Ä–∞–º–ø': ['donald trump president', 'trump politics'],
            '–ø—É—Ç–∏–Ω': ['vladimir putin russia', 'putin kremlin'],
            '–±–∏—Ç–∫–æ–∏–Ω': ['bitcoin crypto', 'cryptocurrency chart', 'btc price'],
            'btc': ['bitcoin mining', 'crypto trading', 'blockchain'],
            '—Ä—É–±–ª—å': ['russian ruble', 'ruble exchange rate', 'russian currency'],
            '–∑–æ–ª–æ—Ç–æ': ['gold bars', 'gold bullion', 'gold price chart'],
            '–Ω–µ—Ñ—Ç—å': ['oil refinery', 'crude oil', 'oil barrels'],
            '–¥–æ–ª–ª–∞—Ä': ['us dollar bills', 'dollar currency', 'usd banknotes'],
            '–µ–≤—Ä–æ': ['euro currency', 'euro banknotes', 'eurozone'],
            '–∫—Ä–∏–ø—Ç': ['cryptocurrency', 'crypto market', 'digital currency'],
            '–≤–æ–π–Ω–∞': ['war zone', 'military conflict', 'soldiers combat'],
            '–æ–±—Å—Ç—Ä–µ–ª': ['artillery fire', 'missile strike', 'explosion'],
            '—É–¥–∞—Ä': ['air strike', 'military attack', 'bombing'],
            '–≤—Å—É': ['ukrainian army', 'military forces', 'soldiers'],
            '–∞—Ä–º–∏—è': ['military troops', 'armed forces', 'army soldiers'],
            '–∞—Ä–µ—Å—Ç': ['police arrest', 'handcuffs', 'detained person'],
            '–∫–æ—Ä—Ä—É–ø—Ü': ['corruption scandal', 'bribery', 'fraud investigation'],
            '—Å—É–¥': ['courtroom', 'judge gavel', 'trial'],
            '—ç–∫–æ–Ω–æ–º–∏–∫': ['economy business', 'stock market'],
            '–ø–æ–ª–∏—Ç–∏–∫': ['politics government', 'parliament'],
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥': ['technology innovation', 'digital tech'],
            '–Ω–∞—É–∫–∞': ['science research', 'laboratory'],
        }
    else:
        themes = {
            'trump': ['donald trump president', 'trump politics'],
            'putin': ['vladimir putin russia', 'putin kremlin'],
            'bitcoin': ['bitcoin', 'btc chart', 'crypto'],
            'crypto': ['cryptocurrency', 'blockchain', 'digital currency'],
            'dollar': ['us dollar', 'usd bills', 'dollar currency'],
            'gold': ['gold bars', 'gold price', 'bullion'],
            'oil': ['crude oil', 'oil refinery', 'petroleum'],
            'stock': ['stock market', 'trading floor', 'wall street'],
            'fed': ['federal reserve', 'fed building', 'central bank'],
            'war': ['war zone', 'military', 'combat'],
            'strike': ['air strike', 'missile attack', 'bombing'],
            'military': ['army', 'soldiers', 'troops'],
            'econom': ['economy business', 'stock market'],
            'politic': ['politics government', 'parliament'],
            'tech': ['technology innovation', 'digital tech'],
            'science': ['science research', 'laboratory'],
            'arrest': ['police arrest', 'handcuffs', 'detained'],
            'scandal': ['corruption', 'investigation', 'fraud'],
            'court': ['courtroom', 'trial', 'judge'],
        }

    for keyword, queries in themes.items():
        if keyword in text_lower:
            theme_queries.extend(queries[:2])

    if not person_queries and not theme_queries:
        if lang == "ru":
            fallback = ['breaking news russia', 'moscow kremlin', 'russian politics',
                        'world events', 'global crisis', 'international news']
        else:
            fallback = ['breaking news visual', 'world politics crisis',
                        'global events', 'international affairs', 'major news']
        theme_queries = [random.choice(fallback)]

    all_queries = person_queries + theme_queries
    random.shuffle(all_queries)

    all_images = []
    for query in all_queries[:5]:
        images = await search_unsplash(query, count=15)
        all_images.extend(images)
        await asyncio.sleep(0.3)
        if len(all_images) >= 40:
            break

    month_ago = (datetime.now() - timedelta(days=30)).isoformat()
    c.execute("SELECT url FROM used_images WHERE used_at > ?", (month_ago,))
    used_urls = {row[0] for row in c.fetchall()}

    fresh_images = [img for img in all_images if img['url'] not in used_urls]

    if fresh_images:
        selected = random.choice(fresh_images[:15])
        track_used_image(selected['url'])
        return selected['url']

    if all_images:
        selected = random.choice(all_images[:15])
        track_used_image(selected['url'])
        return selected['url']

    return None


def escape_md_v2(text: str) -> str:
    if not text:
        return ""
    special = r'\_*[]()~`>#+-=|{}.!'
    escaped = ""
    for char in text:
        if char in special:
            escaped += '\\' + char
        else:
            escaped += char
    return escaped


# ================== –°–ë–û–† –ù–û–í–û–°–¢–ï–ô ==================
async def collect_fresh_news(lang: str, limit=30):
    candidates = []
    sources = RSS_SOURCES_RU if lang == "ru" else RSS_SOURCES_EN
    keywords = KEYWORDS_RU if lang == "ru" else KEYWORDS_EN
    boring = BORING_KEYWORDS_RU if lang == "ru" else BORING_KEYWORDS_EN

    sources_list = list(sources.items())
    random.shuffle(sources_list)

    for source_name, rss_url in sources_list:
        if len(candidates) >= limit:
            break
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                if len(candidates) >= limit:
                    break

                title = BeautifulSoup(entry.title.strip(), "html.parser").get_text()
                url = entry.link
                desc = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser").get_text()

                rss_image = None
                if hasattr(entry, 'media_content') and entry.media_content:
                    rss_image = entry.media_content[0].get('url')
                if not rss_image and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            rss_image = enc.get('href')
                            break
                if not rss_image:
                    soup = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        rss_image = img_tag['src']
                if rss_image and (not rss_image.startswith('http') or len(rss_image) < 30):
                    rss_image = None

                if len(title) < 20:
                    continue
                if is_duplicate(title, url, lang):
                    continue
                if any(b in title.lower() for b in boring):
                    continue
                if not any(k in title.lower() for k in keywords):
                    continue

                # –ù–û–í–û–ï! –°—á–∏—Ç–∞–µ–º breaking-score
                breaking_score = calculate_breaking_score(title, lang)

                candidates.append({
                    "title": title,
                    "url": url,
                    "desc": desc,
                    "source": source_name,
                    "rss_image": rss_image,
                    "breaking_score": breaking_score,
                })

        except Exception as e:
            log.error(f"RSS {source_name}: {e}")

    return candidates


# ================== –ü–û–°–¢–ò–ù–ì –ù–û–í–û–°–¢–ï–ô (—Å —Ç—Ä–µ–¥–∞–º–∏) ==================
async def post_news(news: dict, lang: str):
    channel = CHANNEL_RU if lang == "ru" else CHANNEL_EN
    title = news.get("ai_title", news["title"])
    summary = news.get("summary", "").strip()
    is_breaking = news.get("is_breaking", False)

    if not summary and news.get("desc"):
        summary = news["desc"][:300].strip()

    if not summary:
        log.warning(f"[{lang.upper()}] Summary –ø—É—Å—Ç–æ–π, —Å–∫–∏–ø–∞–µ–º")
        return False

    # –°—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞–Ω–∞–ª
    if lang == "ru":
        channel_text = "üëâ –ë—É–ª—å–º—è—à +18. –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è"
        channel_url = "https://t.me/+QYYYj7ofUM8yODRi"
    else:
        channel_text = "üëâ WORLD // ALERT +18. Subscribe"
        channel_url = "https://t.me/+MSAD4bRuxxY0Nzc6"

    # –ù–û–í–û–ï! –ò—Å—Ç–æ—á–Ω–∏–∫
    source_name = news.get("source", "").upper()

    # –ù–û–í–û–ï! Breaking-–ø—Ä–µ—Ñ–∏–∫—Å
    if is_breaking:
        if lang == "ru":
            breaking_prefix = "‚ö°Ô∏è –°–†–û–ß–ù–û\n\n"
        else:
            breaking_prefix = "‚ö°Ô∏è BREAKING\n\n"
    else:
        breaking_prefix = ""

    escaped_title = escape_md_v2(title)
    escaped_summary = escape_md_v2(summary)
    escaped_channel = escape_md_v2(channel_text)
    escaped_source = escape_md_v2(source_name)
    escaped_prefix = escape_md_v2(breaking_prefix)

    caption = (
        f"{escaped_prefix}"
        f"**{escaped_title}**\n\n"
        f"{escaped_summary}\n\n"
        f"üì° {escaped_source}\n\n"
        f"[{escaped_channel}]({channel_url})"
    )

    # –ö–∞—Ä—Ç–∏–Ω–∫–∞
    chart_bytes = await get_economic_chart(title, lang)

    if chart_bytes:
        img_data = chart_bytes
    else:
        img_url = await get_perfect_image(title, news.get("desc", ""), news.get("rss_image"), lang)
        if not img_url:
            log.warning(f"[{lang.upper()}] –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        img_data = await download_image(img_url)

    if not img_data or len(img_data) <= 1024:
        return False

    try:
        # –ù–û–í–û–ï! –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–¥ ‚Äî –µ—Å—Ç—å –ª–∏ —Å–≤—è–∑–∞–Ω–Ω–∞—è —Ç–µ–º–∞?
        related_topic = find_related_topic(news["title"], lang)
        reply_to = None

        if related_topic:
            reply_to = related_topic["first_message_id"]
            update_count = related_topic["update_count"]

            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            if lang == "ru":
                update_marker = f"üîÑ –û–ë–ù–û–í–õ–ï–ù–ò–ï \\#{update_count + 1}\n\n"
            else:
                update_marker = f"üîÑ UPDATE \\#{update_count + 1}\n\n"

            caption = update_marker + caption
            update_hot_topic(related_topic["id"])
            log.info(f"üîó [{lang.upper()}] –¢—Ä–µ–¥! –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ #{update_count + 1}")

        file = BufferedInputFile(img_data, filename="news.jpg")

        sent = await bot.send_photo(
            channel, file,
            caption=caption,
            parse_mode=ParseMode.MARKDOWN_V2,
            reply_to_message_id=reply_to
        )

        save_posted(news["title"], news["url"], lang)
        increment_stat(lang, "news")
        log_analytics(lang, "news", "breaking" if is_breaking else "news", title, success=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –≥–æ—Ä—è—á—É—é —Ç–µ–º—É –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–¥
        if not related_topic:
            save_hot_topic(news["title"], sent.message_id, channel, lang)

        # –õ–æ–≥ —Å —Å—Å—ã–ª–∫–æ–π
        channel_link = channel.replace('@', '')
        post_url = f"https://t.me/{channel_link}/{sent.message_id}"
        prefix = "‚ö°Ô∏è BREAKING" if is_breaking else "‚úÖ"
        log.info(f"{prefix} [{lang.upper()}] {title[:50]} ‚Üí {post_url}")

        if is_breaking:
            save_breaking_event(news["title"], lang)

        return True

    except Exception as e:
        log.error(f"‚ùå [{lang.upper()}] –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        log_analytics(lang, "news", "news", title, success=False)
        return False

# ================== YOUTUBE SHORTS ==================
def parse_duration_to_seconds(iso_duration):
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, iso_duration)
    if not match:
        return 0
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    return hours * 3600 + minutes * 60 + seconds


def format_views(views):
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}M"
    elif views >= 1_000:
        return f"{views / 1_000:.1f}K"
    return str(views)


async def search_shorts(lang: str, category: str):
    log.info(f"üîç [{lang.upper()}] –ü–æ–∏—Å–∫ Shorts, –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    recent_channels = get_recent_channels(12, lang)
    all_shorts = []

    cat_data = CONTENT_CATEGORIES.get(category, CONTENT_CATEGORIES["news"])
    queries = cat_data["queries_ru"] if lang == "ru" else cat_data["queries_en"]
    random.shuffle(queries)

    for query in queries[:5]:
        try:
            url = "https://www.googleapis.com/youtube/v3/search"
            params = {
                "part": "id,snippet",
                "q": query + " shorts",
                "type": "video",
                "maxResults": 50,
                "order": "date",
                "publishedAfter": (datetime.now() - timedelta(days=3)).isoformat() + "Z",
                "regionCode": "RU" if lang == "ru" else "US",
                "relevanceLanguage": lang,
                "key": YOUTUBE_API_KEY
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status != 200:
                        continue
                    data = await response.json()
                    video_ids = [item["id"]["videoId"] for item in data.get("items", [])
                                 if item["id"].get("kind") == "youtube#video"]
                    if not video_ids:
                        continue

                    details_url = "https://www.googleapis.com/youtube/v3/videos"
                    details_params = {
                        "part": "snippet,statistics,contentDetails",
                        "id": ",".join(video_ids[:50]),
                        "key": YOUTUBE_API_KEY
                    }

                    async with session.get(details_url, params=details_params, timeout=15) as resp:
                        if resp.status != 200:
                            continue
                        details_data = await resp.json()

                        for item in details_data.get("items", []):
                            try:
                                duration = item["contentDetails"]["duration"]
                                total_sec = parse_duration_to_seconds(duration)
                                if not (8 <= total_sec <= 65):
                                    continue

                                snippet = item["snippet"]
                                stats = item["statistics"]
                                title = snippet.get("title", "")
                                channel_name = snippet.get("channelTitle", "")
                                description = snippet.get("description", "")

                                # –ù–û–í–û–ï! –§–∏–ª—å—Ç—Ä –ø–æ audio language
                                audio_lang = snippet.get("defaultAudioLanguage", "")
                                default_lang = snippet.get("defaultLanguage", "")

                                if lang == "en":
                                    # –ï—Å–ª–∏ YouTube –∑–Ω–∞–µ—Ç —è–∑—ã–∫ –∏ –æ–Ω –ù–ï –∞–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Äî —Å–∫–∏–ø
                                    if audio_lang and not audio_lang.startswith("en"):
                                        continue
                                    if default_lang and not default_lang.startswith("en"):
                                        continue

                                if is_blacklisted(title, channel_name):
                                    continue
                                if channel_name.lower() in recent_channels:
                                    continue
                                if get_channel_usage_count(channel_name, 24, lang) >= 2:
                                    continue

                                if lang == "ru" and not is_russian_content(title, channel_name, description):
                                    continue
                                if lang == "en" and not is_english_content(title, channel_name, description):
                                    continue

                                views = int(stats.get("viewCount", 0))

                                if category == "commentary":
                                    min_views = 500 if is_trusted_channel(channel_name, lang) else 2000
                                else:
                                    min_views = 1000 if is_trusted_channel(channel_name, lang) else 3000

                                if views < min_views:
                                    continue

                                all_shorts.append({
                                    "id": item["id"],
                                    "title": title,
                                    "channel": channel_name,
                                    "views": views,
                                    "likes": int(stats.get("likeCount", 0)),
                                    "duration_sec": total_sec,
                                    "is_trusted": is_trusted_channel(channel_name, lang),
                                    "category": category
                                })

                            except:
                                continue

            await asyncio.sleep(0.4)

        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            continue

    seen_ids = set()
    unique = []
    for s in all_shorts:
        if s["id"] not in seen_ids:
            seen_ids.add(s["id"])
            unique.append(s)

    unique.sort(key=lambda x: (get_channel_usage_count(x["channel"], 48, lang),
                                not x["is_trusted"], -x["views"]))

    log.info(f"‚úÖ [{lang.upper()}] –ù–∞–π–¥–µ–Ω–æ {len(unique)} Shorts")
    return unique


async def download_shorts_video(video_id: str):
    output_file = os.path.join(TEMP_DIR, f"shorts_{video_id}.mp4")
    try:
        url = f"https://www.youtube.com/watch?v={video_id}"
        cmd = [
            sys.executable, "-m", "yt_dlp",
            "-f", "bv*+ba/b",
            "-o", output_file,
            "--no-playlist",
            "--merge-output-format", "mp4",
            "--extractor-args", "youtube:player_client=android",
            "--no-check-certificate",
            "--socket-timeout", "30",
            url
        ]
        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        await asyncio.wait_for(process.communicate(), timeout=90)
        if process.returncode == 0 and os.path.exists(output_file):
            return output_file
    except Exception as e:
        log.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
    if os.path.exists(output_file):
        os.remove(output_file)
    return None


async def post_youtube_shorts(lang: str):
    stats = get_today_stats(lang)
    if stats["shorts"] >= 12:
        log.info(f"[{lang.upper()}] –õ–∏–º–∏—Ç 12 shorts")
        return

    channel = CHANNEL_RU if lang == "ru" else CHANNEL_EN
    category = select_category_by_time(lang)

    log.info(f"üé¨ [{lang.upper()}] –ó–∞–ø—É—Å–∫ Shorts, –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    shorts = await search_shorts(lang, category)

    if not shorts:
        log.warning(f"[{lang.upper()}] Shorts –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    for i, short in enumerate(shorts[:15], 1):
        if is_youtube_posted(short["id"], lang):
            continue

        log.info(f"[{lang.upper()}] [{i}/15] {short['title'][:50]}...")
        video_path = await download_shorts_video(short["id"])

        if not video_path:
            continue

        try:
            clean_title = short['title']
            clean_title = re.sub(r'#\S+', '', clean_title).strip()
            clean_title = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', clean_title).strip()
            if '//' in clean_title:
                clean_title = clean_title.split('//')[0].strip()
            if '|' in clean_title:
                clean_title = clean_title.split('|')[0].strip()
            if '‚ñ∫' in clean_title:
                clean_title = clean_title.split('‚ñ∫')[0].strip()
            clean_title = clean_title.replace('*', '').replace('_', '').replace('`', '')
            if len(clean_title) > 100:
                clean_title = clean_title[:97] + "..."
            if not clean_title or len(clean_title) < 10:
                clean_title = f"Video from {short['channel']}" if lang == "en" else f"–í–∏–¥–µ–æ –æ—Ç {short['channel']}"

            clean_channel = short['channel'].replace('*', '').replace('_', '').replace('`', '')

            if lang == "ru":
                caption = (
                    f"{clean_title}\n\n"
                    f"üì∫ {clean_channel}\n"
                    f"üëÄ {format_views(short['views'])} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"
                )
            else:
                caption = (
                    f"{clean_title}\n\n"
                    f"üì∫ {clean_channel}\n"
                    f"üëÄ {format_views(short['views'])} views"
                )

            if len(caption) > 1000:
                caption = caption[:997] + "..."

            with open(video_path, 'rb') as f:
                video_data = f.read()

            video_file = BufferedInputFile(video_data, filename=f"{short['id']}.mp4")

            sent = await bot.send_video(
                channel, video=video_file, caption=caption,
                parse_mode=None, supports_streaming=True, width=1080, height=1920
            )

            channel_link = channel.replace('@', '')
            post_url = f"https://t.me/{channel_link}/{sent.message_id}"
            log.info(f"üì¨ [{lang.upper()}] Shorts: {post_url}")

            save_youtube_posted(short['id'], 'shorts', category, lang)
            track_youtube_channel(short['channel'], lang)
            increment_stat(lang, "shorts")
            log_analytics(lang, "shorts", category, short['title'],
                          short['channel'], short['views'], short['likes'], True)

            os.remove(video_path)
            return True

        except Exception as e:
            log.error(f"‚ùå [{lang.upper()}] –û—à–∏–±–∫–∞: {e}")
            log_analytics(lang, "shorts", category, short['title'],
                          short['channel'], short['views'], short['likes'], False)
            if os.path.exists(video_path):
                os.remove(video_path)
            continue

    return False


# ================== –ù–û–í–û–ï! –î–ê–ô–î–ñ–ï–°–¢ ==================
async def post_daily_digest(lang: str):
    """–í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç ‚Äî —Ç–æ–ø-5 –Ω–æ–≤–æ—Å—Ç–µ–π –¥–Ω—è"""
    log.info(f"üìã [{lang.upper()}] –§–æ—Ä–º–∏—Ä—É—é –¥–∞–π–¥–∂–µ—Å—Ç...")

    channel = CHANNEL_RU if lang == "ru" else CHANNEL_EN
    today = datetime.now().date().isoformat()

    # –ë–µ—Ä—ë–º –≤—Å–µ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
    c.execute("""SELECT title FROM analytics
                 WHERE lang = ? AND content_type = 'news' AND success = 1
                 AND timestamp LIKE ? ORDER BY timestamp DESC LIMIT 20""",
              (lang, f"{today}%"))
    rows = c.fetchall()

    if len(rows) < 3:
        log.info(f"[{lang.upper()}] –ú–∞–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ ({len(rows)})")
        return

    titles = [row[0] for row in rows]
    titles_text = "\n".join([f"{i+1}. {t}" for i, t in enumerate(titles)])

    if lang == "ru":
        prompt = f"""–¢—ã ‚Äî —Ü–∏–Ω–∏—á–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –≤ —Å—Ç–∏–ª–µ –õ–µ–ø—Ä—ã/–¢–æ–ø–æ—Ä–∞. –°–æ—Å—Ç–∞–≤—å –í–ï–ß–ï–†–ù–ò–ô –î–ê–ô–î–ñ–ï–°–¢ –∏–∑ —ç—Ç–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.

–í—ã–±–µ—Ä–∏ 5 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö/–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö. –î–ª—è –∫–∞–∂–¥–æ–π –Ω–∞–ø–∏—à–∏ –û–î–ù–û –µ–¥–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.

–§–æ—Ä–º–∞—Ç:
1. –ö—Ä–∞—Ç–∫–∏–π —Ü–∏–Ω–∏—á–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑ –Ω–æ–≤–æ—Å—Ç–∏
2. –ö—Ä–∞—Ç–∫–∏–π —Ü–∏–Ω–∏—á–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑ –Ω–æ–≤–æ—Å—Ç–∏
...–∏ —Ç–∞–∫ 5 —à—Ç—É–∫.

–í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å –æ–¥–Ω—É —Ñ—Ä–∞–∑—É-–ø–æ–¥–≤–æ–¥–∫—É —Ç–∏–ø–∞ "–°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏, —Å—Ç—Ä–∞–Ω–∞" –∏–ª–∏ "–ù—É, –≤—ã –ø–æ–Ω—è–ª–∏".

–ù–æ–≤–æ—Å—Ç–∏ –¥–Ω—è:
{titles_text}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞, –±–µ–∑ JSON."""

    else:
        prompt = f"""You're a cynical news editor. Write an EVENING DIGEST from these stories.

Pick 5 most important/interesting. For each, write ONE sharp witty sentence.

Format:
1. Sharp one-liner about the story
2. Sharp one-liner about the story
...5 total.

End with a closing quip like "Sleep tight, world" or "That's your Tuesday."

Today's news:
{titles_text}

Return ONLY the digest text, no JSON."""

    digest_text = await ask_ai(prompt, temperature=0.9)

    if not digest_text or len(digest_text) < 50:
        log.warning(f"[{lang.upper()}] AI –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–∞–π–¥–∂–µ—Å—Ç")
        return

    stats = get_today_stats(lang)

    if lang == "ru":
        header = "üåô –ò–¢–û–ì–ò –î–ù–Ø"
        footer = f"\n\nüìä –°–µ–≥–æ–¥–Ω—è: {stats['news']} –Ω–æ–≤–æ—Å—Ç–µ–π, {stats['shorts']} –≤–∏–¥–µ–æ"
    else:
        header = "üåô DAY IN REVIEW"
        footer = f"\n\nüìä Today: {stats['news']} news, {stats['shorts']} videos"

    full_text = f"{header}\n\n{digest_text}{footer}"

    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –¥–ª—è MarkdownV2
    # –ù–æ –¥–∞–π–¥–∂–µ—Å—Ç —Å–ª–æ–∂–Ω—ã–π, —à–ª—ë–º –±–µ–∑ Markdown
    try:
        await bot.send_message(channel, full_text, parse_mode=None)
        log.info(f"üìã [{lang.upper()}] –î–∞–π–¥–∂–µ—Å—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
    except Exception as e:
        log.error(f"‚ùå [{lang.upper()}] –û—à–∏–±–∫–∞ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")


# ================== CHECK NEWS —Å BREAKING ==================
async def check_news(lang: str):
    stats = get_today_stats(lang)
    if stats["news"] >= 25:
        log.info(f"[{lang.upper()}] –õ–∏–º–∏—Ç 25 –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    log.info(f"[{lang.upper()}] –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏...")
    candidates = await collect_fresh_news(lang, 30)

    if not candidates:
        log.info(f"[{lang.upper()}] –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç")
        return

    # –ù–û–í–û–ï! –ü—Ä–æ–≤–µ—Ä—è–µ–º BREAKING
    breaking_candidates = [n for n in candidates if n.get("breaking_score", 0) >= 3]

    if breaking_candidates:
        # –ï—Å—Ç—å breaking! –ü–æ—Å—Ç–∏–º —Å—Ä–æ—á–Ω–æ
        for bc in breaking_candidates[:2]:  # –ú–∞–∫—Å 2 breaking –∑–∞ —Ä–∞–∑
            if not is_breaking_duplicate(bc["title"], lang):
                log.info(f"‚ö°Ô∏è [{lang.upper()}] BREAKING DETECTED: {bc['title'][:60]}")
                selected = await ai_select_and_summarize([bc], lang, is_breaking=True)
                if selected:
                    await post_news(selected, lang)
                    return

    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
    selected = await ai_select_and_summarize(candidates, lang, is_breaking=False)
    if selected:
        await post_news(selected, lang)


# ================== –ù–û–í–û–ï! BREAKING MONITOR (–æ—Ç–¥–µ–ª—å–Ω—ã–π –±—ã—Å—Ç—Ä—ã–π —Ü–∏–∫–ª) ==================
async def breaking_monitor(lang: str):
    """–ë—ã—Å—Ç—Ä—ã–π —Ü–∏–∫–ª ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç RSS –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω—É—Ç—ã –¢–û–õ–¨–ö–û –Ω–∞ breaking"""
    log.info(f"‚ö°Ô∏è [{lang.upper()}] Breaking monitor –∑–∞–ø—É—â–µ–Ω")

    while True:
        try:
            sources = RSS_SOURCES_RU if lang == "ru" else RSS_SOURCES_EN
            keywords = KEYWORDS_RU if lang == "ru" else KEYWORDS_EN
            boring = BORING_KEYWORDS_RU if lang == "ru" else BORING_KEYWORDS_EN

            # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–±—ã—Å—Ç—Ä–æ!)
            quick_sources = random.sample(list(sources.items()), min(5, len(sources)))

            for source_name, rss_url in quick_sources:
                try:
                    feed = feedparser.parse(rss_url)
                    for entry in feed.entries[:3]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                        title = BeautifulSoup(entry.title.strip(), "html.parser").get_text()
                        url = entry.link

                        if len(title) < 20:
                            continue
                        if is_duplicate(title, url, lang):
                            continue
                        if any(b in title.lower() for b in boring):
                            continue

                        score = calculate_breaking_score(title, lang)

                        if score >= 3 and not is_breaking_duplicate(title, lang):
                            log.info(f"üö® [{lang.upper()}] BREAKING –ù–ê–ô–î–ï–ù (score={score}): {title[:60]}")

                            desc = BeautifulSoup(
                                entry.get("summary", "") or entry.get("description", ""),
                                "html.parser"
                            ).get_text()

                            # RSS Image
                            rss_image = None
                            if hasattr(entry, 'media_content') and entry.media_content:
                                rss_image = entry.media_content[0].get('url')

                            news_item = {
                                "title": title,
                                "url": url,
                                "desc": desc,
                                "source": source_name,
                                "rss_image": rss_image,
                                "breaking_score": score,
                            }

                            selected = await ai_select_and_summarize([news_item], lang, is_breaking=True)
                            if selected:
                                await post_news(selected, lang)
                                # –ü–æ—Å–ª–µ breaking ‚Äî –ø–∞—É–∑–∞ 10 –º–∏–Ω—É—Ç
                                await asyncio.sleep(600)

                except Exception as e:
                    log.debug(f"Breaking monitor RSS error {source_name}: {e}")
                    continue

        except Exception as e:
            log.error(f"Breaking monitor error [{lang}]: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω—É—Ç—ã
        await asyncio.sleep(120)

# ================== –¶–ò–ö–õ–´ ==================
async def news_loop_ru():
    log.info("‚è∞ [RU] –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 5 —Å–µ–∫...")
    await asyncio.sleep(5)
    while True:
        await check_news("ru")
        interval = random.randint(15, 45)
        log.info(f"‚è∞ [RU] –°–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)


async def news_loop_en():
    log.info("‚è∞ [EN] –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 30 —Å–µ–∫...")
    await asyncio.sleep(30)
    while True:
        await check_news("en")
        interval = random.randint(15, 45)
        log.info(f"‚è∞ [EN] –°–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)


async def shorts_loop_ru():
    log.info("‚è∞ [RU] –ü–µ—Ä–≤—ã–π Shorts —á–µ—Ä–µ–∑ 2 –º–∏–Ω...")
    await asyncio.sleep(120)
    while True:
        await post_youtube_shorts("ru")
        interval = random.randint(90, 150)
        log.info(f"‚è∞ [RU] –°–ª–µ–¥—É—é—â–∏–π Shorts —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)


async def shorts_loop_en():
    log.info("‚è∞ [EN] –ü–µ—Ä–≤—ã–π Shorts —á–µ—Ä–µ–∑ 3 –º–∏–Ω...")
    await asyncio.sleep(180)
    while True:
        await post_youtube_shorts("en")
        interval = random.randint(90, 150)
        log.info(f"‚è∞ [EN] –°–ª–µ–¥—É—é—â–∏–π Shorts —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)


def cleanup_old_files():
    try:
        now = datetime.now().timestamp()
        for filename in os.listdir(TEMP_DIR):
            filepath = os.path.join(TEMP_DIR, filename)
            if os.path.isfile(filepath):
                if now - os.path.getmtime(filepath) > 86400:
                    os.remove(filepath)
                    log.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω: {filename}")
    except Exception as e:
        log.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")


async def daily_analytics():
    summary = get_analytics_summary(1)
    log.info("=" * 50)
    log.info("üìä –ê–ù–ê–õ–ò–¢–ò–ö–ê –ó–ê –î–ï–ù–¨:")
    log.info(f"–ü–æ —è–∑—ã–∫–∞–º: {summary['by_lang']}")
    log.info(f"–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {summary['by_category']}")
    log.info(f"–¢–æ–ø –∫–∞–Ω–∞–ª—ã: {summary['top_channels'][:5]}")
    log.info("=" * 50)


# ================== MAIN ==================
async def main():
    migrate_database()

    scheduler = AsyncIOScheduler(timezone=TIMEZONE)

    # –û—á–∏—Å—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    scheduler.add_job(cleanup_old_files, "cron", hour=3, minute=0)
    scheduler.add_job(daily_analytics, "cron", hour=23, minute=55)

    # –ù–û–í–û–ï! –í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç
    scheduler.add_job(lambda: asyncio.ensure_future(post_daily_digest("ru")),
                      "cron", hour=22, minute=0)
    scheduler.add_job(lambda: asyncio.ensure_future(post_daily_digest("en")),
                      "cron", hour=22, minute=30)

    scheduler.start()

    log.info("=" * 70)
    log.info("ü§ñ –ù–û–í–û–°–¢–ù–û–ô –ë–û–¢ v4.0 ‚Äî DUAL LANG + BREAKING + THREADS + DIGEST")
    log.info("=" * 70)
    log.info(f"üì∞ RU –∫–∞–Ω–∞–ª: {CHANNEL_RU}")
    log.info(f"üåç EN –∫–∞–Ω–∞–ª: {CHANNEL_EN}")
    log.info("")
    log.info("üì∞ –ù–æ–≤–æ—Å—Ç–∏: –∫–∞–∂–¥—ã–µ 15-45 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å/–∫–∞–Ω–∞–ª)")
    log.info("üé¨ Shorts: –∫–∞–∂–¥—ã–µ 1.5-2.5 —á–∞—Å–∞ (–º–∞–∫—Å 12/–¥–µ–Ω—å/–∫–∞–Ω–∞–ª)")
    log.info("‚ö°Ô∏è Breaking monitor: –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω")
    log.info("üßµ –¢—Ä–µ–¥—ã: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ª–∏–Ω–∫–æ–≤–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π")
    log.info("üåô –î–∞–π–¥–∂–µ—Å—Ç: 22:00 MSK (RU), 22:30 MSK (EN)")
    log.info("üó£Ô∏è –ì–æ–ª–æ—Å: –õ–µ–ø—Ä–∞/–¢–æ–ø–æ—Ä (RU), Daily Show (EN)")
    log.info("")
    log.info("üÜï v4.0:")
    log.info("   ‚ö°Ô∏è BREAKING-—Ä–µ–∂–∏–º (–º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è)")
    log.info("   üßµ –¢—Ä–µ–¥-—Ñ–æ—Ä–º–∞—Ç (–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ —Ç–µ–º–µ)")
    log.info("   üåô –í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç")
    log.info("   üó£Ô∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å (—Å–∞—Ä–∫–∞–∑–º)")
    log.info("   üáÆüá≥ –ñ—ë—Å—Ç–∫–∏–π –∞–Ω—Ç–∏–∏–Ω–¥–∏–π—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä")
    log.info("   üì° –ò—Å—Ç–æ—á–Ω–∏–∫ –≤ –∫–∞–∂–¥–æ–º –ø–æ—Å—Ç–µ")
    log.info("=" * 70)

    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤
    await asyncio.gather(
        news_loop_ru(),
        news_loop_en(),
        shorts_loop_ru(),
        shorts_loop_en(),
        breaking_monitor("ru"),   # –ù–û–í–û–ï!
        breaking_monitor("en"),   # –ù–û–í–û–ï!
    )


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("üëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        conn.close()
    except Exception as e:
        log.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        conn.close()

import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile, InputMediaPhoto, InputMediaVideo
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# ================== CONFIG ==================
BOT_TOKEN = '7885944156:AAHrh2o1UPzJ67jviCULfOmP_BGPExdh6l8'
GROQ_API_KEY = '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'
OPENROUTER_API_KEY = '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'
CHANNEL_ID = '@bulmyash'
TIMEZONE = "Europe/Moscow"

# –ë–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
RSS_SOURCES = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "bbc_ru": "https://feeds.bbci.co.uk/russian/rss.xml",
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "rt": "https://www.rt.com/rss/",
    "lenta": "https://lenta.ru/rss",
    "meduza": "https://meduza.io/rss/all",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rosbalt": "https://www.rosbalt.ru/feed/",
}

# ================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (–†–ê–°–®–ò–†–ï–ù–ù–´–ï) ==================
KEYWORDS = [
    # –ü–æ–ª–∏—Ç–∏–∫–∞
    '—Å–∞–Ω–∫—Ü', '—Ç—Ä–∞–º–ø', '–ø—É—Ç–∏–Ω', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '–ª—É–∫–∞—à–µ–Ω–∫', 
    '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–¥—É–º–∞', '–º–∏–Ω–∏—Å—Ç', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',
    '–≤—ã–±–æ—Ä', '–≥–æ–ª–æ—Å–æ–≤–∞–Ω', '—Ä–µ—Ñ–µ—Ä–µ–Ω–¥—É–º', '–æ–ø–ø–æ–∑–∏—Ü',
    
    # –≠–∫–æ–Ω–æ–º–∏–∫–∞
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–∫—É—Ä—Å', '—Ü–±', '–±–∞–Ω–∫',
    '–∏–Ω—Ñ–ª—è—Ü', '—Ä—ã–Ω–æ–∫', '–±–∏—Ä–∂–∞', '–∞–∫—Ü–∏–∏', '–∫—Ä–∏–ø—Ç', '–±–∏—Ç–∫–æ–∏–Ω',
    '–º–∏–Ω—Ñ–∏–Ω', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç', '–≤–Ω–ø', '–≤–≤–ø',
    
    # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
    '—Å—à–∞', '–∫–∏—Ç–∞–π', '–µ—Å', '–µ–≤—Ä–æ—Å–æ—é–∑', '–Ω–∞—Ç–æ', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç',
    '–æ–ø–µ—Ä–∞—Ü', '–≤–æ–π—Å–∫', '–∞—Ä–º–∏—è', '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫',
    '–ø–µ—Ä–µ–≥–æ–≤–æ—Ä', '—Å–∞–º–º–∏—Ç', '–≤—Å—Ç—Ä–µ—á–∞', '–¥–æ–≥–æ–≤–æ—Ä', '—Å–æ–≥–ª–∞—à–µ–Ω',
    
    # –ß–ü –∏ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è
    '–∞–≤–∞—Ä', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤', '–æ–±—Ä—É—à–µ–Ω', '–∫—Ä—É—à–µ–Ω',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '—Ä–∞–Ω–µ–Ω', '—Å–ø–∞—Å', '—ç–≤–∞–∫—É–∞—Ü', '–º—á—Å',
    
    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'openai', 'google',
    'apple', 'microsoft', 'tesla', 'spacex', '–º–∞—Å–∫',
    '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä', '–∫–≤–∞–Ω—Ç–æ–≤',
    
    # –ö—Ä–∏–º–∏–Ω–∞–ª
    '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '–æ–±—ã—Å–∫', '—Å–ª–µ–¥—Å—Ç–≤', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '–º–æ—à–µ–Ω–Ω', '–≤–∑—è—Ç–∫', '–∫–æ—Ä—Ä—É–ø—Ü', '—É–∫—Ä–∞–ª', '–æ–≥—Ä–∞–±–ª',
    
    # –ù–∞—É–∫–∞
    '—É—á–µ–Ω', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–∏–∑–æ–±—Ä–µ—Ç', '–∫–æ—Å–º–æ—Å',
    '—Ä–∞–∫–µ—Ç', '—Å–ø—É—Ç–Ω–∏–∫', '–º–∞—Ä—Å', '–ª—É–Ω–∞',
    
    # –°–ø–æ—Ä—Ç (—Ç–æ–ø–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è)
    '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç –º–∏—Ä–∞', '—Ñ–∏–Ω–∞–ª', '—Å–±–æ—Ä–Ω–∞—è',
    '–º–µ—Å—Å–∏', '—Ä–æ–Ω–∞–ª–¥—É', '–æ–≤–µ—á–∫–∏–Ω'
]

BORING_KEYWORDS = ['–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', 
                   '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã', '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π',
                   '—Å–æ–Ω–Ω–∏–∫', '–ø—Ä–∏–º–µ—Ç–∞']

FALLBACK_IMAGES = [
    "https://images.unsplash.com/photo-1504711434969-e33886168f5c",
    "https://images.unsplash.com/photo-1495020689067-958852a7765e",
    "https://images.unsplash.com/photo-1523995462485-3d171b5c8fa9"
]

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

# –ú–∏–≥—Ä–∞—Ü–∏—è –ë–î
try:
    c.execute("ALTER TABLE posted ADD COLUMN priority TEXT")
    conn.commit()
except:
    pass

c.execute('''CREATE TABLE IF NOT EXISTS posted (
    hash TEXT UNIQUE, 
    posted_at TEXT, 
    priority TEXT,
    title TEXT,
    url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted (
    video_id TEXT UNIQUE, 
    posted_at TEXT, 
    type TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats (
    date TEXT UNIQUE, 
    normal_count INT
)''')
conn.commit()

def get_today_stats():
    today = datetime.now().date().isoformat()
    c.execute("SELECT normal_count FROM daily_stats WHERE date = ?", (today,))
    result = c.fetchone()
    if result:
        return {"normal": result[0]}
    return {"normal": 0}

def increment_stat():
    today = datetime.now().date().isoformat()
    stats = get_today_stats()
    stats["normal"] += 1
    c.execute("""INSERT OR REPLACE INTO daily_stats (date, normal_count) 
                 VALUES (?, ?)""", (today, stats["normal"]))
    conn.commit()

def is_duplicate(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("SELECT 1 FROM posted WHERE hash = ?", (h,))
    return c.fetchone() is not None

def save_posted(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("INSERT OR IGNORE INTO posted (hash, posted_at, title, url) VALUES (?, ?, ?, ?)", 
              (h, datetime.now().isoformat(), title, url))
    conn.commit()

def is_youtube_posted_today(video_id):
    today = datetime.now().date().isoformat()
    c.execute("SELECT 1 FROM youtube_posted WHERE video_id = ? AND DATE(posted_at) = ?", (video_id, today))
    return c.fetchone() is not None

def save_youtube_posted(video_id, video_type):
    c.execute("INSERT OR IGNORE INTO youtube_posted (video_id, posted_at, type) VALUES (?, ?, ?)", 
              (video_id, datetime.now().isoformat(), video_type))
    conn.commit()

# ================== –õ–û–ì–ò –ò –ë–û–¢ ==================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI: GROQ + OPENROUTER ==================
async def ai_summarize(title: str, desc: str, source: str) -> dict:
    """Groq (Llama 70B) ‚Üí OpenRouter (Mistral Large)"""
    
    if len(title) < 20 or any(boring in title.lower() for boring in BORING_KEYWORDS):
        return None
    
    prompt = f"""–ü–µ—Ä–µ—Å–∫–∞–∂–∏ –Ω–æ–≤–æ—Å—Ç—å –ö–†–ê–¢–ö–û (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –¥–æ–±–∞–≤—å –æ—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π.
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:
{{
  "summary": "—Ç–µ–∫—Å—Ç",
  "hashtags": "#—Ç–µ–≥1 #—Ç–µ–≥2"
}}

{title}
{desc[:200]}"""
    
    # 1Ô∏è‚É£ GROQ
    try:
        async with aiohttp.ClientSession() as s:
            headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
            payload = {
                "model": "llama-3.3-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 200,
                "response_format": {"type": "json_object"}
            }
            async with s.post("https://api.groq.com/openai/v1/chat/completions", 
                            headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=15)) as r:
                if r.status == 200:
                    data = await r.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    # –£–±–∏—Ä–∞–µ–º markdown –∏ –º—É—Å–æ—Ä
                    content = re.sub(r'```json\s*|\s*```', '', content).strip()
                    result = json.loads(content)
                    log.info("‚úÖ AI: Groq")
                    return result
                elif r.status == 429:
                    log.warning("‚ö†Ô∏è Groq rate limit")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è Groq failed: {e}")
    
    # 2Ô∏è‚É£ OPENROUTER (—É—Å–∏–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
    try:
        async with aiohttp.ClientSession() as s:
            headers = {
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/news-bot",
                "X-Title": "News Bot"
            }
            payload = {
                "model": "mistralai/mistral-large-2411",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 300  # –£–≤–µ–ª–∏—á–∏–ª –ª–∏–º–∏—Ç
            }
            async with s.post("https://openrouter.ai/api/v1/chat/completions",
                            headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=20)) as r:
                if r.status == 200:
                    data = await r.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    
                    # –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –û–ß–ò–°–¢–ö–ê
                    # –£–±–∏—Ä–∞–µ–º –≤—Å—ë –¥–æ –ø–µ—Ä–≤–æ–π {
                    json_start = content.find('{')
                    if json_start == -1:
                        raise ValueError("–ù–µ—Ç JSON –≤ –æ—Ç–≤–µ—Ç–µ")
                    content = content[json_start:]
                    
                    # –£–±–∏—Ä–∞–µ–º –≤—Å—ë –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π }
                    json_end = content.rfind('}')
                    if json_end == -1:
                        raise ValueError("–ù–µ—Ç –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏")
                    content = content[:json_end+1]
                    
                    # –£–±–∏—Ä–∞–µ–º markdown
                    content = re.sub(r'```(?:json)?\s*|\s*```', '', content).strip()
                    
                    result = json.loads(content)
                    log.info("‚úÖ AI: OpenRouter")
                    return result
                else:
                    text = await r.text()
                    log.error(f"OpenRouter HTTP {r.status}: {text[:200]}")
    except json.JSONDecodeError as e:
        log.error(f"‚ö†Ô∏è OpenRouter JSON error: {e} | Content: {content[:100]}")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è OpenRouter failed: {e}")
    
    log.error("‚ùå –í—Å–µ AI –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    return None
# ================== –ú–ï–î–ò–ê ==================
async def get_og_image(url: str) -> str | None:
    for attempt in range(3):
        try:
            connector = aiohttp.TCPConnector(ssl=False, force_close=True)
            async with aiohttp.ClientSession(connector=connector) as s:
                async with s.get(url, 
                               headers={"User-Agent": "Mozilla/5.0"}, 
                               timeout=aiohttp.ClientTimeout(total=8)) as r:
                    if r.status != 200: 
                        return None
                    html = await r.text()
                    soup = BeautifulSoup(html, "html.parser")
                    og = soup.find("meta", property="og:image")
                    if og and og.get("content"):
                        img_url = og["content"]
                        if img_url.startswith("//"):
                            img_url = "https:" + img_url
                        return img_url
                    return None
        except Exception as e:
            if attempt == 2:
                log.error(f"OG image error: {e}")
            await asyncio.sleep(1)
    return None

async def download_image(url: str) -> bytes | None:
    for attempt in range(3):
        try:
            connector = aiohttp.TCPConnector(ssl=False, force_close=True)
            async with aiohttp.ClientSession(connector=connector) as s:
                async with s.get(url, timeout=aiohttp.ClientTimeout(total=10)) as r:
                    if r.status == 200:
                        return await r.read()
        except Exception as e:
            if attempt == 2:
                log.error(f"Download image error: {e}")
            await asyncio.sleep(1)
    return None

async def download_video(url: str) -> bytes | None:
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=30)) as r:
                if r.status == 200 and int(r.headers.get('content-length', 0)) < 50_000_000:
                    return await r.read()
    except Exception as e:
        log.error(f"Download video error: {e}")
    return None

async def extract_videos(entry):
    videos = []
    seen_urls = set()
    
    try:
        if hasattr(entry, "media_content"):
            for m in entry.media_content:
                u = m.get("url")
                if u and ("video" in m.get("medium", "") or u.endswith((".mp4", ".webm"))):
                    if u not in seen_urls:
                        videos.append(u)
                        seen_urls.add(u)
        
        if hasattr(entry, "enclosures"):
            for enc in entry.enclosures:
                if "video" in enc.get("type", ""):
                    href = enc["href"]
                    if href not in seen_urls:
                        videos.append(href)
                        seen_urls.add(href)
    except Exception as e:
        log.error(f"Extract videos error: {e}")
    
    return videos[:3]

# ================== YOUTUBE –¢–û–ü–´ ==================
async def parse_youtube_trending():
    url = "https://www.youtube.com/feed/trending?gl=RU&hl=ru"
    
    try:
        async with aiohttp.ClientSession() as s:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept-Language": "ru-RU,ru;q=0.9"
            }
            async with s.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=15)) as r:
                if r.status != 200:
                    return []
                
                html = await r.text()
                match = re.search(r'var ytInitialData = ({.*?});', html)
                if not match:
                    return []
                
                data = json.loads(match.group(1))
                videos = []
                try:
                    tabs = data["contents"]["twoColumnBrowseResultsRenderer"]["tabs"]
                    for tab in tabs:
                        if "tabRenderer" in tab:
                            content = tab["tabRenderer"].get("content", {})
                            section = content.get("richGridRenderer", {}).get("contents", [])
                            
                            for item in section:
                                if "richItemRenderer" in item:
                                    video_data = item["richItemRenderer"]["content"]["videoRenderer"]
                                    
                                    video_id = video_data.get("videoId")
                                    title = video_data.get("title", {}).get("runs", [{}])[0].get("text", "")
                                    views = video_data.get("viewCountText", {}).get("simpleText", "0")
                                    length = video_data.get("lengthText", {}).get("simpleText", "")
                                    
                                    if video_id and title:
                                        videos.append({
                                            "id": video_id,
                                            "title": title,
                                            "views": views,
                                            "length": length,
                                            "url": f"https://www.youtube.com/watch?v={video_id}"
                                        })
                except:
                    pass
                
                return videos[:20]
    except Exception as e:
        log.error(f"YouTube error: {e}")
        return []

def is_short_video(length_str: str) -> bool:
    if not length_str:
        return False
    try:
        parts = length_str.split(":")
        if len(parts) == 2:
            minutes, seconds = map(int, parts)
            return minutes == 0 and seconds < 60
        elif len(parts) == 1:
            return int(parts[0]) < 60
    except:
        pass
    return False

async def post_youtube_tops():
    videos = await parse_youtube_trending()
    if not videos:
        return
    
    full_videos = [v for v in videos if not is_short_video(v.get("length", ""))]
    short_videos = [v for v in videos if is_short_video(v.get("length", ""))]
    
    top_full = None
    for v in full_videos:
        if not is_youtube_posted_today(v["id"]):
            top_full = v
            break
    
    top_short = None
    for v in short_videos:
        if not is_youtube_posted_today(v["id"]):
            top_short = v
            break
    
    if top_full:
        try:
            caption = f"üî• **–°–∞–º–æ–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ –≤–∏–¥–µ–æ —Å–µ–≥–æ–¥–Ω—è –≤ –†–§**\n\n{top_full['title']}\n\nüëÄ {top_full['views']}\n\n{top_full['url']}"
            await bot.send_message(CHANNEL_ID, caption, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=False)
            save_youtube_posted(top_full['id'], 'full')
            log.info(f"‚úÖ YouTube —Ç–æ–ø")
        except Exception as e:
            log.error(f"YouTube error: {e}")
    
    await asyncio.sleep(3)
    
    if top_short:
        try:
            caption = f"‚ö° **–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π Shorts —Å–µ–≥–æ–¥–Ω—è**\n\n{top_short['title']}\n\nüëÄ {top_short['views']}\n\n{top_short['url']}"
            await bot.send_message(CHANNEL_ID, caption, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=False)
            save_youtube_posted(top_short['id'], 'shorts')
            log.info(f"‚úÖ YouTube shorts")
        except Exception as e:
            log.error(f"YouTube error: {e}")

# ================== –ü–û–°–¢–ò–ù–ì ==================
async def post_news(entry, source_name: str):
    """–ü–û–°–¢–ò–¢ –û–î–ù–£ –ù–û–í–û–°–¢–¨ (—Ñ–æ—Ä–º–∞—Ç –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω–∞—Ö)"""
    title = entry.title.strip()
    url = entry.link
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è
    if is_duplicate(title, url):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞
    stats = get_today_stats()
    if stats["normal"] >= 25:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–∫—É—á–Ω–æ–µ
    title_lower = title.lower()
    if any(boring in title_lower for boring in BORING_KEYWORDS):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    if not any(k in title_lower for k in KEYWORDS):
        return False
    
    # –ë–µ—Ä—ë–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ RSS
    desc = entry.get("summary", "") or entry.get("description", "") or ""
    
    # AI –æ–±—Ä–∞–±–æ—Ç–∫–∞
    analysis = await ai_summarize(title, desc, source_name)
    
    if not analysis:
        log.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç AI): {title[:50]}")
        return False
    
    # –§–æ—Ä–º–∞—Ç –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω–∞—Ö (–ë–ï–ó —ç–º–æ–¥–∑–∏, –ë–ï–ó —Å—Å—ã–ª–æ–∫)
    caption = f"**{title}**\n\n{analysis['summary']}\n\n_{source_name}_\n\n{analysis['hashtags']}"
    
    # 1Ô∏è‚É£ –í–ò–î–ï–û (–º–µ–¥–∏–∞–≥—Ä—É–ø–ø–∞)
    videos = await extract_videos(entry)
    if videos:
        try:
            media_group = []
            for i, video_url in enumerate(videos):
                video_data = await download_video(video_url)
                if video_data:
                    video_file = BufferedInputFile(video_data, filename=f"video{i}.mp4")
                    if i == 0:
                        media_group.append(InputMediaVideo(media=video_file, caption=caption, parse_mode=ParseMode.MARKDOWN))
                    else:
                        media_group.append(InputMediaVideo(media=video_file))
            
            if media_group:
                await bot.send_media_group(CHANNEL_ID, media_group)
                save_posted(title, url)
                increment_stat()
                log.info(f"‚úÖ –í–∏–¥–µ–æ ({len(media_group)}—à—Ç): {title[:40]}")
                return True
        except Exception as e:
            log.error(f"Video error: {e}")
    
    # 2Ô∏è‚É£ –§–û–¢–û
    img_url = await get_og_image(url)
    if not img_url:
        img_url = random.choice(FALLBACK_IMAGES)
    
    img_data = await download_image(img_url)
    
    try:
        if img_data:
            file = BufferedInputFile(img_data, filename="news.jpg")
            await bot.send_photo(CHANNEL_ID, file, caption=caption, parse_mode=ParseMode.MARKDOWN)
        else:
            await bot.send_message(CHANNEL_ID, caption, parse_mode=ParseMode.MARKDOWN)
    except Exception as e:
        log.error(f"Post error: {e}")
        return False
    
    save_posted(title, url)
    increment_stat()
    log.info(f"‚úÖ –ü–æ—Å—Ç: {title[:40]}")
    return True

# ================== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ==================
async def check_news():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç RSS –∏ –ø–æ—Å—Ç–∏—Ç –û–î–ù–£ –Ω–æ–≤–æ—Å—Ç—å"""
    sources_list = list(RSS_SOURCES.items())
    random.shuffle(sources_list)
    
    for source_name, rss_url in sources_list:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:10]:
                success = await post_news(entry, source_name)
                if success:
                    return  # –°–¢–û–ü –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–æ—Å—Ç–∞
        except Exception as e:
            log.error(f"RSS error {source_name}: {e}")
    
    log.info("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

async def news_loop():
    """–ü–æ—Å—Ç–∏–Ω–≥ –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω"""
    log.info("‚è∞ –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 3 –º–∏–Ω...")
    await asyncio.sleep(3 * 60)
    
    while True:
        await check_news()
        next_interval = random.randint(20, 70)
        log.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ {next_interval} –º–∏–Ω")
        await asyncio.sleep(next_interval * 60)

async def main():
    scheduler = AsyncIOScheduler(timezone=TIMEZONE)
    scheduler.add_job(post_youtube_tops, "cron", hour=19, minute=0)
    scheduler.start()
    
    log.info("ü§ñ –ë–û–¢ –ó–ê–ü–£–©–ï–ù")
    log.info("üì∞ –ü–æ—Å—Ç—ã –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å)")
    log.info("üé¨ YouTube: 19:00 (—Ç–æ–ø –≤–∏–¥–µ–æ + shorts)")
    log.info("ü§ñ AI: Groq ‚Üí OpenRouter")
    log.info(f"üì° –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(RSS_SOURCES)}")
    log.info(f"üîë –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(KEYWORDS)}")
    
    await news_loop()

if __name__ == "__main__":
    asyncio.run(main())
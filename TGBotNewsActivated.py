import os
import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
import sys
from dotenv import load_dotenv
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile
from apscheduler.schedulers.asyncio import AsyncIOScheduler

load_dotenv()

# ================== CONFIG ==================
BOT_TOKEN = os.getenv("BOT_TOKEN")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")

CHANNEL_ID = '@bulmyash'
TIMEZONE = "Europe/Moscow"

if sys.platform == "win32":
    TEMP_DIR = "C:/temp/shorts"
else:
    TEMP_DIR = "/tmp/shorts"
os.makedirs(TEMP_DIR, exist_ok=True)

RSS_SOURCES = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "lenta": "https://lenta.ru/rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rt": "https://www.rt.com/rss/",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "rosbalt": "https://www.rosbalt.ru/feed/",
    "forbes": "https://www.forbes.ru/newrss.xml",
    "rbc_economics": "https://rssexport.rbc.ru/rbcnews/news/20/full.rss",
    "cnews": "https://www.cnews.ru/inc/rss/news.xml",
    "habr": "https://habr.com/ru/rss/all/all/",
    "bbc_ru": "https://feeds.bbci.co.uk/russian/rss.xml",
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "meduza": "https://meduza.io/rss/all",
}

KEYWORDS = [
    '–ø—É—Ç–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–∫—Ä–µ–º–ª', '–≥–æ—Å–¥—É–º', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', 
    '–º–∏–Ω–∏—Å—Ç—Ä', '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '—Å—à–∞', '–∫–∏—Ç–∞–π',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∫—É—Ä—Å', '—Ü–±', '–±–∞–Ω–∫', '–∏–Ω—Ñ–ª—è—Ü',
    '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—Å–∞–Ω–∫—Ü', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–∞—Ä–º–∏—è',
    '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫', '–∞–≤–∞—Ä', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'google', 'apple',
    '—É—á–µ–Ω', '–∫–æ—Å–º–æ—Å', '–≤—ã–±–æ—Ä', '–∑–∞–∫–æ–Ω', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç'
]

BORING_KEYWORDS = [
    '–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã',
    '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π', '—Å–æ–Ω–Ω–∏–∫', '–ø—Ä–∏–º–µ—Ç—ã', '–∏–º–µ–Ω–∏–Ω—ã',
]

RU_NEWS_CHANNELS = [
    "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏", "–¢–ê–°–°", "–ò–∑–≤–µ—Å—Ç–∏—è", "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å", "–†–ë–ö",
    "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä", "–í–µ–¥–æ–º–æ—Å—Ç–∏", "–ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª", "–†–æ—Å—Å–∏—è 24",
    "–ù–¢–í", "RT", "–î–ï–ù–¨ –¢–í", "–ö—Ä–µ–º–ª—å", "–î–æ–∂–¥—å", "–ú–µ–¥—É–∑–∞",
    "–≤–î—É–¥—å", "–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞", "–§–ï–ô–ì–ò–ù LIVE", "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ",
]

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

c.execute('''CREATE TABLE IF NOT EXISTS posted (
    hash TEXT UNIQUE, 
    posted_at TEXT, 
    title TEXT,
    url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted (
    video_id TEXT UNIQUE, 
    posted_at TEXT, 
    type TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats (
    date TEXT UNIQUE, 
    normal_count INT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS used_images (
    url TEXT,
    used_at TEXT
)''')
conn.commit()

def get_today_stats():
    today = datetime.now().date().isoformat()
    c.execute("SELECT normal_count FROM daily_stats WHERE date = ?", (today,))
    result = c.fetchone()
    return {"normal": result[0]} if result else {"normal": 0}

def increment_stat():
    today = datetime.now().date().isoformat()
    stats = get_today_stats()
    stats["normal"] += 1
    c.execute("INSERT OR REPLACE INTO daily_stats (date, normal_count) VALUES (?, ?)", 
              (today, stats["normal"]))
    conn.commit()

def is_duplicate(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("SELECT 1 FROM posted WHERE hash = ?", (h,))
    return c.fetchone() is not None

def save_posted(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("INSERT OR IGNORE INTO posted (hash, posted_at, title, url) VALUES (?, ?, ?, ?)", 
              (h, datetime.now().isoformat(), title, url))
    conn.commit()

def track_used_image(url: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ –ë–î"""
    c.execute("INSERT INTO used_images (url, used_at) VALUES (?, ?)", 
              (url, datetime.now().isoformat()))
    conn.commit()
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ (>7 –¥–Ω–µ–π)
    week_ago = (datetime.now() - timedelta(days=7)).isoformat()
    c.execute("DELETE FROM used_images WHERE used_at < ?", (week_ago,))
    conn.commit()

def get_recent_images() -> list:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞"""
    yesterday = (datetime.now() - timedelta(hours=24)).isoformat()
    c.execute("SELECT url FROM used_images WHERE used_at > ?", (yesterday,))
    return [row[0] for row in c.fetchall()]

def is_youtube_posted_today(video_id):
    today = datetime.now().date().isoformat()
    c.execute("SELECT 1 FROM youtube_posted WHERE video_id = ? AND DATE(posted_at) = ?", 
              (video_id, today))
    return c.fetchone() is not None

def save_youtube_posted(video_id, video_type):
    c.execute("INSERT OR IGNORE INTO youtube_posted (video_id, posted_at, type) VALUES (?, ?, ?)", 
              (video_id, datetime.now().isoformat(), video_type))
    conn.commit()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI: –í–´–ë–û–† –ù–û–í–û–°–¢–ò ==================
async def ai_select_and_summarize(news_list: list) -> dict:
    """AI –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ –¥–µ–ª–∞–µ—Ç –Ø–ó–í–ò–¢–ï–õ–¨–ù–´–ô/–ö–û–ú–ò–ß–ù–´–ô –ø–µ—Ä–µ—Å–∫–∞–∑"""
    
    news_text = "\n".join([f"{i+1}. {n['title']}" for i, n in enumerate(news_list[:25])])
    
    prompt = f"""–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –î–ï–†–ó–ö–û–ì–û –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –≤ —Å—Ç–∏–ª–µ "–ú–µ–¥—É–∑—ã" –∏–ª–∏ "–ü–∏–≤–Ω–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∞".

–í—ã–±–µ—Ä–∏ –û–î–ù–£ —Å–∞–º—É—é –≤–∑—Ä—ã–≤–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ —Å–¥–µ–ª–∞–π —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π/–∏—Ä–æ–Ω–∏—á–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑.

–í–ê–ñ–ù–û:
1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ú (–Ω–µ –∫–æ–ø–∏—Ä—É–π –∏—Å—Ö–æ–¥–Ω—ã–π)
2. –ü–µ—Ä–µ—Å–∫–∞–∑ –¥–æ–ª–∂–µ–Ω –î–û–ü–û–õ–ù–Ø–¢–¨ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏/–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
3. –¢–æ–Ω: –∏—Ä–æ–Ω–∏—á–Ω—ã–π, —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π, –Ω–æ –±–µ–∑ –º–∞—Ç–∞
4. –ü–µ—Ä–µ—Å–∫–∞–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ù–û –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞

–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å—Ç–∏–ª—è:

–ò—Å—Ö–æ–¥–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å: "–ü—É—Ç–∏–Ω –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –ú–†–û–¢"
‚ùå –ü–õ–û–•–û:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: –ü—É—Ç–∏–Ω –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –ú–†–û–¢
–ü–µ—Ä–µ—Å–∫–∞–∑: –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏ –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –æ–ø–ª–∞—Ç—ã —Ç—Ä—É–¥–∞.

‚úÖ –•–û–†–û–®–û:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: –ú–†–û–¢ –ø–æ–¥—Ä–æ—Å –Ω–∞ 300 —Ä—É–±–ª–µ–π
–ü–µ—Ä–µ—Å–∫–∞–∑: –ö—Ä–µ–º–ª—å —Ä–µ—à–∏–ª –ø–æ—Ä–∞–¥–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –±–µ–¥–Ω—è–∫–æ–≤ –ø—Ä–∏–±–∞–≤–∫–æ–π, –∫–æ—Ç–æ—Ä–æ–π —Ö–≤–∞—Ç–∏—Ç —Ä–æ–≤–Ω–æ –Ω–∞ –¥–≤–∞ –ø–æ—Ö–æ–¥–∞ –≤ –ú–∞–∫–¥–æ–Ω–∞–ª–¥—Å. –≠–∫–æ–Ω–æ–º–∏—Å—Ç—ã —É–∂–µ –ø–æ–¥—Å—á–∏—Ç–∞–ª–∏, —á—Ç–æ —ç—Ç–æ –ø–æ–∫—Ä–æ–µ—Ç —Ä–æ–≤–Ω–æ —Ç—Ä–µ—Ç—å –∏–Ω—Ñ–ª—è—Ü–∏–∏.

–ò—Å—Ö–æ–¥–Ω–∞—è: "–¢—Ä–∞–º–ø –æ–±—ä—è–≤–∏–ª –æ –ø–æ—à–ª–∏–Ω–∞—Ö –∏–∑-–∑–∞ –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏–∏"
‚ùå –ü–õ–û–•–û:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: –¢—Ä–∞–º–ø –≤–≤–æ–¥–∏—Ç –ø–æ—à–ª–∏–Ω—ã
–ü–µ—Ä–µ—Å–∫–∞–∑: –î–æ–Ω–∞–ª—å–¥ –¢—Ä–∞–º–ø –æ–±—ä—è–≤–∏–ª –æ –≤–≤–µ–¥–µ–Ω–∏–∏ –ø–æ—à–ª–∏–Ω –∏–∑-–∑–∞ —Å–∏—Ç—É–∞—Ü–∏–∏ —Å –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏–µ–π.

‚úÖ –•–û–†–û–®–û:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: –î–∞–Ω–∏—è –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –ø—Ä–æ–¥–∞–≤–∞—Ç—å –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏—é ‚Äì –¢—Ä–∞–º–ø –≤–∫–ª—é—á–∏–ª —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Å–∞–Ω–∫—Ü–∏–∏
–ü–µ—Ä–µ—Å–∫–∞–∑: –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê —Ä–µ—à–∏–ª –Ω–∞–¥–∞–≤–∏—Ç—å –Ω–∞ "–∂–∞–¥–Ω—ã—Ö –¥–∞—Ç—á–∞–Ω" —á–µ—Ä–µ–∑ –∫–æ—à–µ–ª—ë–∫. –ü–æ—à–ª–∏–Ω—ã –∫–æ—Å–Ω—É—Ç—Å—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω –ï–≤—Ä–æ–ø—ã, –∫–æ—Ç–æ—Ä—ã–µ "–º–µ—à–∞—é—Ç —Å–¥–µ–ª–∫–µ –≤–µ–∫–∞". –î–∞–Ω–∏—è –ø–æ–∫–∞ –º–æ–ª—á–∏—Ç, –Ω–æ –µ—ë —ç–∫—Å–ø–æ—Ä—Ç —É–∂–µ –ø–ª–∞—á–µ—Ç.

–í–µ—Ä–Ω–∏ JSON:
{{
  "selected": –Ω–æ–º–µ—Ä (1-{len(news_list[:25])}),
  "title": "–ü–ï–†–ï–ü–ò–°–ê–ù–ù–´–ô –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–∫–æ—Ä–æ—Ç–∫–∏–π, —Ü–µ–ø–ª—è—é—â–∏–π)",
  "summary": "–ü–µ—Ä–µ—Å–∫–∞–∑ —Å –ù–û–í–´–ú–ò —Ñ–∞–∫—Ç–∞–º–∏/–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–±–µ–∑ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞)",
  "hashtags": "2-4 —Ö–µ—à—Ç–µ–≥–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª"
}}

–ö–†–ò–¢–ò–ß–ù–û:
- –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ù–ï –¥–æ–ª–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π
- –ü–µ—Ä–µ—Å–∫–∞–∑ –ù–ï –¥–æ–ª–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
- –ú–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ + –∏—Ä–æ–Ω–∏—è

–ù–æ–≤–æ—Å—Ç–∏:
{news_text}"""
    
    # –í–°–ï –ö–õ–Æ–ß–ò –ß–ï–†–ï–ó OPENROUTER
    api_keys = [
        ("OpenRouter-1", GROQ_API_KEY),
        ("OpenRouter-2", OPENROUTER_API_KEY),
        ("OpenRouter-3", os.getenv("OPENROUTER_API_KEY_2"))
    ]
    
    # –†–ê–ë–û–ß–ò–ï –ë–ï–°–ü–õ–ê–¢–ù–´–ï –ú–û–î–ï–õ–ò (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ 2026)
    models = [
        "nousresearch/hermes-3-llama-3.1-405b:free",
        "meta-llama/llama-3.1-8b-instruct:free",
        "google/gemini-flash-1.5-8b:free",
        "qwen/qwen-2-7b-instruct:free",
    ]
    
    for key_name, api_key in api_keys:
        if not api_key:
            continue
            
        for model in models:
            try:
                log.info(f"   ü§ñ –ü—Ä–æ–±—É—é {key_name} ‚Üí {model}...")
                
                async with aiohttp.ClientSession() as s:
                    headers = {
                        "Authorization": f"Bearer {api_key}", 
                        "Content-Type": "application/json"
                    }
                    payload = {
                        "model": model, 
                        "messages": [{"role": "user", "content": prompt}], 
                        "temperature": 0.9, 
                        "max_tokens": 500
                    }
                    
                    async with s.post("https://openrouter.ai/api/v1/chat/completions",
                                     headers=headers, json=payload, 
                                     timeout=aiohttp.ClientTimeout(total=30)) as r:
                        if r.status == 200:
                            data = await r.json()
                            content = data["choices"][0]["message"]["content"].strip()
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON
                            json_start = content.find('{')
                            json_end = content.rfind('}')
                            if json_start != -1 and json_end != -1:
                                content = content[json_start:json_end+1]
                            
                            result = json.loads(content)
                            selected_idx = int(result.get("selected", 1)) - 1
                            
                            if 0 <= selected_idx < len(news_list):
                                selected_news = news_list[selected_idx]
                                selected_news["ai_title"] = result.get("title", selected_news["title"])
                                selected_news["summary"] = result.get("summary", "")
                                selected_news["hashtags"] = result.get("hashtags", "")
                                log.info(f"‚úÖ {key_name}/{model} –≤—ã–±—Ä–∞–ª #{selected_idx+1}")
                                log.info(f"   üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫: {selected_news['ai_title'][:60]}")
                                return selected_news
                        else:
                            error_text = await r.text()
                            log.warning(f"‚ö†Ô∏è {key_name}/{model} HTTP {r.status}: {error_text[:150]}")
                            
            except asyncio.TimeoutError:
                log.warning(f"‚ö†Ô∏è {key_name}/{model} timeout")
                continue
            except Exception as e:
                log.warning(f"‚ö†Ô∏è {key_name}/{model} error: {e}")
                continue
    
    log.warning("‚ö†Ô∏è –í—Å–µ AI –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –¥–µ–ª–∞—é fallback —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º")
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å
    priority_keywords = ['—Ç—Ä–∞–º–ø', '–ø—É—Ç–∏–Ω', '–≤–æ–π–Ω–∞', '–≤–∑—Ä—ã–≤', '–¥–æ–ª–ª–∞—Ä', '—Å–∞–Ω–∫—Ü', '–∞—Ä–µ—Å—Ç']
    scored = []
    for news in news_list[:10]:
        score = sum(1 for kw in priority_keywords if kw in news["title"].lower())
        scored.append((score, news))
    
    scored.sort(key=lambda x: x[0], reverse=True)
    selected = scored[0][1] if scored else random.choice(news_list[:5])
    
    # –î–ï–õ–ê–ï–ú –ù–û–†–ú–ê–õ–¨–ù–´–ô –ü–ï–†–ï–°–ö–ê–ó –ë–ï–ó –î–£–ë–õ–Ø–ñ–ê
    original_title = selected["title"]
    desc = selected["desc"] if selected["desc"] else ""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
    sentences = re.split(r'[.!?]\s+', desc)
    
    # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    unique_sentences = []
    for sent in sentences:
        if len(sent) > 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
            words_in_title = set(original_title.lower().split())
            words_in_sent = set(sent.lower().split())
            overlap = len(words_in_title & words_in_sent) / max(len(words_in_sent), 1)
            
            if overlap < 0.5:  # –ú–µ–Ω—å—à–µ 50% —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                unique_sentences.append(sent)
    
    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 2 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if unique_sentences:
        summary = '. '.join(unique_sentences[:2]) + '.'
    else:
        # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è - –¥–µ–ª–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        summary = f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞ –≤—ã—è—Å–Ω—è—é—Ç—Å—è. –°–∏—Ç—É–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º."
    
    # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
    if len(summary) > 300:
        summary = summary[:297] + '...'
    
    selected["ai_title"] = original_title  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    selected["summary"] = summary
    selected["hashtags"] = generate_smart_hashtags(original_title, desc)
    
    log.info(f"   üìù Fallback –≤—ã–±—Ä–∞–ª: {original_title[:60]}")
    log.info(f"   üìù –ü–µ—Ä–µ—Å–∫–∞–∑: {summary[:80]}...")
    
    return selected

# ================== –°–ë–û–† –ù–û–í–û–°–¢–ï–ô + –ü–ê–†–°–ò–ù–ì –ö–ê–†–¢–ò–ù–û–ö ==================
async def collect_fresh_news(limit=30):
    candidates = []
    sources = list(RSS_SOURCES.items())
    random.shuffle(sources)
    
    for source_name, rss_url in sources:
        if len(candidates) >= limit: break
        
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                if len(candidates) >= limit: break
                
                title = entry.title.strip()
                url = entry.link
                desc = entry.get("summary", "") or entry.get("description", "") or ""
                
                title = BeautifulSoup(title, "html.parser").get_text()
                desc = BeautifulSoup(desc, "html.parser").get_text()
                
                # ========== –ü–ê–†–°–ò–ú –ö–ê–†–¢–ò–ù–ö–£ –ò–ó RSS ==========
                rss_image = None
                
                # 1. –ü—Ä–æ–±—É–µ–º media:content
                if hasattr(entry, 'media_content') and entry.media_content:
                    rss_image = entry.media_content[0].get('url')
                
                # 2. –ü—Ä–æ–±—É–µ–º enclosure
                if not rss_image and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            rss_image = enc.get('href')
                            break
                
                # 3. –ò—â–µ–º <img> –≤ description
                if not rss_image:
                    soup = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        rss_image = img_tag['src']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                if rss_image:
                    if not rss_image.startswith('http'):
                        rss_image = None
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –Ω–µ –æ–±—Ä–µ–∑–∞–Ω (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞)
                    elif len(rss_image) < 30:
                        rss_image = None
                        log.debug(f"   ‚ö†Ô∏è RSS –∫–∞—Ä—Ç–∏–Ω–∫–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è: {rss_image}")
                
                if len(title) < 20: continue
                if is_duplicate(title, url): continue
                if any(boring in title.lower() for boring in BORING_KEYWORDS): continue
                if not any(k in title.lower() for k in KEYWORDS): continue
                
                candidates.append({
                    "title": title, 
                    "url": url, 
                    "desc": desc, 
                    "source": source_name,
                    "rss_image": rss_image  # –°–û–•–†–ê–ù–Ø–ï–ú –ö–ê–†–¢–ò–ù–ö–£ –ò–ó RSS
                })
                
        except Exception as e:
            log.error(f"RSS {source_name}: {e}")
    
    return candidates

# ================== –£–ú–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–û–ò–°–ö–ê –ö–ê–†–¢–ò–ù–û–ö ==================

def extract_keywords_for_image_search(title: str, description: str = "") -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
    text = f"{title} {description}".lower()
    queries = []
    
    # –ì–ï–û–ì–†–ê–§–ò–Ø - —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ
    places = {
        '–≥—Ä–µ–Ω–ª–∞–Ω–¥': ['greenland ice', 'greenland landscape', 'arctic greenland'],
        '–∏—Å–ª–∞–Ω–¥': ['iceland volcano', 'iceland nature', 'reykjavik'],
        '–Ω–æ—Ä–≤–µ–≥': ['norway fjord', 'norway landscape'],
        '—à–≤–µ—Ü': ['sweden stockholm', 'sweden flag'],
        '–¥–∞–Ω': ['denmark copenhagen', 'denmark flag'],
        '—Å—ã–∫—Ç—ã–≤–∫–∞—Ä': ['russian city', 'komi republic russia'],
        '–º–æ—Å–∫–≤': ['moscow kremlin', 'red square moscow'],
        '–ø–µ—Ç–µ—Ä–±—É—Ä–≥': ['saint petersburg', 'hermitage russia'],
        '–∫–∏–µ–≤': ['kyiv ukraine', 'kiev city'],
        '—É–∫—Ä–∞–∏–Ω': ['ukraine flag', 'ukraine country'],
        '–≤–∞—à–∏–Ω–≥—Ç–æ–Ω': ['washington dc', 'white house', 'capitol building'],
        '–Ω—å—é-–π–æ—Ä–∫': ['new york city', 'manhattan skyline'],
        '–ª–æ–Ω–¥–æ–Ω': ['london big ben', 'london eye'],
        '–ø–∞—Ä–∏–∂': ['paris eiffel tower', 'paris france'],
        '–±–µ—Ä–ª–∏–Ω': ['berlin brandenburg gate', 'berlin germany'],
        '–ø–µ–∫–∏–Ω': ['beijing forbidden city', 'beijing china'],
        '—Ç–æ–∫–∏–æ': ['tokyo japan', 'tokyo tower'],
    }
    
    for key, search_terms in places.items():
        if key in text:
            queries.extend(search_terms)
            break
    
    # –ü–ï–†–°–û–ù–´
    if '—Ç—Ä–∞–º–ø' in text: queries.append('donald trump president')
    if '–ø—É—Ç–∏–Ω' in text: queries.append('vladimir putin')
    if '–±–∞–π–¥–µ–Ω' in text: queries.append('joe biden')
    if '–∑–µ–ª–µ–Ω—Å–∫' in text: queries.append('zelensky ukraine')
    
    # –°–û–ë–´–¢–ò–Ø
    if '–≤–∑—Ä—ã–≤' in text: queries.extend(['explosion fire', 'emergency disaster'])
    if '–ø–æ–∂–∞—Ä' in text: queries.extend(['fire building', 'firefighters'])
    if '–¥–æ–ª–ª–∞—Ä' in text or '–∫—É—Ä—Å' in text: queries.extend(['us dollar bills', 'currency money'])
    if '–≤–æ–π–Ω–∞' in text: queries.extend(['military conflict', 'war soldiers'])
    if '–Ω–µ—Ñ—Ç—å' in text: queries.append('oil refinery petroleum')
    if '–≥–∞–∑' in text: queries.append('natural gas pipeline')
    if '–∫–æ—Å–º–æ—Å' in text or '—Ä–∞–∫–µ—Ç' in text: queries.extend(['rocket launch', 'space exploration'])
    if '–∏–∏' in text or '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω' in text: queries.extend(['artificial intelligence', 'ai technology'])
    
    if not queries:
        queries.append('breaking news')
    
    return queries[:4]  # –¢–æ–ø-4 –∑–∞–ø—Ä–æ—Å–∞

async def search_unsplash_with_retries(query: str, retries=2) -> str:
    """–ò—â–µ—Ç –Ω–∞ Unsplash —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    if not UNSPLASH_ACCESS_KEY:
        return None
    
    for attempt in range(retries):
        try:
            url = "https://api.unsplash.com/search/photos"
            params = {
                "query": query,
                "per_page": 30,
                "orientation": "landscape",
                "order_by": "relevant",
            }
            headers = {
                "Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}",
                "Accept-Version": "v1"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=headers, 
                                      timeout=aiohttp.ClientTimeout(total=10)) as r:
                    if r.status == 200:
                        data = await r.json()
                        results = data.get("results", [])
                        
                        if results and len(results) > 0:
                            recent = get_recent_images()
                            available = [
                                photo["urls"]["regular"] 
                                for photo in results[:20]
                                if photo["urls"]["regular"] not in recent
                            ]
                            
                            if available:
                                selected = random.choice(available)
                                log.info(f"   ‚úÖ Unsplash –Ω–∞—à—ë–ª –ø–æ '{query}': {len(available)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
                                return selected
                    elif r.status == 403:
                        log.error(f"   ‚ùå Unsplash API –ª–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω")
                        return None
                    elif r.status == 401:
                        log.error(f"   ‚ùå Unsplash API –∫–ª—é—á –Ω–µ–≤–µ—Ä–Ω—ã–π")
                        return None
                        
        except asyncio.TimeoutError:
            log.warning(f"   ‚è±Ô∏è Unsplash timeout (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{retries})")
            if attempt < retries - 1:
                await asyncio.sleep(1)
        except Exception as e:
            log.debug(f"   ‚ö†Ô∏è Unsplash '{query}' error: {e}")
            if attempt < retries - 1:
                await asyncio.sleep(1)
    
    return None

async def get_perfect_image(title: str, description: str = "", rss_image: str = None) -> str:
    """
    –ü–†–ò–û–†–ò–¢–ï–¢–´ –ü–û–ò–°–ö–ê –ö–ê–†–¢–ò–ù–ö–ò:
    1. –ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ RSS (–µ—Å–ª–∏ –µ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–Ω–∞)
    2. Fallback –ø—É–ª (Unsplash –æ—Ç–∫–ª—é—á—ë–Ω, –Ω–µ—Ç –∫–ª—é—á–∞)
    """
    
    # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ RSS
    if rss_image and len(rss_image) > 50:
        log.info(f"   üéØ –ü—Ä–æ–≤–µ—Ä—è—é RSS –∫–∞—Ä—Ç–∏–Ω–∫—É: {rss_image[:80]}...")
        
        img_data = await download_image(rss_image)
        if img_data and len(img_data) > 5000:
            recent = get_recent_images()
            if rss_image not in recent:
                track_used_image(rss_image)
                log.info(f"   ‚úÖ RSS –∫–∞—Ä—Ç–∏–Ω–∫–∞ –û–ö ({len(img_data)//1024}KB)")
                return rss_image
            else:
                log.info(f"   ‚ö†Ô∏è RSS –∫–∞—Ä—Ç–∏–Ω–∫–∞ —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å")
        else:
            log.warning(f"   ‚ùå RSS –∫–∞—Ä—Ç–∏–Ω–∫–∞ –±–∏—Ç–∞—è")
    
    # –ü–†–ò–û–†–ò–¢–ï–¢ 2: Fallback (Unsplash –æ—Ç–∫–ª—é—á—ë–Ω)
    log.info("   üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—É–ª")
    return get_fallback_image(f"{title} {description}".lower())

def get_fallback_image(text: str) -> str:
    """–û–≥—Ä–æ–º–Ω—ã–π –ø—É–ª —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫"""
    
    pools = {
        'greenland': [
            "https://images.unsplash.com/photo-1531366936337-7c912a4589a7",
            "https://images.unsplash.com/photo-1583422409516-2895a77efded",
            "https://images.unsplash.com/photo-1528127269322-539801943592",
        ],
        'usa': [
            "https://images.unsplash.com/photo-1529107386315-e1a2ed48e620",
            "https://images.unsplash.com/photo-1485081669829-bacb8c7bb1f3",
            "https://images.unsplash.com/photo-1563306406-e66174fa3787",
            "https://images.unsplash.com/photo-1509024644558-2f56ce76c490",
            "https://images.unsplash.com/photo-1566073771259-6a8506099945",
        ],
        'russia': [
            "https://images.unsplash.com/photo-1513326738677-b964603b136d",
            "https://images.unsplash.com/photo-1520106212299-d99c443e4568",
            "https://images.unsplash.com/photo-1547448415-e9f5b28e570d",
            "https://images.unsplash.com/photo-1523906834658-6e24ef2386f9",
        ],
        'ukraine': [
            "https://images.unsplash.com/photo-1562077772-3bd90403f7f0",
            "https://images.unsplash.com/photo-1599930113854-d6d7fd521f10",
        ],
        'war': [
            "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5",
            "https://images.unsplash.com/photo-1580982172477-9373ff52ae43",
            "https://images.unsplash.com/photo-1562007908-17c67e878c88",
        ],
        'finance': [
            "https://images.unsplash.com/photo-1611974789855-9c2a0a7236a3",
            "https://images.unsplash.com/photo-1460925895917-afdab827c52f",
            "https://images.unsplash.com/photo-1579621970563-ebec7560ff3e",
        ],
        'general': [
            "https://images.unsplash.com/photo-1504711434969-e33886168f5c",
            "https://images.unsplash.com/photo-1495020689067-958852a7765e",
            "https://images.unsplash.com/photo-1586339949916-3e9457bef6d3",
        ]
    }
    
    # –í—ã–±–æ—Ä –ø—É–ª–∞
    if '–≥—Ä–µ–Ω–ª–∞–Ω–¥' in text:
        pool = pools['greenland']
    elif any(w in text for w in ['—Ç—Ä–∞–º–ø', '—Å—à–∞', '–∞–º–µ—Ä–∏–∫']):
        pool = pools['usa']
    elif any(w in text for w in ['–ø—É—Ç–∏–Ω', '—Ä–æ—Å—Å–∏—è', '–∫—Ä–µ–º–ª']):
        pool = pools['russia']
    elif '—É–∫—Ä–∞–∏–Ω' in text:
        pool = pools['ukraine']
    elif '–≤–æ–π–Ω–∞' in text:
        pool = pools['war']
    elif any(w in text for w in ['–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª—å', '–∫—É—Ä—Å']):
        pool = pools['finance']
    else:
        pool = pools['general']
    
    recent = get_recent_images()
    available = [img for img in pool if img not in recent]
    
    if not available:
        available = pool
    
    selected = random.choice(available)
    track_used_image(selected)
    return selected

async def download_image(url: str):
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    return await r.read()
    except:
        pass
    return None

def generate_smart_hashtags(title: str, description: str = "") -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à—Ç–µ–≥–∏"""
    text = f"{title} {description}".lower()
    tags = []
    
    if any(w in text for w in ['–ø—É—Ç–∏–Ω', '–∫—Ä–µ–º–ª']): tags.append('#–ü—É—Ç–∏–Ω')
    if '—Ç—Ä–∞–º–ø' in text: tags.append('#–¢—Ä–∞–º–ø')
    if '—Å—à–∞' in text: tags.append('#–°–®–ê')
    if '—É–∫—Ä–∞–∏–Ω' in text: tags.append('#–£–∫—Ä–∞–∏–Ω–∞')
    if any(w in text for w in ['—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä']): tags.append('#–≤–∞–ª—é—Ç–∞')
    if '–≤–æ–π–Ω–∞' in text: tags.append('#–≤–æ–π–Ω–∞')
    if any(w in text for w in ['–≤–∑—Ä—ã–≤', '–ø–æ–∂–∞—Ä']): tags.append('#–ß–ü')
    if any(w in text for w in ['–∞—Ä–µ—Å—Ç', '—Å—É–¥']): tags.append('#–∫—Ä–∏–º–∏–Ω–∞–ª')
    
    if not tags:
        tags.append('#–Ω–æ–≤–æ—Å—Ç–∏')
    
    return ' '.join(tags[:4])

# ================== –ü–û–°–¢–ò–ù–ì ==================
async def post_selected_news(news):
    title = news.get("ai_title", news["title"])
    url = news["url"]
    summary = news.get("summary", "")
    hashtags = news.get("hashtags", "")
    desc = news.get("desc", "")
    rss_image = news.get("rss_image")
    
    hashtags = re.sub(r'@\w+', '', hashtags).strip()
    
    if not hashtags:
        hashtags = generate_smart_hashtags(title, desc)
    
    # –§–û–†–ú–ê–¢ –ö–ê–ö –¢–´ –•–û–ß–ï–®–¨
    caption = f"**{title}**\n\n{summary}\n\n{hashtags}"
    
    log.info(f"   üì∞ –ü–û–°–¢:")
    log.info(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
    log.info(f"   –ü–µ—Ä–µ—Å–∫–∞–∑: {summary[:100]}...")
    log.info(f"   –•–µ—à—Ç–µ–≥–∏: {hashtags}")
    
    log.info(f"   üé® –ò—â—É –∏–¥–µ–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É...")
    img_url = await get_perfect_image(title, desc, rss_image)
    
    img_data = await download_image(img_url)
    
    for attempt in range(3):
        try:
            if img_data and len(img_data) > 1024:
                file = BufferedInputFile(img_data, filename="news.jpg")
                await bot.send_photo(CHANNEL_ID, file, caption=caption, parse_mode=ParseMode.MARKDOWN)
            else:
                if attempt == 0:
                    log.warning("   ‚ö†Ô∏è –ë–∏—Ç–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞, –ø—Ä–æ–±—É—é fallback")
                    img_url = get_fallback_image(f"{title} {desc}".lower())
                    img_data = await download_image(img_url)
                    continue
                else:
                    await bot.send_message(CHANNEL_ID, caption, parse_mode=ParseMode.MARKDOWN)
            
            save_posted(news["title"], url)
            increment_stat()
            log.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}")
            return True
        except Exception as e:
            if attempt == 2:
                log.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                return False
            await asyncio.sleep(2)
    
    return False

# ================== –¶–ò–ö–õ ==================
async def check_news():
    stats = get_today_stats()
    if stats["normal"] >= 25:
        log.info("üìä –õ–∏–º–∏—Ç 25 –ø–æ—Å—Ç–æ–≤")
        return
    
    log.info("üì• –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏...")
    candidates = await collect_fresh_news(30)
    
    if not candidates:
        log.info("‚ö†Ô∏è –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç")
        return
    
    log.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    
    selected = await ai_select_and_summarize(candidates)
    
    if not selected:
        log.warning("‚ö†Ô∏è AI –Ω–µ –≤—ã–±—Ä–∞–ª –Ω–æ–≤–æ—Å—Ç—å")
        return
    
    await post_selected_news(selected)

async def news_loop():
    log.info("‚è∞ –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
    await asyncio.sleep(5)
    
    while True:
        await check_news()
        next_interval = random.randint(20, 70)
        log.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ {next_interval} –º–∏–Ω")
        await asyncio.sleep(next_interval * 60)

# ================== YOUTUBE ==================
def has_cyrillic(text):
    return bool(re.search('[–∞-—è–ê-–Ø—ë–Å]', text))

def has_ukrainian(text):
    return any(l in text for l in ['—î', '—ñ', '—ó', '“ë', '–Ñ', '–Ü', '–á', '“ê'])

def is_russian_content(title, channel_title, description=""):
    full_text = f"{title} {channel_title} {description}".lower()
    
    if not has_cyrillic(title + channel_title):
        return False
    
    if has_ukrainian(title + channel_title + description):
        return False
    
    ua_keywords = ['—É–∫—Ä–∞—ó–Ω', 'ukrainian', 'kiev', 'kyiv', '–∫–∏—ó–≤', '–∑–µ–ª–µ–Ω—Å—å–∫', 'zelensky', '–∞–∑–æ–≤', '–≤—Å—É', '–∑—Å—É']
    if any(kw in full_text for kw in ua_keywords):
        return False
    
    return True

def is_trusted_news_channel(channel_title):
    return any(t.lower() in channel_title.lower() for t in RU_NEWS_CHANNELS)

def is_any_news_related(title: str, channel: str, description: str = "") -> bool:
    text = f"{title} {channel} {description}".lower()
    
    if is_trusted_news_channel(channel):
        return True
    
    news_keywords = [
        '–Ω–æ–≤–æ—Å—Ç', '—Å–µ–≥–æ–¥–Ω—è', '—Å—Ä–æ—á–Ω', '–≥–ª–∞–≤–Ω–æ–µ', '–∏—Ç–æ–≥–∏',
        '–ø—É—Ç–∏–Ω', '—Ä–æ—Å—Å–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–º–∏–Ω–∏—Å—Ç—Ä',
        '–∫—Ä–µ–º–ª—å', '–¥—É–º–∞', '–ø–æ–ª–∏—Ç–∏–∫', '–∑–∞–∫–æ–Ω', '—Ä–µ—Ñ–æ—Ä–º',
        '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '—Å—à–∞', '—É–∫—Ä–∞–∏–Ω', '–≤–æ–π–Ω–∞', '–º–∏—Ä',
        '–µ–≤—Ä–æ–ø', '–∫–∏—Ç–∞–π', '–Ω–∞—Ç–æ',
        '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–∫—É—Ä—Å', '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü',
        '—Ü–µ–Ω—ã', '–∑–∞—Ä–ø–ª–∞—Ç', '–ø–µ–Ω—Å–∏', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑',
        '–∑–∞—è–≤–∏–ª', '–æ–±—ä—è–≤–∏–ª', '—Å–æ–æ–±—â–∏–ª', '–ø—Ä–æ–∏–∑–æ—à–ª', '—Å–ª—É—á–∏–ª',
        '—Ä–µ—à–∏–ª', '–ø–æ–¥–ø–∏—Å–∞–ª', '–ø—Ä–∏–Ω—è–ª',
        '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤', '–∞–≤–∞—Ä–∏—è', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç',
        '–≤–∞–∂–Ω', '–≥–ª–∞–≤–Ω', '—Å–∫–∞–Ω–¥–∞–ª', '—Å–µ–Ω—Å–∞—Ü'
    ]
    
    matches = sum(1 for kw in news_keywords if kw in text)
    return matches >= 1

def parse_duration_to_seconds(iso_duration):
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, iso_duration)
    
    if not match:
        return 0
    
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    
    return hours * 3600 + minutes * 60 + seconds

def format_views(views):
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}–ú"
    elif views >= 1_000:
        return f"{views / 1_000:.1f}–ö"
    else:
        return str(views)

async def search_diverse_shorts():
    log.info("üîç –ü–æ–∏—Å–∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö Shorts...")
    
    all_shorts = []
    diverse_queries = [
        "–Ω–æ–≤–æ—Å—Ç–∏ —Ä–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è",
        "–ø–æ–ª–∏—Ç–∏–∫–∞ –ø—É—Ç–∏–Ω –∫—Ä–µ–º–ª—å",
        "–ø—É—Ç–∏–Ω –∑–∞—è–≤–∏–ª",
        "—Ç—Ä–∞–º–ø –Ω–æ–≤–æ—Å—Ç–∏",
        "—É–∫—Ä–∞–∏–Ω–∞ –≤–æ–π–Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏",
        "–º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Ä—É–±–ª—å",
        "—ç–∫–æ–Ω–æ–º–∏–∫–∞ —Ä–æ—Å—Å–∏–∏",
        "—Ä–æ—Å—Å–∏—è –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è",
        "–≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–Ω—è",
    ]
    
    for query in diverse_queries[:8]:
        try:
            log.info(f"   üîé '{query}'...")
            
            url = "https://www.googleapis.com/youtube/v3/search"
            params = {
                "part": "id,snippet",
                "q": query + " shorts",
                "type": "video",
                "maxResults": 40,
                "order": "viewCount",
                "publishedAfter": (datetime.now() - timedelta(days=3)).isoformat() + "Z",
                "regionCode": "RU",
                "relevanceLanguage": "ru",
                "videoCategoryId": "25",
                "key": YOUTUBE_API_KEY
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=15)) as response:
                    if response.status != 200:
                        continue
                    
                    data = await response.json()
                    video_ids = [item["id"]["videoId"] for item in data.get("items", []) 
                                if item["id"].get("kind") == "youtube#video"]
                    
                    if not video_ids:
                        continue
                    
                    details_url = "https://www.googleapis.com/youtube/v3/videos"
                    details_params = {
                        "part": "snippet,statistics,contentDetails",
                        "id": ",".join(video_ids[:50]),
                        "key": YOUTUBE_API_KEY
                    }
                    
                    async with session.get(details_url, params=details_params, 
                                          timeout=aiohttp.ClientTimeout(total=15)) as resp:
                        if resp.status != 200:
                            continue
                        
                        details_data = await resp.json()
                        
                        for item in details_data.get("items", []):
                            try:
                                duration = item["contentDetails"]["duration"]
                                total_sec = parse_duration_to_seconds(duration)
                                
                                if not (8 <= total_sec <= 65):
                                    continue
                                
                                snippet = item["snippet"]
                                stats = item["statistics"]
                                
                                title = snippet.get("title", "")
                                channel_title = snippet.get("channelTitle", "")
                                description = snippet.get("description", "")
                                
                                if not is_russian_content(title, channel_title, description):
                                    continue
                                
                                if not is_any_news_related(title, channel_title, description):
                                    continue
                                
                                views = int(stats.get("viewCount", 0))
                                
                                min_views = 2000 if is_trusted_news_channel(channel_title) else 5000
                                if views < min_views:
                                    continue
                                
                                all_shorts.append({
                                    "id": item["id"],
                                    "title": title,
                                    "channel": channel_title,
                                    "views": views,
                                    "likes": int(stats.get("likeCount", 0)),
                                    "duration_sec": total_sec,
                                    "url": f"https://youtube.com/shorts/{item['id']}",
                                    "is_trusted": is_trusted_news_channel(channel_title)
                                })
                                
                            except Exception as e:
                                continue
            
            await asyncio.sleep(0.4)
            
        except Exception as e:
            log.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
            continue
    
    seen_ids = set()
    unique_shorts = []
    for short in all_shorts:
        if short["id"] not in seen_ids:
            seen_ids.add(short["id"])
            unique_shorts.append(short)
    
    unique_shorts.sort(key=lambda x: (not x["is_trusted"], -x["views"]))
    
    log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_shorts)} —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö Shorts")
    return unique_shorts

async def download_shorts_video(video_id):
    output_file = os.path.join(TEMP_DIR, f"shorts_{video_id}.mp4")
    
    try:
        log.info("   üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ yt-dlp...")
        
        url = f"https://www.youtube.com/watch?v={video_id}"
        
        cmd = [
            sys.executable,
            "-m", "yt_dlp",
            "-f", "bv*+ba/b",
            "-o", output_file,
            "--no-playlist",
            "--merge-output-format", "mp4",
            "--extractor-args", "youtube:player_client=android",
            "--no-check-certificate",
            "--socket-timeout", "30",
            url
        ]
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await asyncio.wait_for(
            process.communicate(), 
            timeout=90
        )
        
        if process.returncode == 0 and os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / 1024 / 1024
            log.info(f"   ‚úÖ –°–∫–∞—á–∞–Ω–æ {file_size:.1f} MB")
            return output_file
        else:
            if os.path.exists(output_file):
                os.remove(output_file)
            return None
            
    except Exception as e:
        log.error(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        if os.path.exists(output_file):
            os.remove(output_file)
        return None

async def post_youtube_shorts():
    log.info("üé¨ –ó–∞–ø—É—Å–∫: YouTube Shorts (19:00)...")
    
    shorts = await search_diverse_shorts()
    
    if not shorts:
        log.warning("‚ö†Ô∏è Shorts –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    for i, short_video in enumerate(shorts[:10], 1):
        if is_youtube_posted_today(short_video["id"]):
            log.info(f"   [{i}/10] ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –ø–æ—Å—Ç–∏–ª–∏): {short_video['title'][:50]}")
            continue
        
        trust_badge = "‚≠ê" if short_video["is_trusted"] else ""
        log.info(f"üéØ [{i}/10] {trust_badge} {short_video['title'][:60]}...")
        log.info(f"   üëÄ {format_views(short_video['views'])} | üì∫ {short_video['channel']}")
        
        video_file_path = await download_shorts_video(short_video['id'])
        
        if not video_file_path:
            log.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â–∏–π...")
            continue
        
        try:
            caption = (
                f"‚ö° **–ì–ª–∞–≤–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π Shorts –¥–Ω—è**\n\n"
                f"**{short_video['title']}**\n\n"
                f"üì∫ {short_video['channel']}\n"
                f"üëÄ {format_views(short_video['views'])} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ | "
                f"‚ù§Ô∏è {format_views(short_video['likes'])}\n\n"
                f"#shorts #–Ω–æ–≤–æ—Å—Ç–∏"
            )
            
            with open(video_file_path, 'rb') as f:
                video_data = f.read()
            
            video_file = BufferedInputFile(
                video_data, 
                filename=f"{short_video['id']}.mp4"
            )
            
            await bot.send_video(
                CHANNEL_ID,
                video=video_file,
                caption=caption,
                parse_mode=ParseMode.MARKDOWN,
                supports_streaming=True,
                width=1080,
                height=1920
            )
            
            save_youtube_posted(short_video['id'], 'shorts')
            log.info("‚úÖ YouTube Shorts –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
            
            os.remove(video_file_path)
            log.info(f"üóëÔ∏è –§–∞–π–ª —É–¥–∞–ª—ë–Ω: {video_file_path}")
            
            return True
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            
            if os.path.exists(video_file_path):
                os.remove(video_file_path)
                log.info(f"üóëÔ∏è –§–∞–π–ª —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏")
            
            continue
    
    log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–æ—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω Shorts –∏–∑ —Ç–æ–ø-10")
    return False

def cleanup_old_files():
    try:
        now = datetime.now().timestamp()
        for filename in os.listdir(TEMP_DIR):
            filepath = os.path.join(TEMP_DIR, filename)
            
            if os.path.isfile(filepath):
                file_age = now - os.path.getmtime(filepath)
                
                if file_age > 86400:
                    os.remove(filepath)
                    log.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {filename}")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

# ================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ==================
async def main():
    scheduler = AsyncIOScheduler(timezone=TIMEZONE)
    
    # YouTube Shorts - 3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å (—É—Ç—Ä–æ, –≤–µ—á–µ—Ä, –Ω–æ—á—å)
    scheduler.add_job(post_youtube_shorts, "cron", hour=9, minute=0, name="shorts_morning")
    scheduler.add_job(post_youtube_shorts, "cron", hour=19, minute=0, name="shorts_evening")
    scheduler.add_job(post_youtube_shorts, "cron", hour=22, minute=0, name="shorts_night")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
    scheduler.add_job(cleanup_old_files, "cron", hour=3, minute=0)
    
    scheduler.start()
    
    log.info("=" * 70)
    log.info("ü§ñ –ù–û–í–û–°–¢–ù–û–ô –ë–û–¢ –ó–ê–ü–£–©–ï–ù")
    log.info("=" * 70)
    log.info("üì∞ –ù–æ–≤–æ—Å—Ç–∏: –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å)")
    log.info("üé¨ YouTube Shorts: 3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å (9:00, 19:00, 22:00)")
    log.info("üé® –ö–∞—Ä—Ç–∏–Ω–∫–∏:")
    log.info("    1Ô∏è‚É£ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∏–∑ RSS —Ñ–∏–¥–∞")
    log.info("    2Ô∏è‚É£ Fallback: —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—É–ª Unsplash")
    log.info("    ‚ö†Ô∏è Unsplash API –æ—Ç–∫–ª—é—á—ë–Ω (–Ω–µ—Ç –∫–ª—é—á–∞)")
    log.info("ü§ñ AI: —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã (OpenRouter)")
    log.info("‚ôªÔ∏è –†–æ—Ç–∞—Ü–∏—è: –Ω–∏–∫–∞–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ 24 —á–∞—Å–∞")
    log.info(f"üì° RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(RSS_SOURCES)}")
    log.info("=" * 70)
    
    await news_loop()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("üëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        conn.close()
    except Exception as e:
        log.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        conn.close()
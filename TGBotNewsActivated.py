import os
import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
import sys
from dotenv import load_dotenv
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile
from apscheduler.schedulers.asyncio import AsyncIOScheduler

load_dotenv()

# ================== CONFIG ==================
BOT_TOKEN = os.getenv("BOT_TOKEN")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
PEXELS_API_KEY = os.getenv("PEXELS_API_KEY")

CHANNEL_ID = '@bulmyash'
TIMEZONE = "Europe/Moscow"

if sys.platform == "win32":
    TEMP_DIR = "C:/temp/shorts"
else:
    TEMP_DIR = "/tmp/shorts"
os.makedirs(TEMP_DIR, exist_ok=True)

RSS_SOURCES = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "lenta": "https://lenta.ru/rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rt": "https://www.rt.com/rss/",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "rosbalt": "https://www.rosbalt.ru/feed/",
    "forbes": "https://www.forbes.ru/newrss.xml",
    "rbc_economics": "https://rssexport.rbc.ru/rbcnews/news/20/full.rss",
    "cnews": "https://www.cnews.ru/inc/rss/news.xml",
    "habr": "https://habr.com/ru/rss/all/all/",
    "bbc_ru": "https://feeds.bbci.co.uk/russian/rss.xml",
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "meduza": "https://meduza.io/rss/all",
}

KEYWORDS = [
    '–ø—É—Ç–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–∫—Ä–µ–º–ª', '–≥–æ—Å–¥—É–º', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', 
    '–º–∏–Ω–∏—Å—Ç—Ä', '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '—Å—à–∞', '–∫–∏—Ç–∞–π',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∫—É—Ä—Å', '—Ü–±', '–±–∞–Ω–∫', '–∏–Ω—Ñ–ª—è—Ü',
    '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—Å–∞–Ω–∫—Ü', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–∞—Ä–º–∏—è',
    '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫', '–∞–≤–∞—Ä', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'google', 'apple',
    '—É—á–µ–Ω', '–∫–æ—Å–º–æ—Å', '–≤—ã–±–æ—Ä', '–∑–∞–∫–æ–Ω', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç'
]

BORING_KEYWORDS = [
    '–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã',
    '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π', '—Å–æ–Ω–Ω–∏–∫', '–ø—Ä–∏–º–µ—Ç—ã', '–∏–º–µ–Ω–∏–Ω—ã',
    '—Å—Ç–∞–∂–∏—Ä–æ–≤–∫', '–æ–±–µ—Å–ø–µ—á–∏—Ç—å', '–ø–æ—Ä—É—á–∏–ª',
]

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ù–û–í–û–°–¢–ù–´–• –ö–ê–ù–ê–õ–û–í
RU_NEWS_CHANNELS = [
    # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –°–ú–ò
    "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏", "–¢–ê–°–°", "–ò–∑–≤–µ—Å—Ç–∏—è", "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å", "–†–ë–ö",
    "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä", "–í–µ–¥–æ–º–æ—Å—Ç–∏", "–ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª", "–†–æ—Å—Å–∏—è 24",
    "–ù–¢–í", "RT", "–î–ï–ù–¨ –¢–í", "–ö—Ä–µ–º–ª—å", 
    # –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ
    "–î–æ–∂–¥—å", "–ú–µ–¥—É–∑–∞", "–ù–æ–≤–∞—è –≥–∞–∑–µ—Ç–∞",
    # –ë–ª–æ–≥–µ—Ä—ã/–∞–≤—Ç–æ—Ä—ã
    "–≤–î—É–¥—å", "–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞", "–§–ï–ô–ì–ò–ù LIVE", 
    "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ", "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ Shorts",
    # –ù–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    "–†–µ–¥–∞–∫—Ü–∏—è", "–ù–∞–≤–∞–ª—å–Ω—ã–π LIVE", "Varlamov", "Varlamov News",
    "Soloviev LIVE", "–°–æ–ª–æ–≤—å—ë–≤ LIVE", "60 –º–∏–Ω—É—Ç",
    "–¶–∞—Ä—å–≥—Ä–∞–¥ –¢–í", "–°–ø—É—Ç–Ω–∏–∫", "Life", "–õ–∞–π—Ñ",
    "Mash", "Shot", "112", "Baza", "–ë–∞–∑–∞",
    "Readovka", "WarGonzo", "Rybar", "–†—ã–±–∞—Ä—å",
    "BRIEF", "–ù–µ–∑—ã–≥–∞—Ä—å", "–ü–æ–¥—ä—ë–º", "–ù–æ–≤–æ—Å—Ç–∏",
    "–ü–æ–ª–∏—Ç–∏–∫–∞ —Å–µ–≥–æ–¥–Ω—è", "–†–æ—Å—Å–∏—è 1", "–û–¢–†",
    "–≠—Ö–æ", "The Insider", "–í–∞–∂–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏",
]

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

c.execute('''CREATE TABLE IF NOT EXISTS posted (
    hash TEXT UNIQUE, 
    posted_at TEXT, 
    title TEXT,
    url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted (
    video_id TEXT UNIQUE, 
    posted_at TEXT, 
    type TEXT
)''')
# –ù–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤ YouTube
c.execute('''CREATE TABLE IF NOT EXISTS youtube_channels_used (
    channel_name TEXT,
    used_at TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats (
    date TEXT UNIQUE, 
    normal_count INT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS used_images (
    url TEXT,
    used_at TEXT
)''')
conn.commit()

def get_today_stats():
    today = datetime.now().date().isoformat()
    c.execute("SELECT normal_count FROM daily_stats WHERE date = ?", (today,))
    result = c.fetchone()
    return {"normal": result[0]} if result else {"normal": 0}

def increment_stat():
    today = datetime.now().date().isoformat()
    stats = get_today_stats()
    stats["normal"] += 1
    c.execute("INSERT OR REPLACE INTO daily_stats (date, normal_count) VALUES (?, ?)", 
              (today, stats["normal"]))
    conn.commit()

def is_duplicate(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("SELECT 1 FROM posted WHERE hash = ?", (h,))
    return c.fetchone() is not None

def save_posted(title, url):
    h = hashlib.md5((title + url).encode()).hexdigest()
    c.execute("INSERT OR IGNORE INTO posted (hash, posted_at, title, url) VALUES (?, ?, ?, ?)", 
              (h, datetime.now().isoformat(), title, url))
    conn.commit()

def track_used_image(url: str):
    c.execute("INSERT INTO used_images (url, used_at) VALUES (?, ?)", 
              (url, datetime.now().isoformat()))
    conn.commit()
    week_ago = (datetime.now() - timedelta(days=7)).isoformat()
    c.execute("DELETE FROM used_images WHERE used_at < ?", (week_ago,))
    conn.commit()

def get_recent_images() -> list:
    yesterday = (datetime.now() - timedelta(hours=24)).isoformat()
    c.execute("SELECT url FROM used_images WHERE used_at > ?", (yesterday,))
    return [row[0] for row in c.fetchall()]

def is_youtube_posted_today(video_id):
    today = datetime.now().date().isoformat()
    c.execute("SELECT 1 FROM youtube_posted WHERE video_id = ? AND DATE(posted_at) = ?", 
              (video_id, today))
    return c.fetchone() is not None

def save_youtube_posted(video_id, video_type):
    c.execute("INSERT OR IGNORE INTO youtube_posted (video_id, posted_at, type) VALUES (?, ?, ?)", 
              (video_id, datetime.now().isoformat(), video_type))
    conn.commit()

# –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤
def track_youtube_channel(channel_name: str):
    """–ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –∫–∞–Ω–∞–ª"""
    c.execute("INSERT INTO youtube_channels_used (channel_name, used_at) VALUES (?, ?)", 
              (channel_name.lower(), datetime.now().isoformat()))
    conn.commit()
    # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (—Å—Ç–∞—Ä—à–µ 3 –¥–Ω–µ–π)
    three_days_ago = (datetime.now() - timedelta(days=3)).isoformat()
    c.execute("DELETE FROM youtube_channels_used WHERE used_at < ?", (three_days_ago,))
    conn.commit()

def get_recent_channels(hours: int = 12) -> list:
    """–ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT DISTINCT channel_name FROM youtube_channels_used WHERE used_at > ?", (cutoff,))
    return [row[0] for row in c.fetchall()]

def get_channel_usage_count(channel_name: str, hours: int = 24) -> int:
    """–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∫–∞–Ω–∞–ª –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT COUNT(*) FROM youtube_channels_used WHERE channel_name = ? AND used_at > ?", 
              (channel_name.lower(), cutoff))
    result = c.fetchone()
    return result[0] if result else 0

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI HELPER ==================
async def ask_ai(prompt: str, temperature=0.7) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è AI –∑–∞–ø—Ä–æ—Å–æ–≤"""
    if not OPENROUTER_API_KEY:
        return None
    
    models = [
        "nousresearch/hermes-3-llama-3.1-405b:free",
        "meta-llama/llama-3.1-8b-instruct:free",
        "google/gemma-2-9b-it:free",
    ]
    
    for model in models:
        try:
            async with aiohttp.ClientSession() as s:
                headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 800
                }
                async with s.post("https://openrouter.ai/api/v1/chat/completions",
                                 headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as r:
                    if r.status == 200:
                        data = await r.json()
                        return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
            log.debug(f"AI error ({model}): {e}")
            continue
    
    return None

# ================== AI: –í–´–ë–û–† –ù–û–í–û–°–¢–ò ==================
async def ai_select_and_summarize(news_list: list) -> dict:
    """AI –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ –¥–µ–ª–∞–µ—Ç –ø–µ—Ä–µ—Å–∫–∞–∑ —Å –ö–û–†–û–¢–ö–ò–ú–ò —Ö–µ—à—Ç–µ–≥–∞–º–∏"""
    news_text = "\n".join([f"{i+1}. {n['title']}" for i, n in enumerate(news_list[:25])])
    
    prompt = f"""–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –î–ï–†–ó–ö–û–ì–û –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞.

–í—ã–±–µ—Ä–∏ –û–î–ù–£ —Å–∞–º—É—é –≤–∑—Ä—ã–≤–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ —Å–¥–µ–ª–∞–π —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑.

–í–ê–ñ–ù–û:
1. –í—ã–±–∏—Ä–∞–π –ì–û–†–Ø–ß–ò–ï –Ω–æ–≤–æ—Å—Ç–∏ (–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã, –¥–µ–Ω—å–≥–∏, –≤–∑—Ä—ã–≤—ã, —Å–∫–∞–Ω–¥–∞–ª—ã)
2. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ö–û–†–û–¢–ö–ò–ô (–º–∞–∫—Å 60 —Å–∏–º–≤–æ–ª–æ–≤)
3. –£–±–µ—Ä–∏ "–∫–∞–∫", "–ø–æ—á–µ–º—É", –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
4. –ü–µ—Ä–µ—Å–∫–∞–∑ –î–û–ü–û–õ–ù–Ø–ï–¢ –∑–∞–≥–æ–ª–æ–≤–æ–∫
5. –ù–ï –í–´–ë–ò–†–ê–ô —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Å–∫—É—á–Ω—É—é —Ö—É–π–Ω—é!

–•–ï–®–¢–ï–ì–ò - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –¢–û–õ–¨–ö–û –æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞!
- –ö–ê–ñ–î–´–ô —Ö–µ—à—Ç–µ–≥ –û–¢–î–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª
- –ú–∞–∫—Å–∏–º—É–º 4 —Ö–µ—à—Ç–µ–≥–∞
- –ë–ï–ó —Å–∫–ª–µ–π–∫–∏ —Å–ª–æ–≤!

–ü—Ä–∏–º–µ—Ä—ã –ü–†–ê–í–ò–õ–¨–ù–´–• —Ö–µ—à—Ç–µ–≥–æ–≤:
‚úÖ #–ü—É—Ç–∏–Ω #–ú–æ—Å–∫–≤–∞ #–ü–µ—Ä–µ–≥–æ–≤–æ—Ä—ã #–î–∏–ø–ª–æ–º–∞—Ç–∏—è
‚úÖ #–¢—Ä–∞–º–ø #–°–®–ê #–°–∞–Ω–∫—Ü–∏–∏ #–≠–∫–æ–Ω–æ–º–∏–∫–∞
‚úÖ #–î–æ–ª–ª–∞—Ä #–ö—É—Ä—Å #–†—É–±–ª—å #–ë–∏—Ä–∂–∞
‚úÖ #–ú–∏–≥—Ä–∞—Ü–∏—è #–°–®–ê #–¢—é—Ä—å–º–∞ #–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∞

–ü—Ä–∏–º–µ—Ä—ã –ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–• —Ö–µ—à—Ç–µ–≥–æ–≤:
‚ùå #–≤–æ–π–Ω–∞–≤–£–∫—Ä–∞–∏–Ω–µ (—Å–∫–ª–µ–π–∫–∞!)
‚ùå #–£–∏—Ç–∫–æ—Ñ—Ñ–ö—É—à–Ω–µ—Ä–ü—É—Ç–∏–Ω (—Å–∫–ª–µ–π–∫–∞!)
‚ùå #–ó–µ–ª–µ–Ω—Å–∫–∏–π–¢—Ä–∞–º–ø (—Å–∫–ª–µ–π–∫–∞!)
‚ùå #–ü–µ—Ä–µ–≥–æ–≤–æ—Ä—ã–í–ú–æ—Å–∫–≤–µ (—Å–∫–ª–µ–π–∫–∞!)

–í–µ—Ä–Ω–∏ JSON:
{{
  "selected": –Ω–æ–º–µ—Ä (1-{len(news_list[:25])}),
  "title": "–ö–û–†–û–¢–ö–ò–ô –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–º–∞–∫—Å 60 —Å–∏–º–≤–æ–ª–æ–≤)",
  "summary": "–ü–µ—Ä–µ—Å–∫–∞–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "hashtags": "#–°–ª–æ–≤–æ1 #–°–ª–æ–≤–æ2 #–°–ª–æ–≤–æ3 #–°–ª–æ–≤–æ4"
}}

–ù–æ–≤–æ—Å—Ç–∏:
{news_text}"""
    
    response = await ask_ai(prompt, temperature=0.9)
    
    if response:
        try:
            json_start = response.find('{')
            json_end = response.rfind('}')
            if json_start != -1 and json_end != -1:
                content = response[json_start:json_end+1]
            else:
                content = response
            
            result = json.loads(content)
            selected_idx = int(result.get("selected", 1)) - 1
            
            if 0 <= selected_idx < len(news_list):
                selected_news = news_list[selected_idx]
                selected_news["ai_title"] = result.get("title", selected_news["title"])
                selected_news["summary"] = result.get("summary", "")
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤ - —Ä–∞–∑–±–∏–≤–∞–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–µ
                raw_hashtags = result.get("hashtags", "")
                selected_news["hashtags"] = fix_hashtags(raw_hashtags)
                
                log.info(f"‚úÖ AI –≤—ã–±—Ä–∞–ª #{selected_idx+1}: {selected_news['ai_title'][:50]}")
                return selected_news
        except Exception as e:
            log.warning(f"‚ö†Ô∏è AI parse error: {e}")
    
    # Fallback
    log.warning("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, fallback")
    priority_keywords = ['—Ç—Ä–∞–º–ø', '–ø—É—Ç–∏–Ω', '–≤–æ–π–Ω–∞', '–≤–∑—Ä—ã–≤', '–¥–æ–ª–ª–∞—Ä']
    scored = [(sum(1 for kw in priority_keywords if kw in n["title"].lower()), n) for n in news_list[:10]]
    scored.sort(key=lambda x: x[0], reverse=True)
    selected = scored[0][1] if scored else random.choice(news_list[:5])
    
    desc = selected["desc"] if selected["desc"] else ""
    sentences = [s for s in re.split(r'[.!?]\s+', desc) if len(s) > 30]
    summary = '. '.join(sentences[:2]) + '.' if sentences else "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤—ã—è—Å–Ω—è—é—Ç—Å—è."
    
    selected["ai_title"] = selected["title"]
    selected["summary"] = summary[:300]
    selected["hashtags"] = generate_smart_hashtags(selected["title"], desc)
    
    return selected

# ================== –§–ò–ö–° –•–ï–®–¢–ï–ì–û–í ==================
def fix_hashtags(raw_hashtags: str) -> str:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞"""
    
    # –£–±–∏—Ä–∞–µ–º @ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    raw_hashtags = re.sub(r'@\w+', '', raw_hashtags).strip()
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ö–µ—à—Ç–µ–≥–∏
    tags = re.findall(r'#\w+', raw_hashtags)
    
    fixed_tags = []
    for tag in tags:
        word = tag[1:]  # —É–±–∏—Ä–∞–µ–º #
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–ª–µ–µ–Ω–æ –ª–∏ (–∏—â–µ–º CamelCase –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö)
        parts = re.findall(r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]*|[–∞-—è—ëa-z]+', word)
        
        if len(parts) > 1 and len(word) > 12:
            # –°–∫–ª–µ–µ–Ω–Ω—ã–π —Ö–µ—à—Ç–µ–≥ - –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —á–∞—Å—Ç–∏ (–¥–ª–∏–Ω–Ω–µ–µ 2 –±—É–∫–≤)
            for part in parts:
                if len(part) > 2:
                    fixed_tags.append(f"#{part}")
        else:
            # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ö–µ—à—Ç–µ–≥
            fixed_tags.append(tag)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–∞–∫—Å–∏–º—É–º 4
    seen = set()
    unique_tags = []
    for tag in fixed_tags:
        tag_lower = tag.lower()
        if tag_lower not in seen:
            seen.add(tag_lower)
            unique_tags.append(tag)
    
    return ' '.join(unique_tags[:4])

# ================== –°–ë–û–† –ù–û–í–û–°–¢–ï–ô ==================
async def collect_fresh_news(limit=30):
    candidates = []
    sources = list(RSS_SOURCES.items())
    random.shuffle(sources)
    
    for source_name, rss_url in sources:
        if len(candidates) >= limit: break
        
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                if len(candidates) >= limit: break
                
                title = BeautifulSoup(entry.title.strip(), "html.parser").get_text()
                url = entry.link
                desc = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser").get_text()
                
                # –ü–∞—Ä—Å–∏–º –∫–∞—Ä—Ç–∏–Ω–∫—É
                rss_image = None
                if hasattr(entry, 'media_content') and entry.media_content:
                    rss_image = entry.media_content[0].get('url')
                
                if not rss_image and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            rss_image = enc.get('href')
                            break
                
                if not rss_image:
                    soup = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        rss_image = img_tag['src']
                
                if rss_image and (not rss_image.startswith('http') or len(rss_image) < 30):
                    rss_image = None
                
                if len(title) < 20: continue
                if is_duplicate(title, url): continue
                if any(boring in title.lower() for boring in BORING_KEYWORDS): continue
                if not any(k in title.lower() for k in KEYWORDS): continue
                
                candidates.append({
                    "title": title,
                    "url": url,
                    "desc": desc,
                    "source": source_name,
                    "rss_image": rss_image
                })
                
        except Exception as e:
            log.error(f"RSS {source_name}: {e}")
    
    return candidates

# ================== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–†–¢–ò–ù–û–ö ==================

# –ü–ï–†–°–û–ù–´ –î–õ–Ø –ü–û–ò–°–ö–ê –ö–û–ù–ö–†–ï–¢–ù–´–• –§–û–¢
PERSON_SEARCH_QUERIES = {
    '—Ç—Ä–∞–º–ø': ['donald trump', 'trump president', 'trump speech'],
    '–ø—É—Ç–∏–Ω': ['vladimir putin', 'putin russia', 'putin kremlin'],
    '–±–∞–π–¥–µ–Ω': ['joe biden', 'biden president', 'biden speech'],
    '–∑–µ–ª–µ–Ω—Å–∫': ['zelensky ukraine', 'zelensky president'],
    '–º–∞–∫—Ä–æ–Ω': ['macron france', 'macron president'],
    '—Å–∏ —Ü–∑–∏–Ω—å–ø–∏–Ω': ['xi jinping', 'china president xi'],
    '–∫—É—à–Ω–µ—Ä': ['jared kushner', 'kushner trump'],
}

async def ai_generate_image_queries(title: str, description: str) -> list:
    """AI –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã, –í–ö–õ–Æ–ß–ê–Ø –ö–û–ù–ö–†–ï–¢–ù–´–• –õ–Æ–î–ï–ô –µ—Å–ª–∏ –æ–Ω–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏"""
    
    text_lower = f"{title} {description}".lower()
    
    # –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä—Å–æ–Ω –≤ –Ω–æ–≤–æ—Å—Ç–∏
    person_queries = []
    for person_key, queries in PERSON_SEARCH_QUERIES.items():
        if person_key in text_lower:
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–π –ø–µ—Ä—Å–æ–Ω—ã
            person_queries.extend(queries[:2])
            log.info(f"   üéØ –ù–∞–π–¥–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞: {person_key} ‚Üí –¥–æ–±–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å—ã: {queries[:2]}")
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–µ—Ä—Å–æ–Ω - —Å—Ä–∞–∑—É –∏—Ö –≤–æ–∑–≤—Ä–∞—â–∞–µ–º (–æ–Ω–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)
    if person_queries:
        return person_queries[:3]
    
    # –ò–Ω–∞—á–µ AI –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã
    prompt = f"""–ù–æ–≤–æ—Å—Ç—å: "{title}"

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3 –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ê–ù–ì–õ–ò–ô–°–ö–û–ú –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–æ—Ç–æ.

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –µ—Å—Ç—å –ò–ó–í–ï–°–¢–ù–´–ï –õ–Æ–î–ò (–ø–æ–ª–∏—Ç–∏–∫–∏, –±–∏–∑–Ω–µ—Å–º–µ–Ω—ã) - –ò–©–ò –ò–• –§–û–¢–û!
- –ú–∞–∫—Å–∏–º—É–º 2-3 —Å–ª–æ–≤–∞
- –ù–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º

–ü—Ä–∏–º–µ—Ä—ã:
"–ü—É—Ç–∏–Ω –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è —Å –¢—Ä–∞–º–ø–æ–º" ‚Üí ["putin trump", "kremlin meeting", "russia usa summit"]
"–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—Ä–æ—Å" ‚Üí ["dollar currency", "stock market", "money exchange"]
"–í–∑—Ä—ã–≤ –≤ –∂–∏–ª–æ–º –¥–æ–º–µ" ‚Üí ["building explosion", "fire rescue", "emergency"]

–í–µ—Ä–Ω–∏ JSON:
{{"queries": ["–∑–∞–ø—Ä–æ—Å1", "–∑–∞–ø—Ä–æ—Å2", "–∑–∞–ø—Ä–æ—Å3"]}}"""
    
    response = await ask_ai(prompt, temperature=0.7)
    
    if response:
        try:
            json_start = response.find('{')
            json_end = response.rfind('}')
            if json_start != -1 and json_end != -1:
                content = response[json_start:json_end+1]
                result = json.loads(content)
                queries = result.get("queries", [])[:3]
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 —Å–ª–æ–≤–∞–º–∏
                cleaned = []
                for q in queries:
                    q = re.sub(r'\b(19|20)\d{2}\b', '', q).strip()
                    words = q.split()
                    if len(words) <= 3:
                        cleaned.append(q)
                    else:
                        cleaned.append(' '.join(words[:3]))
                
                if cleaned:
                    log.info(f"   ‚úÖ AI —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã: {cleaned}")
                    return cleaned
        except Exception as e:
            log.warning(f"   ‚ö†Ô∏è AI parse error: {e}")
    
    return generate_fallback_queries(title, description)


def generate_fallback_queries(title: str, description: str) -> list:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ AI –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã"""
    text = f"{title} {description}".lower()
    queries = []
    
    # –ü–ï–†–°–û–ù–´ - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!
    if '—Ç—Ä–∞–º–ø' in text: queries.append('donald trump')
    if '–ø—É—Ç–∏–Ω' in text: queries.append('vladimir putin')
    if '–±–∞–π–¥–µ–Ω' in text: queries.append('joe biden')
    if '–∑–µ–ª–µ–Ω—Å–∫' in text: queries.append('zelensky')
    if '–º–∞–∫—Ä–æ–Ω' in text: queries.append('macron')
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–µ—Ä—Å–æ–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    if queries:
        return queries[:3]
    
    # –ü–æ–ª–∏—Ç–∏–∫–∞ –∏ –¥–∏–ø–ª–æ–º–∞—Ç–∏—è
    if any(w in text for w in ['–ø–µ—Ä–µ–≥–æ–≤', '–≤—Å—Ç—Ä–µ—á', '–≤–∏–∑–∏—Ç', '—Å–∞–º–º–∏—Ç']):
        queries.append('diplomatic meeting')
        queries.append('conference room')
    
    # –†–æ—Å—Å–∏—è
    if any(w in text for w in ['–∫—Ä–µ–º–ª', '–º–æ—Å–∫–≤', '—Ä–æ—Å—Å–∏']):
        queries.append('kremlin moscow')
        queries.append('russian government')
    
    # –°–®–ê
    if any(w in text for w in ['—Å—à–∞', '–∞–º–µ—Ä–∏–∫', '–≤–∞—à–∏–Ω–≥—Ç–æ–Ω', '–±–µ–ª—ã–π –¥–æ–º']):
        queries.append('white house washington')
        queries.append('american flag')
    
    # –£–∫—Ä–∞–∏–Ω–∞
    if '—É–∫—Ä–∞–∏–Ω' in text or '–∫–∏–µ–≤' in text:
        queries.append('ukraine kyiv')
    
    # –í–æ–π–Ω–∞/–∫–æ–Ω—Ñ–ª–∏–∫—Ç
    if any(w in text for w in ['–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–≤–æ–µ–Ω–Ω', '–∞—Ä–º–∏—è']):
        queries.append('military conflict')
        queries.append('war zone')
    
    # –≠–∫–æ–Ω–æ–º–∏–∫–∞
    if any(w in text for w in ['–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª—å', '–∫—É—Ä—Å', '–±–∏—Ä–∂–∞', '—ç–∫–æ–Ω–æ–º–∏–∫']):
        queries.append('stock market trading')
        queries.append('dollar currency')
    
    # –ß–ü
    if any(w in text for w in ['–≤–∑—Ä—ã–≤', '–ø–æ–∂–∞—Ä', '–∞–≤–∞—Ä–∏']):
        queries.append('explosion fire')
        queries.append('emergency rescue')
    
    # –¢—é—Ä—å–º–∞/–º–∏–≥—Ä–∞—Ü–∏—è
    if any(w in text for w in ['—Ç—é—Ä—å–º', '–º–∏–≥—Ä', '–¥–µ–ø–æ—Ä—Ç', '–∑–∞–¥–µ—Ä–∂']):
        queries.append('prison bars')
        queries.append('detention center')
    
    # –î–∞–≤–æ—Å
    if '–¥–∞–≤–æ—Å' in text:
        queries.append('davos forum')
        queries.append('economic summit')
    
    # –î–µ—Ñ–æ–ª—Ç
    if not queries:
        queries = ['world news', 'breaking news', 'global politics']
    
    log.info(f"   ‚ö†Ô∏è Fallback –∑–∞–ø—Ä–æ—Å—ã: {queries[:3]}")
    return queries[:3]

async def search_unsplash(query: str, count=30) -> list:
    """–ò—â–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–∞ Unsplash"""
    if not UNSPLASH_ACCESS_KEY:
        log.warning("   ‚ùå Unsplash API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return []
    
    try:
        log.info(f"   üîç Unsplash –∑–∞–ø—Ä–æ—Å: '{query}' (–∏—â—É {count} —Ñ–æ—Ç–æ)")
        
        url = "https://api.unsplash.com/search/photos"
        params = {"query": query, "per_page": count, "orientation": "landscape"}
        headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
        
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as r:
                log.info(f"   üì° Unsplash –æ—Ç–≤–µ—Ç: HTTP {r.status}")
                
                if r.status == 200:
                    data = await r.json()
                    results = data.get("results", [])
                    log.info(f"   ‚úÖ Unsplash –≤–µ—Ä–Ω—É–ª {len(results)} —Ñ–æ—Ç–æ")
                    
                    return [{"url": p["urls"]["regular"], "desc": p.get("description", "") or p.get("alt_description", ""), "source": "unsplash"} 
                            for p in results[:count]]
                elif r.status == 401:
                    error_text = await r.text()
                    log.error(f"   ‚ùå Unsplash 401 (–Ω–µ–≤–µ—Ä–Ω—ã–π –∫–ª—é—á): {error_text[:200]}")
                elif r.status == 403:
                    error_text = await r.text()
                    log.error(f"   ‚ùå Unsplash 403 (–ª–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω): {error_text[:200]}")
                else:
                    error_text = await r.text()
                    log.error(f"   ‚ùå Unsplash {r.status}: {error_text[:200]}")
                    
    except asyncio.TimeoutError:
        log.error(f"   ‚è±Ô∏è Unsplash timeout –¥–ª—è '{query}'")
    except Exception as e:
        log.error(f"   ‚ùå Unsplash exception: {e}")
    
    return []

async def search_pexels(query: str, count=30) -> list:
    """–ò—â–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–∞ Pexels"""
    if not PEXELS_API_KEY:
        log.warning("   ‚ùå Pexels API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return []
    
    try:
        log.info(f"   üîç Pexels –∑–∞–ø—Ä–æ—Å: '{query}' (–∏—â—É {count} —Ñ–æ—Ç–æ)")
        
        url = "https://api.pexels.com/v1/search"
        params = {"query": query, "per_page": count, "orientation": "landscape"}
        headers = {"Authorization": PEXELS_API_KEY}
        
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as r:
                log.info(f"   üì° Pexels –æ—Ç–≤–µ—Ç: HTTP {r.status}")
                
                if r.status == 200:
                    data = await r.json()
                    photos = data.get("photos", [])
                    log.info(f"   ‚úÖ Pexels –≤–µ—Ä–Ω—É–ª {len(photos)} —Ñ–æ—Ç–æ")
                    
                    return [{"url": p["src"]["large"], "desc": p.get("alt", ""), "source": "pexels"} 
                            for p in photos[:count]]
                else:
                    error_text = await r.text()
                    log.error(f"   ‚ùå Pexels {r.status}: {error_text[:100]}")
                    
    except asyncio.TimeoutError:
        log.error(f"   ‚è±Ô∏è Pexels timeout –¥–ª—è '{query}'")
    except Exception as e:
        log.error(f"   ‚ùå Pexels exception: {e}")
    
    return []

async def ai_rate_images(images: list, title: str) -> dict:
    """AI –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–æ URL + –û–ü–ò–°–ê–ù–ò–Ø–ú"""
    if not images:
        return None
    
    # –ü–û–ö–ê–ó–´–í–ê–ï–ú –ö–ê–†–¢–ò–ù–ö–ò –° –û–ü–ò–°–ê–ù–ò–Ø–ú–ò
    log.info(f"   üìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ ({len(images)} —à—Ç):")
    for i, img in enumerate(images[:10], 1):
        desc_preview = img.get('desc', '–Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è')[:50]
        log.info(f"      {i}. {img['source']}: {desc_preview}")
    
    images_text = "\n".join([
        f"{i+1}. –û–ø–∏—Å–∞–Ω–∏–µ: \"{img.get('desc', '–Ω–µ—Ç')}\" | –ò—Å—Ç–æ—á–Ω–∏–∫: {img['source']}" 
        for i, img in enumerate(images[:30])
    ])
    
    prompt = f"""–ù–æ–≤–æ—Å—Ç—å: "{title}"

–í–æ—Ç {len(images[:30])} –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Å –û–ü–ò–°–ê–ù–ò–Ø–ú–ò:
{images_text}

–û—Ü–µ–Ω–∏ –∫–∞–∂–¥—É—é –æ—Ç 1 –¥–æ 10 –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –Ω–æ–≤–æ—Å—Ç–∏. –ò–°–ü–û–õ–¨–ó–£–ô –û–ü–ò–°–ê–ù–ò–Ø –¥–ª—è –æ—Ü–µ–Ω–∫–∏!

–í–µ—Ä–Ω–∏ JSON:
{{
  "best_id": –Ω–æ–º–µ—Ä –ª—É—á—à–µ–π (1-{len(images[:30])}),
  "score": –æ—Ü–µ–Ω–∫–∞ (1-10),
  "reason": "–ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–ª"
}}

–ï—Å–ª–∏ –í–°–ï –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–ª–æ—Ö–∏–µ (–æ—Ü–µ–Ω–∫–∞ < 5), –≤–µ—Ä–Ω–∏ {{"best_id": 0, "score": 0, "reason": "–≤—Å–µ –ø–ª–æ—Ö–∏–µ"}}"""
    
    response = await ask_ai(prompt, temperature=0.5)
    
    if response:
        try:
            json_start = response.find('{')
            json_end = response.rfind('}')
            if json_start != -1 and json_end != -1:
                content = response[json_start:json_end+1]
                result = json.loads(content)
                best_id = int(result.get("best_id", 0))
                score = int(result.get("score", 0))
                reason = result.get("reason", "")
                
                log.info(f"   ü§ñ AI –≤—ã–±—Ä–∞–ª –∫–∞—Ä—Ç–∏–Ω–∫—É #{best_id}, –æ—Ü–µ–Ω–∫–∞ {score}/10")
                log.info(f"   üí≠ –ü—Ä–∏—á–∏–Ω–∞: {reason}")
                
                if best_id > 0 and score >= 5 and best_id <= len(images):
                    return {"image": images[best_id - 1], "score": score}
        except Exception as e:
            log.warning(f"   ‚ö†Ô∏è AI parse error: {e}")
    
    # Fallback - –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é
    if images:
        log.warning("   ‚ö†Ô∏è AI –Ω–µ —Å–º–æ–≥ –≤—ã–±—Ä–∞—Ç—å, –±–µ—Ä—É –ø–µ—Ä–≤—É—é")
        return {"image": images[0], "score": 5}
    
    return None

async def get_perfect_image(title: str, description: str, rss_image: str = None) -> str:
    """
    –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê —Å –ù–ï–°–ö–û–õ–¨–ö–ò–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏:
    1. AI –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã (–≤–∫–ª—é—á–∞—è –ø–µ—Ä—Å–æ–Ω)
    2. –î–µ–ª–∞–µ–º –î–û 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Unsplash (—Ä–∞–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
    3. AI –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ
    """
    
    log.info("   üé® –ó–∞–ø—É—Å–∫–∞—é –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫...")
    
    all_images = []
    
    # –®–∞–≥ 1: AI –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã
    queries = await ai_generate_image_queries(title, description)
    
    # –®–∞–≥ 2: –î–µ–ª–∞–µ–º –ù–ï–°–ö–û–õ–¨–ö–û –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Unsplash (–¥–æ 5)
    for i, query in enumerate(queries[:3]):
        log.info(f"   üîç –ó–∞–ø—Ä–æ—Å {i+1}/3: '{query}'")
        
        unsplash_images = await search_unsplash(query, count=15)
        if unsplash_images:
            all_images.extend(unsplash_images)
            log.info(f"   ‚úÖ +{len(unsplash_images)} —Ñ–æ—Ç–æ –æ—Ç Unsplash")
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        await asyncio.sleep(0.3)
    
    # –®–∞–≥ 3: Pexels –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
    if queries:
        pexels_images = await search_pexels(queries[0], count=15)
        if pexels_images:
            all_images.extend(pexels_images)
            log.info(f"   ‚úÖ +{len(pexels_images)} —Ñ–æ—Ç–æ –æ—Ç Pexels")
    
    # –®–∞–≥ 4: RSS –∫–∞—Ä—Ç–∏–Ω–∫–∞
    if rss_image and len(rss_image) > 50:
        log.info(f"   üéØ –î–æ–±–∞–≤–ª—è—é RSS –∫–∞—Ä—Ç–∏–Ω–∫—É...")
        img_data = await download_image(rss_image)
        if img_data and len(img_data) > 5000:
            all_images.append({"url": rss_image, "desc": "RSS original image", "source": "rss"})
    
    # –®–∞–≥ 5: –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen_urls = set()
    unique_images = []
    for img in all_images:
        if img["url"] not in seen_urls:
            seen_urls.add(img["url"])
            unique_images.append(img)
    
    log.info(f"   üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫: {len(unique_images)}")
    
    if not unique_images:
        log.error("   ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û –ö–ê–†–¢–ò–ù–û–ö!")
        return None
    
    # –®–∞–≥ 6: AI –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é
    best = await ai_rate_images(unique_images, title)
    
    if best and best["score"] >= 5:
        img_url = best["image"]["url"]
        log.info(f"   üèÜ –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {best['image']['source']} (–æ—Ü–µ–Ω–∫–∞ {best['score']}/10)")
        track_used_image(img_url)
        return img_url
    
    # Fallback - –ø–µ—Ä–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞
    if unique_images:
        log.warning("   ‚ö†Ô∏è –í—Å–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–ª–æ—Ö–∏–µ, –±–µ—Ä—É –ø–µ—Ä–≤—É—é")
        img_url = unique_images[0]["url"]
        track_used_image(img_url)
        return img_url
    
    return None

async def download_image(url: str):
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    return await r.read()
    except:
        pass
    return None

def generate_smart_hashtags(title: str, description: str = "") -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ö–û–†–û–¢–ö–ò–ï –æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏"""
    text = f"{title} {description}".lower()
    tags = []
    
    # –ü–µ—Ä—Å–æ–Ω—ã (–æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ!)
    if '–ø—É—Ç–∏–Ω' in text: tags.append('#–ü—É—Ç–∏–Ω')
    if '—Ç—Ä–∞–º–ø' in text: tags.append('#–¢—Ä–∞–º–ø')
    if '–±–∞–π–¥–µ–Ω' in text: tags.append('#–ë–∞–π–¥–µ–Ω')
    if '–∑–µ–ª–µ–Ω—Å–∫' in text: tags.append('#–ó–µ–ª–µ–Ω—Å–∫–∏–π')
    if '–∫—É—à–Ω–µ—Ä' in text: tags.append('#–ö—É—à–Ω–µ—Ä')
    if '—É–∏—Ç–∫–æ—Ñ—Ñ' in text: tags.append('#–£–∏—Ç–∫–æ—Ñ—Ñ')
    if '–º–∞–∫—Ä–æ–Ω' in text: tags.append('#–ú–∞–∫—Ä–æ–Ω')
    if '—Å–∏ —Ü–∑–∏–Ω—å–ø–∏–Ω' in text or '—Ü–∑–∏–Ω—å–ø–∏–Ω' in text: tags.append('#–ö–∏—Ç–∞–π')
    
    # –°—Ç—Ä–∞–Ω—ã (–æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ!)
    if '—Å—à–∞' in text or '–∞–º–µ—Ä–∏–∫' in text: tags.append('#–°–®–ê')
    if '—É–∫—Ä–∞–∏–Ω' in text: tags.append('#–£–∫—Ä–∞–∏–Ω–∞')
    if '—Ä–æ—Å—Å–∏' in text or ' —Ä—Ñ ' in text: tags.append('#–†–æ—Å—Å–∏—è')
    if '–≥–µ—Ä–º–∞–Ω' in text: tags.append('#–ì–µ—Ä–º–∞–Ω–∏—è')
    if '–∫–∏—Ç–∞–π' in text or '–ø–µ–∫–∏–Ω' in text: tags.append('#–ö–∏—Ç–∞–π')
    if '–º–æ—Å–∫–≤' in text: tags.append('#–ú–æ—Å–∫–≤–∞')
    if '–¥–∞–≤–æ—Å' in text: tags.append('#–î–∞–≤–æ—Å')
    
    # –¢–µ–º—ã (–æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ!)
    if any(w in text for w in ['–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª—å', '–∫—É—Ä—Å', '–≤–∞–ª—é—Ç']): tags.append('#–ö—É—Ä—Å')
    if any(w in text for w in ['—ç–∫–æ–Ω–æ–º–∏–∫', '—Å–∞–Ω–∫—Ü–∏', '–ø–æ—à–ª–∏–Ω']): tags.append('#–≠–∫–æ–Ω–æ–º–∏–∫–∞')
    if '–≤–æ–π–Ω–∞' in text or '–∫–æ–Ω—Ñ–ª–∏–∫—Ç' in text: tags.append('#–í–æ–π–Ω–∞')
    if '–ø–µ—Ä–µ–≥–æ–≤' in text or '–≤—Å—Ç—Ä–µ—á' in text or '–≤–∏–∑–∏—Ç' in text: tags.append('#–ü–µ—Ä–µ–≥–æ–≤–æ—Ä—ã')
    if any(w in text for w in ['–≤–∑—Ä—ã–≤', '–ø–æ–∂–∞—Ä', '–∞–≤–∞—Ä–∏']): tags.append('#–ß–ü')
    if any(w in text for w in ['–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–∑–∞–¥–µ—Ä–∂']): tags.append('#–ö—Ä–∏–º–∏–Ω–∞–ª')
    if any(w in text for w in ['—Ç—é—Ä—å–º', '–º–∏–≥—Ä', '–¥–µ–ø–æ—Ä—Ç']): tags.append('#–ú–∏–≥—Ä–∞—Ü–∏—è')
    if any(w in text for w in ['–∂—É—Ä–Ω–∞–ª', '—Å–º–∏', '–≥–∞–∑–µ—Ç']): tags.append('#–°–ú–ò')
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen = set()
    unique = []
    for tag in tags:
        if tag.lower() not in seen:
            seen.add(tag.lower())
            unique.append(tag)
    
    if not unique:
        unique.append('#–ù–æ–≤–æ—Å—Ç–∏')
    
    return ' '.join(unique[:4])

# ================== –ü–û–°–¢–ò–ù–ì ==================
async def post_selected_news(news):
    title = news.get("ai_title", news["title"])
    url = news["url"]
    summary = news.get("summary", "")
    hashtags = news.get("hashtags", "")
    desc = news.get("desc", "")
    rss_image = news.get("rss_image")
    
    hashtags = re.sub(r'@\w+', '', hashtags).strip()
    if not hashtags:
        hashtags = generate_smart_hashtags(title, desc)
    
    caption = f"**{title}**\n\n{summary}\n\n{hashtags}"
    
    log.info(f"   üì∞ –ü–û–°–¢:")
    log.info(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
    log.info(f"   –•–µ—à—Ç–µ–≥–∏: {hashtags}")
    
    img_url = await get_perfect_image(title, desc, rss_image)
    
    if not img_url:
        log.warning("   ‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç")
        return False
    
    img_data = await download_image(img_url)
    
    if img_data and len(img_data) > 1024:
        try:
            file = BufferedInputFile(img_data, filename="news.jpg")
            await bot.send_photo(CHANNEL_ID, file, caption=caption, parse_mode=ParseMode.MARKDOWN)
            save_posted(news["title"], url)
            increment_stat()
            log.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}")
            return True
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False
    else:
        log.warning("   ‚ö†Ô∏è –ë–∏—Ç–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞")
        return False

# ================== –¶–ò–ö–õ ==================
async def check_news():
    stats = get_today_stats()
    if stats["normal"] >= 25:
        log.info("üìä –õ–∏–º–∏—Ç 25 –ø–æ—Å—Ç–æ–≤")
        return
    
    log.info("üì• –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏...")
    candidates = await collect_fresh_news(30)
    
    if not candidates:
        log.info("‚ö†Ô∏è –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç")
        return
    
    log.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    
    selected = await ai_select_and_summarize(candidates)
    
    if not selected:
        log.warning("‚ö†Ô∏è AI –Ω–µ –≤—ã–±—Ä–∞–ª –Ω–æ–≤–æ—Å—Ç—å")
        return
    
    await post_selected_news(selected)

async def news_loop():
    log.info("‚è∞ –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
    await asyncio.sleep(5)
    
    while True:
        await check_news()
        next_interval = random.randint(20, 70)
        log.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ {next_interval} –º–∏–Ω")
        await asyncio.sleep(next_interval * 60)

# ================== YOUTUBE SHORTS - –£–õ–£–ß–®–ï–ù–ù–´–ô ==================
def has_cyrillic(text):
    return bool(re.search('[–∞-—è–ê-–Ø—ë–Å]', text))

def has_ukrainian(text):
    return any(l in text for l in ['—î', '—ñ', '—ó', '“ë', '–Ñ', '–Ü', '–á', '“ê'])

def is_russian_content(title, channel_title, description=""):
    full_text = f"{title} {channel_title} {description}".lower()
    
    if not has_cyrillic(title + channel_title):
        return False
    
    if has_ukrainian(title + channel_title + description):
        return False
    
    ua_keywords = ['—É–∫—Ä–∞—ó–Ω', 'ukrainian', 'kiev', 'kyiv', '–∫–∏—ó–≤', '–∑–µ–ª–µ–Ω—Å—å–∫', 'zelensky', '–∞–∑–æ–≤', '–≤—Å—É', '–∑—Å—É']
    if any(kw in full_text for kw in ua_keywords):
        return False
    
    return True

def is_trusted_news_channel(channel_title):
    return any(t.lower() in channel_title.lower() for t in RU_NEWS_CHANNELS)

def is_any_news_related(title: str, channel: str, description: str = "") -> bool:
    text = f"{title} {channel} {description}".lower()
    
    if is_trusted_news_channel(channel):
        return True
    
    news_keywords = [
        '–Ω–æ–≤–æ—Å—Ç', '—Å–µ–≥–æ–¥–Ω—è', '—Å—Ä–æ—á–Ω', '–≥–ª–∞–≤–Ω–æ–µ', '–∏—Ç–æ–≥–∏',
        '–ø—É—Ç–∏–Ω', '—Ä–æ—Å—Å–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–º–∏–Ω–∏—Å—Ç—Ä',
        '–∫—Ä–µ–º–ª—å', '–¥—É–º–∞', '–ø–æ–ª–∏—Ç–∏–∫', '–∑–∞–∫–æ–Ω', '—Ä–µ—Ñ–æ—Ä–º',
        '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '—Å—à–∞', '—É–∫—Ä–∞–∏–Ω', '–≤–æ–π–Ω–∞', '–º–∏—Ä',
        '–µ–≤—Ä–æ–ø', '–∫–∏—Ç–∞–π', '–Ω–∞—Ç–æ',
        '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–∫—É—Ä—Å', '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü',
        '—Ü–µ–Ω—ã', '–∑–∞—Ä–ø–ª–∞—Ç', '–ø–µ–Ω—Å–∏', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑',
        '–∑–∞—è–≤–∏–ª', '–æ–±—ä—è–≤–∏–ª', '—Å–æ–æ–±—â–∏–ª', '–ø—Ä–æ–∏–∑–æ—à–ª', '—Å–ª—É—á–∏–ª',
        '—Ä–µ—à–∏–ª', '–ø–æ–¥–ø–∏—Å–∞–ª', '–ø—Ä–∏–Ω—è–ª',
        '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤', '–∞–≤–∞—Ä–∏—è', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç',
        '–≤–∞–∂–Ω', '–≥–ª–∞–≤–Ω', '—Å–∫–∞–Ω–¥–∞–ª', '—Å–µ–Ω—Å–∞—Ü'
    ]
    
    matches = sum(1 for kw in news_keywords if kw in text)
    return matches >= 1

def parse_duration_to_seconds(iso_duration):
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, iso_duration)
    
    if not match:
        return 0
    
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    
    return hours * 3600 + minutes * 60 + seconds

def format_views(views):
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}–ú"
    elif views >= 1_000:
        return f"{views / 1_000:.1f}–ö"
    else:
        return str(views)

async def search_diverse_shorts():
    """–£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ —Å –†–û–¢–ê–¶–ò–ï–ô –∫–∞–Ω–∞–ª–æ–≤"""
    log.info("üîç –ü–æ–∏—Å–∫ –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–• Shorts...")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
    recent_channels = get_recent_channels(hours=12)
    log.info(f"   ‚è≠Ô∏è –ò—Å–∫–ª—é—á–∞—é {len(recent_channels)} –Ω–µ–¥–∞–≤–Ω–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {recent_channels[:5]}...")
    
    all_shorts = []
    
    # –†–ê–°–®–ò–†–ï–ù–ù–´–ô —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    diverse_queries = [
        "–Ω–æ–≤–æ—Å—Ç–∏ —Ä–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è",
        "–ø–æ–ª–∏—Ç–∏–∫–∞ –ø—É—Ç–∏–Ω –∫—Ä–µ–º–ª—å",
        "–ø—É—Ç–∏–Ω –∑–∞—è–≤–∏–ª",
        "—Ç—Ä–∞–º–ø –Ω–æ–≤–æ—Å—Ç–∏",
        "–º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Ä—É–±–ª—å",
        "—ç–∫–æ–Ω–æ–º–∏–∫–∞ —Ä–æ—Å—Å–∏–∏",
        "—Ä–æ—Å—Å–∏—è –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è",
        "–≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–Ω—è",
        "—Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "–∏—Ç–æ–≥–∏ –Ω–µ–¥–µ–ª–∏ —Ä–æ—Å—Å–∏—è",
        "–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å",
        "–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è",
        "—Å–∫–∞–Ω–¥–∞–ª —Ä–æ—Å—Å–∏—è",
    ]
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    random.shuffle(diverse_queries)
    
    for query in diverse_queries[:10]:
        try:
            log.info(f"   üîé '{query}'...")
            
            url = "https://www.googleapis.com/youtube/v3/search"
            params = {
                "part": "id,snippet",
                "q": query + " shorts",
                "type": "video",
                "maxResults": 50,  # –£–≤–µ–ª–∏—á–∏–ª
                "order": "date",   # –ò–ó–ú–ï–ù–ò–õ: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–∞—Ç–µ, –ø–æ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                "publishedAfter": (datetime.now() - timedelta(days=3)).isoformat() + "Z",
                "regionCode": "RU",
                "relevanceLanguage": "ru",
                "videoCategoryId": "25",
                "key": YOUTUBE_API_KEY
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=15)) as response:
                    if response.status != 200:
                        continue
                    
                    data = await response.json()
                    video_ids = [item["id"]["videoId"] for item in data.get("items", []) 
                                if item["id"].get("kind") == "youtube#video"]
                    
                    if not video_ids:
                        continue
                    
                    details_url = "https://www.googleapis.com/youtube/v3/videos"
                    details_params = {
                        "part": "snippet,statistics,contentDetails",
                        "id": ",".join(video_ids[:50]),
                        "key": YOUTUBE_API_KEY
                    }
                    
                    async with session.get(details_url, params=details_params, 
                                          timeout=aiohttp.ClientTimeout(total=15)) as resp:
                        if resp.status != 200:
                            continue
                        
                        details_data = await resp.json()
                        
                        for item in details_data.get("items", []):
                            try:
                                duration = item["contentDetails"]["duration"]
                                total_sec = parse_duration_to_seconds(duration)
                                
                                if not (8 <= total_sec <= 65):
                                    continue
                                
                                snippet = item["snippet"]
                                stats = item["statistics"]
                                
                                title = snippet.get("title", "")
                                channel_title = snippet.get("channelTitle", "")
                                description = snippet.get("description", "")
                                
                                # –ü–†–û–í–ï–†–ö–ê: –∫–∞–Ω–∞–ª —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ?
                                if channel_title.lower() in recent_channels:
                                    log.debug(f"      ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (–Ω–µ–¥–∞–≤–Ω–∏–π –∫–∞–Ω–∞–ª): {channel_title}")
                                    continue
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∫–∞–Ω–∞–ª –∑–∞ 24—á
                                channel_usage = get_channel_usage_count(channel_title, hours=24)
                                if channel_usage >= 2:  # –ú–∞–∫—Å–∏–º—É–º 2 –≤–∏–¥–µ–æ —Å –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –≤ –¥–µ–Ω—å
                                    log.debug(f"      ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (–ª–∏–º–∏—Ç –∫–∞–Ω–∞–ª–∞): {channel_title} ({channel_usage}/2)")
                                    continue
                                
                                if not is_russian_content(title, channel_title, description):
                                    continue
                                
                                if not is_any_news_related(title, channel_title, description):
                                    continue
                                
                                views = int(stats.get("viewCount", 0))
                                
                                min_views = 1000 if is_trusted_news_channel(channel_title) else 3000
                                if views < min_views:
                                    continue
                                
                                all_shorts.append({
                                    "id": item["id"],
                                    "title": title,
                                    "channel": channel_title,
                                    "views": views,
                                    "likes": int(stats.get("likeCount", 0)),
                                    "duration_sec": total_sec,
                                    "url": f"https://youtube.com/shorts/{item['id']}",
                                    "is_trusted": is_trusted_news_channel(channel_title)
                                })
                                
                            except Exception as e:
                                continue
            
            await asyncio.sleep(0.4)
            
        except Exception as e:
            log.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
            continue
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–∏–¥–µ–æ
    seen_ids = set()
    unique_shorts = []
    for short in all_shorts:
        if short["id"] not in seen_ids:
            seen_ids.add(short["id"])
            unique_shorts.append(short)
    
    # –ù–û–í–ê–Ø –°–û–†–¢–ò–†–û–í–ö–ê: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞–Ω–∞–ª–∞–º –∫–æ—Ç–æ—Ä—ã–µ –¥–∞–≤–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å + –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
    def sort_key(x):
        channel_usage = get_channel_usage_count(x["channel"], hours=48)
        # –ß–µ–º –º–µ–Ω—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ - —Ç–µ–º –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        # Trusted –∫–∞–Ω–∞–ª—ã –≤—Å—ë –µ—â—ë –∏–º–µ—é—Ç –±–æ–Ω—É—Å
        return (channel_usage, not x["is_trusted"], -x["views"])
    
    unique_shorts.sort(key=sort_key)
    
    log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_shorts)} –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–• Shorts")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    for i, s in enumerate(unique_shorts[:5], 1):
        usage = get_channel_usage_count(s["channel"], hours=24)
        log.info(f"   {i}. [{s['channel'][:20]}] (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π: {usage}) - {s['title'][:40]}...")
    
    return unique_shorts

async def download_shorts_video(video_id):
    output_file = os.path.join(TEMP_DIR, f"shorts_{video_id}.mp4")
    
    try:
        log.info("   üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ yt-dlp...")
        
        url = f"https://www.youtube.com/watch?v={video_id}"
        
        cmd = [
            sys.executable,
            "-m", "yt_dlp",
            "-f", "bv*+ba/b",
            "-o", output_file,
            "--no-playlist",
            "--merge-output-format", "mp4",
            "--extractor-args", "youtube:player_client=android",
            "--no-check-certificate",
            "--socket-timeout", "30",
            url
        ]
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await asyncio.wait_for(
            process.communicate(), 
            timeout=90
        )
        
        if process.returncode == 0 and os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / 1024 / 1024
            log.info(f"   ‚úÖ –°–∫–∞—á–∞–Ω–æ {file_size:.1f} MB")
            return output_file
        else:
            if os.path.exists(output_file):
                os.remove(output_file)
            return None
            
    except Exception as e:
        log.error(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        if os.path.exists(output_file):
            os.remove(output_file)
        return None

async def post_youtube_shorts():
    """–û–ë–ù–û–í–õ–Å–ù–ù–´–ô –ø–æ—Å—Ç–∏–Ω–≥ Shorts —Å –ù–û–í–´–ú —Ñ–æ—Ä–º–∞—Ç–æ–º"""
    log.info("üé¨ –ó–∞–ø—É—Å–∫: YouTube Shorts...")
    
    shorts = await search_diverse_shorts()
    
    if not shorts:
        log.warning("‚ö†Ô∏è Shorts –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    for i, short_video in enumerate(shorts[:15], 1):  # –£–≤–µ–ª–∏—á–∏–ª –¥–æ 15 –ø–æ–ø—ã—Ç–æ–∫
        if is_youtube_posted_today(short_video["id"]):
            log.info(f"   [{i}/15] ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –ø–æ—Å—Ç–∏–ª–∏): {short_video['title'][:50]}")
            continue
        
        trust_badge = "‚≠ê" if short_video["is_trusted"] else ""
        log.info(f"üéØ [{i}/15] {trust_badge} {short_video['title'][:60]}...")
        log.info(f"   üëÄ {format_views(short_video['views'])} | üì∫ {short_video['channel']}")
        
        video_file_path = await download_shorts_video(short_video['id'])
        
        if not video_file_path:
            log.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â–∏–π...")
            continue
        
        try:
            # ================== –ù–û–í–´–ô –§–û–†–ú–ê–¢ –ü–û–°–¢–ê ==================
            # –£–±–∏—Ä–∞–µ–º "–ì–ª–∞–≤–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π Short –¥–Ω—è" –∏ —Ö–µ—à—Ç–µ–≥–∏ –ø–æ—Å–ª–µ //
            # –ü—Ä–æ—Å—Ç–æ: –Ω–∞–∑–≤–∞–Ω–∏–µ + –∫–∞–Ω–∞–ª + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –ª–∏—à–Ω–µ–≥–æ
            clean_title = short_video['title']
            # –£–±–∏—Ä–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
            clean_title = re.sub(r'#\S+', '', clean_title).strip()
            # –£–±–∏—Ä–∞–µ–º // –∏ –≤—Å—ë –ø–æ—Å–ª–µ
            if '//' in clean_title:
                clean_title = clean_title.split('//')[0].strip()
            # –£–±–∏—Ä–∞–µ–º | –∏ –≤—Å—ë –ø–æ—Å–ª–µ
            if '|' in clean_title:
                clean_title = clean_title.split('|')[0].strip()
            
            caption = (
                f"‚ùó {clean_title}\n\n"
                f"üì∫ {short_video['channel']}\n"
                f"üëÄ {format_views(short_video['views'])} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ | "
                f"‚ù§Ô∏è {format_views(short_video['likes'])}\n\n"
                f"#shorts #–Ω–æ–≤–æ—Å—Ç–∏"
            )
            
            with open(video_file_path, 'rb') as f:
                video_data = f.read()
            
            video_file = BufferedInputFile(
                video_data, 
                filename=f"{short_video['id']}.mp4"
            )
            
            await bot.send_video(
                CHANNEL_ID,
                video=video_file,
                caption=caption,
                parse_mode=ParseMode.MARKDOWN,
                supports_streaming=True,
                width=1080,
                height=1920
            )
            
            save_youtube_posted(short_video['id'], 'shorts')
            # –í–ê–ñ–ù–û: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –∫–∞–Ω–∞–ª
            track_youtube_channel(short_video['channel'])
            log.info(f"‚úÖ YouTube Shorts –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω! (–∫–∞–Ω–∞–ª: {short_video['channel']})")
            
            os.remove(video_file_path)
            log.info(f"üóëÔ∏è –§–∞–π–ª —É–¥–∞–ª—ë–Ω: {video_file_path}")
            
            return True
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            
            if os.path.exists(video_file_path):
                os.remove(video_file_path)
                log.info(f"üóëÔ∏è –§–∞–π–ª —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏")
            
            continue
    
    log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–æ—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω Shorts –∏–∑ —Ç–æ–ø-15")
    return False

def cleanup_old_files():
    try:
        now = datetime.now().timestamp()
        for filename in os.listdir(TEMP_DIR):
            filepath = os.path.join(TEMP_DIR, filename)
            
            if os.path.isfile(filepath):
                file_age = now - os.path.getmtime(filepath)
                
                if file_age > 86400:
                    os.remove(filepath)
                    log.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {filename}")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

# ================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ==================
async def main():
    scheduler = AsyncIOScheduler(timezone=TIMEZONE)
    
    # YouTube Shorts - 3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å
    scheduler.add_job(post_youtube_shorts, "cron", hour=9, minute=0, name="shorts_morning")
    scheduler.add_job(post_youtube_shorts, "cron", hour=19, minute=0, name="shorts_evening")
    scheduler.add_job(post_youtube_shorts, "cron", hour=22, minute=0, name="shorts_night")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
    scheduler.add_job(cleanup_old_files, "cron", hour=3, minute=0)
    
    scheduler.start()
    
    log.info("=" * 70)
    log.info("ü§ñ –ù–û–í–û–°–¢–ù–û–ô –ë–û–¢ v2.0 - –£–õ–£–ß–®–ï–ù–ù–´–ô")
    log.info("=" * 70)
    log.info("üì∞ –ù–æ–≤–æ—Å—Ç–∏: –∫–∞–∂–¥—ã–µ 20-70 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å)")
    log.info("üé¨ YouTube Shorts: 3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å (9:00, 19:00, 22:00)")
    log.info("")
    log.info("üÜï –ß–¢–û –ù–û–í–û–ì–û:")
    log.info("   ‚úÖ –†–æ—Ç–∞—Ü–∏—è –∫–∞–Ω–∞–ª–æ–≤ YouTube (–º–∞–∫—Å 2 –≤–∏–¥–µ–æ/–∫–∞–Ω–∞–ª/–¥–µ–Ω—å)")
    log.info("   ‚úÖ –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç Shorts –ø–æ—Å—Ç–æ–≤ (–±–µ–∑ '–ì–ª–∞–≤–Ω—ã–π Short –¥–Ω—è')")
    log.info("   ‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω—ã)")
    log.info("   ‚úÖ –î–æ 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Unsplash –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    log.info("   ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤")
    log.info("=" * 70)
    
    await news_loop()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("üëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        conn.close()
    except Exception as e:
        log.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        conn.close()
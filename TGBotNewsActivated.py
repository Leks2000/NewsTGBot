import os
import asyncio
import feedparser
import aiohttp
import logging
import random
import sqlite3
import hashlib
import json
import re
import sys
from dotenv import load_dotenv
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.enums import ParseMode
from aiogram.types import BufferedInputFile
from apscheduler.schedulers.asyncio import AsyncIOScheduler

load_dotenv()

# ================== CONFIG ==================
BOT_TOKEN = os.getenv("BOT_TOKEN")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
PEXELS_API_KEY = os.getenv("PEXELS_API_KEY")

# –ö–ê–ù–ê–õ–´
CHANNEL_RU = '@bulmyash'
CHANNEL_EN = '@WORLD_ALERT_NEWS'

TIMEZONE = "Europe/Moscow"

if sys.platform == "win32":
    TEMP_DIR = "C:/temp/shorts"
else:
    TEMP_DIR = "/tmp/shorts"
os.makedirs(TEMP_DIR, exist_ok=True)

# ================== RSS –ò–°–¢–û–ß–ù–ò–ö–ò ==================
RSS_SOURCES_RU = {
    "rbc": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "tass": "https://tass.ru/rss/v2.xml",
    "interfax": "https://www.interfax.ru/rss.asp",
    "kommersant": "https://www.kommersant.ru/RSS/news.xml",
    "ria": "https://ria.ru/export/rss2/index.xml",
    "lenta": "https://lenta.ru/rss",
    "gazeta": "https://www.gazeta.ru/export/rss/first.xml",
    "vedomosti": "https://www.vedomosti.ru/rss/news",
    "izvestia": "https://iz.ru/xml/rss/all.xml",
    "rt_ru": "https://russian.rt.com/rss",
    "fontanka": "https://www.fontanka.ru/fontanka.rss",
    "rosbalt": "https://www.rosbalt.ru/feed/",
    "forbes_ru": "https://www.forbes.ru/newrss.xml",
    "cnews": "https://www.cnews.ru/inc/rss/news.xml",
    "habr": "https://habr.com/ru/rss/all/all/",
    "meduza": "https://meduza.io/rss/all",
}

RSS_SOURCES_EN = {
    "reuters": "https://feeds.reuters.com/reuters/worldNews",
    "bbc": "https://feeds.bbci.co.uk/news/world/rss.xml",
    "cnn": "http://rss.cnn.com/rss/edition_world.rss",
    "ap": "https://rsshub.app/apnews/topics/world-news",
    "guardian": "https://www.theguardian.com/world/rss",
    "nyt": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "aljazeera": "https://www.aljazeera.com/xml/rss/all.xml",
    "france24": "https://www.france24.com/en/rss",
    "dw": "https://rss.dw.com/rdf/rss-en-all",
    "rt_en": "https://www.rt.com/rss/news/",
    "politico": "https://www.politico.com/rss/politicopicks.xml",
    "thehill": "https://thehill.com/feed/",
    "npr": "https://feeds.npr.org/1001/rss.xml",
    "abc": "https://abcnews.go.com/abcnews/internationalheadlines",
    "sky": "https://feeds.skynews.com/feeds/rss/world.xml",
}

# ================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ==================
KEYWORDS_RU = [
    '–ø—É—Ç–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–∫—Ä–µ–º–ª', '–≥–æ—Å–¥—É–º', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', 
    '–º–∏–Ω–∏—Å—Ç—Ä', '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '–∑–µ–ª–µ–Ω—Å–∫', '—Å—à–∞', '–∫–∏—Ç–∞–π',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∫—É—Ä—Å', '—Ü–±', '–±–∞–Ω–∫', '–∏–Ω—Ñ–ª—è—Ü',
    '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—Å–∞–Ω–∫—Ü', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–∞—Ä–º–∏—è',
    '—É–¥–∞—Ä', '–æ–±—Å—Ç—Ä–µ–ª', '–∞—Ç–∞–∫', '–∞–≤–∞—Ä', '–ø–æ–∂–∞—Ä', '–≤–∑—Ä—ã–≤',
    '–ø–æ–≥–∏–±', '–∂–µ—Ä—Ç–≤', '–∑–∞–¥–µ—Ä–∂–∞', '–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–ø—Ä–∏–≥–æ–≤–æ—Ä',
    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–π—Ä–æ—Å–µ—Ç', 'chatgpt', 'google', 'apple',
    '—É—á–µ–Ω', '–∫–æ—Å–º–æ—Å', '–≤—ã–±–æ—Ä', '–∑–∞–∫–æ–Ω', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç'
]

KEYWORDS_EN = [
    'putin', 'kremlin', 'russia', 'president', 'government',
    'trump', 'biden', 'zelensky', 'usa', 'china', 'nato',
    'dollar', 'euro', 'stock', 'fed', 'inflation', 'economy',
    'oil', 'gas', 'sanctions', 'war', 'conflict', 'military',
    'attack', 'strike', 'explosion', 'fire', 'crash',
    'killed', 'death', 'arrest', 'court', 'verdict',
    'ai', 'chatgpt', 'google', 'apple', 'tesla', 'musk',
    'science', 'space', 'election', 'law', 'breaking'
]

# ================== –ß–Å–†–ù–´–ï –°–ü–ò–°–ö–ò ==================
BORING_KEYWORDS_RU = [
    '–ø–æ–≥–æ–¥–∞', '—Å–∏–Ω–æ–ø—Ç–∏–∫', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–æ—Å–∞–¥–∫', '–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã',
    '–≥–æ—Ä–æ—Å–∫–æ–ø', '–ª—É–Ω–Ω—ã–π', '—Å–æ–Ω–Ω–∏–∫', '–ø—Ä–∏–º–µ—Ç—ã', '–∏–º–µ–Ω–∏–Ω—ã',
    '—Å—Ç–∞–∂–∏—Ä–æ–≤–∫', '–æ–±–µ—Å–ø–µ—á–∏—Ç—å', '–ø–æ—Ä—É—á–∏–ª',
]

BORING_KEYWORDS_EN = [
    'weather', 'forecast', 'horoscope', 'zodiac', 'lottery',
    'celebrity', 'kardashian', 'royal family', 'recipe',
]

# –ß—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ YouTube
BLACKLIST_CHANNELS = [
    # –î–µ—Ç—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
    'kids', 'children', 'cartoon', 'animation', 'nursery',
    # –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç –∏ –∏–≥—Ä—ã
    'minecraft', '–º–∞–π–Ω–∫—Ä–∞—Ñ—Ç', 'roblox', 'fortnite', 'gaming', '–≥–µ–π–º–µ—Ä',
    # –ú—É—Å–æ—Ä
    'asmr', '–∞—Å–º—Ä', 'mukbang', '–º—É–∫–±–∞–Ω–≥', 'prank', '–ø—Ä–∞–Ω–∫',
    'tiktok compilation', 'shorts compilation',
]

# –ß—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–º
BLACKLIST_TOPICS = [
    '–º–∞–π–Ω–∫—Ä–∞—Ñ—Ç', 'minecraft', 'roblox', 'fortnite',
    'asmr', '–∞—Å–º—Ä', '–º—É–∫–±–∞–Ω–≥', 'mukbang',
    '–¥–µ—Ç—Å–∫–∏–π', 'kids', 'children', 'cartoon',
    '–ø—Ä–∞–Ω–∫', 'prank', '—á–µ–ª–ª–µ–Ω–¥–∂', 'challenge',
    '–≥–∞–¥–∞–Ω–∏–µ', 'tarot', '–∞—Å—Ç—Ä–æ–ª–æ–≥',
]

# ================== YOUTUBE –ö–ê–ù–ê–õ–´ ==================
RU_NEWS_CHANNELS = [
    "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏", "–¢–ê–°–°", "–ò–∑–≤–µ—Å—Ç–∏—è", "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å", "–†–ë–ö",
    "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä", "–í–µ–¥–æ–º–æ—Å—Ç–∏", "–ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª", "–†–æ—Å—Å–∏—è 24",
    "–ù–¢–í", "RT", "–î–ï–ù–¨ –¢–í", "–ö—Ä–µ–º–ª—å", 
    "–î–æ–∂–¥—å", "–ú–µ–¥—É–∑–∞", "–ù–æ–≤–∞—è –≥–∞–∑–µ—Ç–∞",
    "–≤–î—É–¥—å", "–ü–æ–ø—É–ª—è—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞", "–§–ï–ô–ì–ò–ù LIVE", 
    "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ", "–í—Ä–µ–º—è –ü—Ä—è–¥–∫–æ Shorts",
    "–†–µ–¥–∞–∫—Ü–∏—è", "Varlamov", "Varlamov News",
    "Soloviev LIVE", "–°–æ–ª–æ–≤—å—ë–≤ LIVE", "60 –º–∏–Ω—É—Ç",
    "–¶–∞—Ä—å–≥—Ä–∞–¥ –¢–í", "–°–ø—É—Ç–Ω–∏–∫", "Life", "–õ–∞–π—Ñ",
    "Mash", "Shot", "112", "Baza", "–ë–∞–∑–∞",
    "Readovka", "WarGonzo", "Rybar", "–†—ã–±–∞—Ä—å",
    "BRIEF", "–ù–µ–∑—ã–≥–∞—Ä—å", "–ü–æ–¥—ä—ë–º", "–ù–æ–≤–æ—Å—Ç–∏",
    "–ü–æ–ª–∏—Ç–∏–∫–∞ —Å–µ–≥–æ–¥–Ω—è", "–†–æ—Å—Å–∏—è 1", "–û–¢–†",
    "–≠—Ö–æ", "The Insider", "–í–∞–∂–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏",
]

EN_NEWS_CHANNELS = [
    "BBC News", "CNN", "Reuters", "Al Jazeera English",
    "Sky News", "NBC News", "ABC News", "CBS News",
    "Fox News", "MSNBC", "Bloomberg", "CNBC",
    "The Guardian", "The New York Times", "Washington Post",
    "AP Archive", "AFP News Agency", "DW News",
    "France 24 English", "Euronews", "WION",
    "Times Radio", "Channel 4 News", "ITV News",
    "Global News", "CTV News", "PBS NewsHour",
    "Vice News", "Vox", "The Economist",
]

# –†–ê–ó–í–õ–ï–ö–ê–¢–ï–õ–¨–ù–´–ï –ö–ê–ù–ê–õ–´ (RU)
RU_ENTERTAINMENT_CHANNELS = [
    # –Æ–º–æ—Ä/–ø—Ä–∏–∫–æ–ª—ã (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
    "–ß–ë–î", "Labelcom", "Stand-Up Club #1", "Roast Battle",
    "–ò–º–ø—Ä–æ–≤–∏–∑–∞—Ü–∏—è", "–ì–¥–µ –ª–æ–≥–∏–∫–∞", "–ß—Ç–æ –±—ã–ª–æ –¥–∞–ª—å—à–µ",
    # –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã/–Ω–∞—É–∫–∞
    "SciOne", "–ù–∞—É—á–ø–æ–∫", "–ê—Ä–∑–∞–º–∞—Å", "–ü—Ä–∞–≤–¥–∞ –ì–ª–∞–∑–∞ –ö–æ–ª–µ—Ç",
    "–¢–æ–ø–ª–µ—Å", "Utopia Show", "Droider", "Wylsacom",
    # –õ–∞–π—Ñ—Ö–∞–∫–∏/–ø–æ–ª–µ–∑–Ω–æ–µ
    "AdMe", "5-Minute Crafts LIKE", 
]

# –†–ê–ó–í–õ–ï–ö–ê–¢–ï–õ–¨–ù–´–ï –ö–ê–ù–ê–õ–´ (EN)
EN_ENTERTAINMENT_CHANNELS = [
    # Facts/Science
    "Veritasium", "Vsauce", "Kurzgesagt", "SmarterEveryDay",
    "Mark Rober", "Tom Scott", "CGP Grey",
    # Tech
    "MKBHD", "Linus Tech Tips", "JerryRigEverything",
    # Interesting
    "Johnny Harris", "Wendover Productions", "RealLifeLore",
    "Half as Interesting", "PolyMatter",
]

# ================== –ö–ê–¢–ï–ì–û–†–ò–ò –ö–û–ù–¢–ï–ù–¢–ê ==================
CONTENT_CATEGORIES = {
    "news": {
        "weight": 50,  # 50% –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        "queries_ru": [
            "–Ω–æ–≤–æ—Å—Ç–∏ —Ä–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è", "–ø—É—Ç–∏–Ω –∑–∞—è–≤–∏–ª", "—Ç—Ä–∞–º–ø –Ω–æ–≤–æ—Å—Ç–∏",
            "–º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏", "—Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏", "–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å",
        ],
        "queries_en": [
            "breaking news today", "world news", "trump news",
            "biden news", "russia news", "china news",
        ]
    },
    "politics": {
        "weight": 20,
        "queries_ru": [
            "–ø–æ–ª–∏—Ç–∏–∫–∞ —Ä–æ—Å—Å–∏—è", "–∫—Ä–µ–º–ª—å –Ω–æ–≤–æ—Å—Ç–∏", "–≥–æ—Å–¥—É–º–∞",
            "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è", "—Å–∞–Ω–∫—Ü–∏–∏",
        ],
        "queries_en": [
            "politics news", "white house", "congress",
            "european union", "nato news",
        ]
    },
    "economy": {
        "weight": 10,
        "queries_ru": [
            "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "—ç–∫–æ–Ω–æ–º–∏–∫–∞ —Ä–æ—Å—Å–∏–∏", "—Ä—É–±–ª—å —Å–µ–≥–æ–¥–Ω—è",
            "–Ω–µ—Ñ—Ç—å –≥–∞–∑", "–±–∏—Ä–∂–∞",
        ],
        "queries_en": [
            "stock market", "economy news", "bitcoin",
            "inflation", "fed rates",
        ]
    },
    "science_tech": {
        "weight": 10,
        "queries_ru": [
            "–Ω–∞—É–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏", "–∫–æ—Å–º–æ—Å",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        ],
        "queries_en": [
            "science news", "tech news", "ai news",
            "space news", "innovation",
        ]
    },
    "entertainment": {
        "weight": 10,
        "queries_ru": [
            "–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏", "—Ç–æ–ø —Ñ–∞–∫—Ç–æ–≤",
            "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ —Ä—è–¥–æ–º", "–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ–µ",
        ],
        "queries_en": [
            "amazing facts", "interesting facts", "mind blowing",
            "did you know", "incredible stories",
        ]
    },
}

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
DB_FILE = "news.db"
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

# –¢–∞–±–ª–∏—Ü—ã –¥–ª—è RU
c.execute('''CREATE TABLE IF NOT EXISTS posted_ru (
    hash TEXT UNIQUE, posted_at TEXT, title TEXT, url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted_ru (
    video_id TEXT UNIQUE, posted_at TEXT, type TEXT, category TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats_ru (
    date TEXT UNIQUE, news_count INT DEFAULT 0, shorts_count INT DEFAULT 0
)''')

# –¢–∞–±–ª–∏—Ü—ã –¥–ª—è EN
c.execute('''CREATE TABLE IF NOT EXISTS posted_en (
    hash TEXT UNIQUE, posted_at TEXT, title TEXT, url TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS youtube_posted_en (
    video_id TEXT UNIQUE, posted_at TEXT, type TEXT, category TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS daily_stats_en (
    date TEXT UNIQUE, news_count INT DEFAULT 0, shorts_count INT DEFAULT 0
)''')

# –û–±—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
c.execute('''CREATE TABLE IF NOT EXISTS youtube_channels_used (
    channel_name TEXT, used_at TEXT, lang TEXT
)''')
c.execute('''CREATE TABLE IF NOT EXISTS used_images (
    url TEXT, used_at TEXT
)''')

# –ê–ù–ê–õ–ò–¢–ò–ö–ê
c.execute('''CREATE TABLE IF NOT EXISTS analytics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT,
    lang TEXT,
    content_type TEXT,
    category TEXT,
    title TEXT,
    channel TEXT,
    views INT,
    likes INT,
    success BOOLEAN
)''')

conn.commit()

# ================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ==================
def get_today_stats(lang: str):
    today = datetime.now().date().isoformat()
    table = f"daily_stats_{lang}"
    c.execute(f"SELECT news_count, shorts_count FROM {table} WHERE date = ?", (today,))
    result = c.fetchone()
    return {"news": result[0], "shorts": result[1]} if result else {"news": 0, "shorts": 0}

def increment_stat(lang: str, stat_type: str):
    today = datetime.now().date().isoformat()
    table = f"daily_stats_{lang}"
    stats = get_today_stats(lang)
    
    if stat_type == "news":
        stats["news"] += 1
    else:
        stats["shorts"] += 1
    
    c.execute(f"INSERT OR REPLACE INTO {table} (date, news_count, shorts_count) VALUES (?, ?, ?)", 
              (today, stats["news"], stats["shorts"]))
    conn.commit()

def is_duplicate(title: str, url: str, lang: str):
    h = hashlib.md5((title + url).encode()).hexdigest()
    table = f"posted_{lang}"
    c.execute(f"SELECT 1 FROM {table} WHERE hash = ?", (h,))
    return c.fetchone() is not None

def save_posted(title: str, url: str, lang: str):
    h = hashlib.md5((title + url).encode()).hexdigest()
    table = f"posted_{lang}"
    c.execute(f"INSERT OR IGNORE INTO {table} (hash, posted_at, title, url) VALUES (?, ?, ?, ?)", 
              (h, datetime.now().isoformat(), title, url))
    conn.commit()

def is_youtube_posted(video_id: str, lang: str):
    table = f"youtube_posted_{lang}"
    c.execute(f"SELECT 1 FROM {table} WHERE video_id = ?", (video_id,))
    return c.fetchone() is not None

def save_youtube_posted(video_id: str, video_type: str, category: str, lang: str):
    table = f"youtube_posted_{lang}"
    c.execute(f"INSERT OR IGNORE INTO {table} (video_id, posted_at, type, category) VALUES (?, ?, ?, ?)", 
              (video_id, datetime.now().isoformat(), video_type, category))
    conn.commit()

def track_youtube_channel(channel_name: str, lang: str):
    c.execute("INSERT INTO youtube_channels_used (channel_name, used_at, lang) VALUES (?, ?, ?)", 
              (channel_name.lower(), datetime.now().isoformat(), lang))
    conn.commit()
    three_days_ago = (datetime.now() - timedelta(days=3)).isoformat()
    c.execute("DELETE FROM youtube_channels_used WHERE used_at < ?", (three_days_ago,))
    conn.commit()

def get_recent_channels(hours: int, lang: str) -> list:
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT DISTINCT channel_name FROM youtube_channels_used WHERE used_at > ? AND lang = ?", 
              (cutoff, lang))
    return [row[0] for row in c.fetchall()]

def get_channel_usage_count(channel_name: str, hours: int, lang: str) -> int:
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    c.execute("SELECT COUNT(*) FROM youtube_channels_used WHERE channel_name = ? AND used_at > ? AND lang = ?", 
              (channel_name.lower(), cutoff, lang))
    result = c.fetchone()
    return result[0] if result else 0

def track_used_image(url: str):
    c.execute("INSERT INTO used_images (url, used_at) VALUES (?, ?)", 
              (url, datetime.now().isoformat()))
    conn.commit()
    week_ago = (datetime.now() - timedelta(days=7)).isoformat()
    c.execute("DELETE FROM used_images WHERE used_at < ?", (week_ago,))
    conn.commit()

# –ê–ù–ê–õ–ò–¢–ò–ö–ê
def log_analytics(lang: str, content_type: str, category: str, title: str, 
                  channel: str = "", views: int = 0, likes: int = 0, success: bool = True):
    c.execute("""INSERT INTO analytics 
                 (timestamp, lang, content_type, category, title, channel, views, likes, success) 
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
              (datetime.now().isoformat(), lang, content_type, category, 
               title[:200], channel[:100], views, likes, success))
    conn.commit()

def get_analytics_summary(days: int = 7):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∑–∞ N –¥–Ω–µ–π"""
    cutoff = (datetime.now() - timedelta(days=days)).isoformat()
    
    summary = {}
    
    # –ü–æ —è–∑—ã–∫–∞–º
    c.execute("""SELECT lang, COUNT(*), SUM(CASE WHEN success THEN 1 ELSE 0 END) 
                 FROM analytics WHERE timestamp > ? GROUP BY lang""", (cutoff,))
    summary["by_lang"] = {row[0]: {"total": row[1], "success": row[2]} for row in c.fetchall()}
    
    # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    c.execute("""SELECT category, COUNT(*), AVG(views) 
                 FROM analytics WHERE timestamp > ? AND content_type = 'shorts' 
                 GROUP BY category ORDER BY COUNT(*) DESC""", (cutoff,))
    summary["by_category"] = {row[0]: {"count": row[1], "avg_views": row[2]} for row in c.fetchall()}
    
    # –¢–æ–ø –∫–∞–Ω–∞–ª—ã
    c.execute("""SELECT channel, COUNT(*), AVG(views) 
                 FROM analytics WHERE timestamp > ? AND channel != '' 
                 GROUP BY channel ORDER BY COUNT(*) DESC LIMIT 10""", (cutoff,))
    summary["top_channels"] = [(row[0], row[1], row[2]) for row in c.fetchall()]
    
    return summary

# ================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ==================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger("news_bot")
bot = Bot(BOT_TOKEN)

# ================== AI HELPER ==================
async def ask_ai(prompt: str, temperature=0.7) -> str:
    if not OPENROUTER_API_KEY:
        return None
    
    models = [
        "nousresearch/hermes-3-llama-3.1-405b:free",
        "meta-llama/llama-3.1-8b-instruct:free",
        "google/gemma-2-9b-it:free",
    ]
    
    for model in models:
        try:
            async with aiohttp.ClientSession() as s:
                headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 800
                }
                async with s.post("https://openrouter.ai/api/v1/chat/completions",
                                 headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as r:
                    if r.status == 200:
                        data = await r.json()
                        return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
            log.debug(f"AI error ({model}): {e}")
            continue
    
    return None

# ================== –ü–†–û–í–ï–†–ö–ê –ö–û–ù–¢–ï–ù–¢–ê ==================
def has_cyrillic(text):
    return bool(re.search('[–∞-—è–ê-–Ø—ë–Å]', text))

def has_ukrainian(text):
    return any(l in text for l in ['—î', '—ñ', '—ó', '“ë', '–Ñ', '–Ü', '–á', '“ê'])

def is_russian_content(title: str, channel: str, description: str = "") -> bool:
    full_text = f"{title} {channel} {description}".lower()
    
    if not has_cyrillic(title + channel):
        return False
    
    if has_ukrainian(title + channel + description):
        return False
    
    ua_keywords = ['—É–∫—Ä–∞—ó–Ω', 'ukrainian', '–∫–∏—ó–≤', '–∑–µ–ª–µ–Ω—Å—å–∫', '–∞–∑–æ–≤', '–≤—Å—É', '–∑—Å—É']
    if any(kw in full_text for kw in ua_keywords):
        return False
    
    return True

def is_english_content(title: str, channel: str, description: str = "") -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
    full_text = f"{title} {channel} {description}"
    
    # –ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    if has_cyrillic(full_text):
        return False
    
    # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –±—É–∫–≤—ã
    if not re.search('[a-zA-Z]', title):
        return False
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –∏—Å–ø–∞–Ω—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π –∏ —Ç.–¥. –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º
    non_english = ['√±', '√ß', '√£', '√µ', '√º', '√∂', '√§', '√ü']
    if any(char in full_text.lower() for char in non_english):
        return False
    
    return True

def is_blacklisted(title: str, channel: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫"""
    text = f"{title} {channel}".lower()
    
    for banned in BLACKLIST_CHANNELS:
        if banned.lower() in text:
            return True
    
    for banned in BLACKLIST_TOPICS:
        if banned.lower() in text:
            return True
    
    return False

def is_trusted_channel(channel: str, lang: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –∫–∞–Ω–∞–ª"""
    channels = RU_NEWS_CHANNELS + RU_ENTERTAINMENT_CHANNELS if lang == "ru" else EN_NEWS_CHANNELS + EN_ENTERTAINMENT_CHANNELS
    return any(t.lower() in channel.lower() for t in channels)

# ================== –í–´–ë–û–† –ö–ê–¢–ï–ì–û–†–ò–ò ==================
def select_category_by_time() -> str:
    """–í—ã–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É—á—ë—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫"""
    hour = datetime.now().hour
    
    # –£—Ç—Ä–æ (6-10) - –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    if 6 <= hour < 10:
        weights = {"news": 60, "politics": 20, "economy": 10, "science_tech": 5, "entertainment": 5}
    # –î–µ–Ω—å (10-18) - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    elif 10 <= hour < 18:
        weights = {"news": 40, "politics": 20, "economy": 15, "science_tech": 15, "entertainment": 10}
    # –í–µ—á–µ—Ä (18-23) - –±–æ–ª—å—à–µ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π
    elif 18 <= hour < 23:
        weights = {"news": 30, "politics": 15, "economy": 10, "science_tech": 20, "entertainment": 25}
    # –ù–æ—á—å (23-6) - –ª—ë–≥–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
    else:
        weights = {"news": 25, "politics": 10, "economy": 5, "science_tech": 25, "entertainment": 35}
    
    categories = list(weights.keys())
    probs = list(weights.values())
    
    return random.choices(categories, weights=probs, k=1)[0]

# ================== –°–ë–û–† –ù–û–í–û–°–¢–ï–ô ==================
async def collect_fresh_news(lang: str, limit=30):
    candidates = []
    sources = RSS_SOURCES_RU if lang == "ru" else RSS_SOURCES_EN
    keywords = KEYWORDS_RU if lang == "ru" else KEYWORDS_EN
    boring = BORING_KEYWORDS_RU if lang == "ru" else BORING_KEYWORDS_EN
    
    sources_list = list(sources.items())
    random.shuffle(sources_list)
    
    for source_name, rss_url in sources_list:
        if len(candidates) >= limit:
            break
        
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                if len(candidates) >= limit:
                    break
                
                title = BeautifulSoup(entry.title.strip(), "html.parser").get_text()
                url = entry.link
                desc = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser").get_text()
                
                # –ü–∞—Ä—Å–∏–º –∫–∞—Ä—Ç–∏–Ω–∫—É
                rss_image = None
                if hasattr(entry, 'media_content') and entry.media_content:
                    rss_image = entry.media_content[0].get('url')
                
                if not rss_image and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            rss_image = enc.get('href')
                            break
                
                if not rss_image:
                    soup = BeautifulSoup(entry.get("summary", "") or entry.get("description", ""), "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        rss_image = img_tag['src']
                
                if rss_image and (not rss_image.startswith('http') or len(rss_image) < 30):
                    rss_image = None
                
                if len(title) < 20:
                    continue
                if is_duplicate(title, url, lang):
                    continue
                if any(b in title.lower() for b in boring):
                    continue
                if not any(k in title.lower() for k in keywords):
                    continue
                
                candidates.append({
                    "title": title,
                    "url": url,
                    "desc": desc,
                    "source": source_name,
                    "rss_image": rss_image
                })
                
        except Exception as e:
            log.error(f"RSS {source_name}: {e}")
    
    return candidates

# ================== AI: –í–´–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê ==================
async def ai_select_and_summarize(news_list: list, lang: str) -> dict:
    news_text = "\n".join([f"{i+1}. {n['title']}" for i, n in enumerate(news_list[:25])])
    
    if lang == "ru":
        prompt = f"""–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –î–ï–†–ó–ö–û–ì–û –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞.
–í—ã–±–µ—Ä–∏ –û–î–ù–£ —Å–∞–º—É—é –≤–∑—Ä—ã–≤–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ —Å–¥–µ–ª–∞–π —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑.

–í–ê–ñ–ù–û:
1. –í—ã–±–∏—Ä–∞–π –ì–û–†–Ø–ß–ò–ï –Ω–æ–≤–æ—Å—Ç–∏ (–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã, –¥–µ–Ω—å–≥–∏, –≤–∑—Ä—ã–≤—ã, —Å–∫–∞–Ω–¥–∞–ª—ã)
2. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ö–û–†–û–¢–ö–ò–ô (–º–∞–∫—Å 60 —Å–∏–º–≤–æ–ª–æ–≤)
3. –ü–µ—Ä–µ—Å–∫–∞–∑ –î–û–ü–û–õ–ù–Ø–ï–¢ –∑–∞–≥–æ–ª–æ–≤–æ–∫
4. –ù–ï –í–´–ë–ò–†–ê–ô —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Å–∫—É—á–Ω—É—é —Ö—É–π–Ω—é!

–•–ï–®–¢–ï–ì–ò - –æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞, –º–∞–∫—Å–∏–º—É–º 4, —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª.

–í–µ—Ä–Ω–∏ JSON:
{{
  "selected": –Ω–æ–º–µ—Ä (1-{len(news_list[:25])}),
  "title": "–ö–û–†–û–¢–ö–ò–ô –∑–∞–≥–æ–ª–æ–≤–æ–∫",
  "summary": "–ü–µ—Ä–µ—Å–∫–∞–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "hashtags": "#–°–ª–æ–≤–æ1 #–°–ª–æ–≤–æ2 #–°–ª–æ–≤–æ3"
}}

–ù–æ–≤–æ—Å—Ç–∏:
{news_text}"""
    else:
        prompt = f"""You are an editor of a BOLD news Telegram channel.
Pick ONE most explosive news and write a catchy summary.

IMPORTANT:
1. Choose HOT news (conflicts, money, explosions, scandals)
2. Title MUST be SHORT (max 60 chars)
3. Summary COMPLEMENTS the title
4. NO boring philosophical stuff!

HASHTAGS - single words, max 4, space-separated.

Return JSON:
{{
  "selected": number (1-{len(news_list[:25])}),
  "title": "SHORT catchy title",
  "summary": "Summary 2-3 sentences",
  "hashtags": "#Word1 #Word2 #Word3"
}}

News:
{news_text}"""
    
    response = await ask_ai(prompt, temperature=0.9)
    
    if response:
        try:
            json_start = response.find('{')
            json_end = response.rfind('}')
            if json_start != -1 and json_end != -1:
                content = response[json_start:json_end+1]
                result = json.loads(content)
                selected_idx = int(result.get("selected", 1)) - 1
                
                if 0 <= selected_idx < len(news_list):
                    selected = news_list[selected_idx]
                    selected["ai_title"] = result.get("title", selected["title"])
                    selected["summary"] = result.get("summary", "")
                    selected["hashtags"] = fix_hashtags(result.get("hashtags", ""))
                    return selected
        except Exception as e:
            log.warning(f"AI parse error: {e}")
    
    # Fallback
    selected = random.choice(news_list[:5])
    selected["ai_title"] = selected["title"]
    selected["summary"] = selected["desc"][:200] if selected["desc"] else ""
    selected["hashtags"] = generate_smart_hashtags(selected["title"], selected["desc"], lang)
    return selected

def fix_hashtags(raw_hashtags: str) -> str:
    raw_hashtags = re.sub(r'@\w+', '', raw_hashtags).strip()
    tags = re.findall(r'#\w+', raw_hashtags)
    
    fixed_tags = []
    for tag in tags:
        word = tag[1:]
        parts = re.findall(r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]*|[–∞-—è—ëa-z]+|[A-Z][a-z]*|[a-z]+', word)
        
        if len(parts) > 1 and len(word) > 12:
            for part in parts:
                if len(part) > 2:
                    fixed_tags.append(f"#{part}")
        else:
            fixed_tags.append(tag)
    
    seen = set()
    unique = []
    for tag in fixed_tags:
        if tag.lower() not in seen:
            seen.add(tag.lower())
            unique.append(tag)
    
    return ' '.join(unique[:4])

def generate_smart_hashtags(title: str, description: str, lang: str) -> str:
    text = f"{title} {description}".lower()
    tags = []
    
    if lang == "ru":
        if '–ø—É—Ç–∏–Ω' in text: tags.append('#–ü—É—Ç–∏–Ω')
        if '—Ç—Ä–∞–º–ø' in text: tags.append('#–¢—Ä–∞–º–ø')
        if '–±–∞–π–¥–µ–Ω' in text: tags.append('#–ë–∞–π–¥–µ–Ω')
        if '—Å—à–∞' in text: tags.append('#–°–®–ê')
        if '—Ä–æ—Å—Å–∏' in text: tags.append('#–†–æ—Å—Å–∏—è')
        if '—É–∫—Ä–∞–∏–Ω' in text: tags.append('#–£–∫—Ä–∞–∏–Ω–∞')
        if '–¥–æ–ª–ª–∞—Ä' in text or '—Ä—É–±–ª—å' in text: tags.append('#–ö—É—Ä—Å')
        if '–≤–æ–π–Ω–∞' in text: tags.append('#–í–æ–π–Ω–∞')
        if not tags: tags.append('#–ù–æ–≤–æ—Å—Ç–∏')
    else:
        if 'putin' in text: tags.append('#Putin')
        if 'trump' in text: tags.append('#Trump')
        if 'biden' in text: tags.append('#Biden')
        if 'russia' in text: tags.append('#Russia')
        if 'usa' in text or 'america' in text: tags.append('#USA')
        if 'ukraine' in text: tags.append('#Ukraine')
        if 'war' in text: tags.append('#War')
        if not tags: tags.append('#News')
    
    return ' '.join(tags[:4])

# ================== –ö–ê–†–¢–ò–ù–ö–ò ==================
PERSON_SEARCH_QUERIES = {
    '—Ç—Ä–∞–º–ø': ['donald trump', 'trump president'],
    '–ø—É—Ç–∏–Ω': ['vladimir putin', 'putin russia'],
    '–±–∞–π–¥–µ–Ω': ['joe biden', 'biden president'],
    'trump': ['donald trump', 'trump president'],
    'putin': ['vladimir putin', 'putin russia'],
    'biden': ['joe biden', 'biden president'],
}

async def get_perfect_image(title: str, description: str, rss_image: str = None) -> str:
    text_lower = f"{title} {description}".lower()
    
    # –ü–µ—Ä—Å–æ–Ω—ã
    queries = []
    for person, person_queries in PERSON_SEARCH_QUERIES.items():
        if person in text_lower:
            queries.extend(person_queries[:2])
            break
    
    if not queries:
        queries = ['world news', 'breaking news', 'politics']
    
    all_images = []
    
    for query in queries[:2]:
        images = await search_unsplash(query, count=10)
        all_images.extend(images)
        await asyncio.sleep(0.3)
    
    if rss_image:
        img_data = await download_image(rss_image)
        if img_data and len(img_data) > 5000:
            all_images.insert(0, {"url": rss_image, "source": "rss"})
    
    if all_images:
        img_url = all_images[0]["url"]
        track_used_image(img_url)
        return img_url
    
    return None

async def search_unsplash(query: str, count=10) -> list:
    if not UNSPLASH_ACCESS_KEY:
        return []
    
    try:
        url = "https://api.unsplash.com/search/photos"
        params = {"query": query, "per_page": count, "orientation": "landscape"}
        headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
        
        async with aiohttp.ClientSession() as s:
            async with s.get(url, params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    data = await r.json()
                    return [{"url": p["urls"]["regular"], "source": "unsplash"} 
                            for p in data.get("results", [])[:count]]
    except:
        pass
    return []

async def download_image(url: str):
    try:
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as s:
            async with s.get(url, timeout=aiohttp.ClientTimeout(total=10)) as r:
                if r.status == 200:
                    return await r.read()
    except:
        pass
    return None

# ================== –ü–û–°–¢–ò–ù–ì –ù–û–í–û–°–¢–ï–ô ==================
async def post_news(news: dict, lang: str):
    channel = CHANNEL_RU if lang == "ru" else CHANNEL_EN
    title = news.get("ai_title", news["title"])
    summary = news.get("summary", "")
    hashtags = news.get("hashtags", "")
    
    caption = f"**{title}**\n\n{summary}\n\n{hashtags}"
    
    img_url = await get_perfect_image(title, news.get("desc", ""), news.get("rss_image"))
    
    if not img_url:
        log.warning(f"[{lang.upper()}] –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    img_data = await download_image(img_url)
    
    if img_data and len(img_data) > 1024:
        try:
            file = BufferedInputFile(img_data, filename="news.jpg")
            await bot.send_photo(channel, file, caption=caption, parse_mode=ParseMode.MARKDOWN)
            save_posted(news["title"], news["url"], lang)
            increment_stat(lang, "news")
            log_analytics(lang, "news", "news", title, success=True)
            log.info(f"‚úÖ [{lang.upper()}] –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}")
            return True
        except Exception as e:
            log.error(f"‚ùå [{lang.upper()}] –û—à–∏–±–∫–∞: {e}")
            log_analytics(lang, "news", "news", title, success=False)
    
    return False

async def check_news(lang: str):
    stats = get_today_stats(lang)
    if stats["news"] >= 25:
        log.info(f"[{lang.upper()}] –õ–∏–º–∏—Ç 25 –Ω–æ–≤–æ—Å—Ç–µ–π")
        return
    
    log.info(f"[{lang.upper()}] –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏...")
    candidates = await collect_fresh_news(lang, 30)
    
    if not candidates:
        log.info(f"[{lang.upper()}] –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç")
        return
    
    selected = await ai_select_and_summarize(candidates, lang)
    if selected:
        await post_news(selected, lang)

# ================== YOUTUBE SHORTS ==================
def parse_duration_to_seconds(iso_duration):
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, iso_duration)
    if not match:
        return 0
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    return hours * 3600 + minutes * 60 + seconds

def format_views(views):
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}M" 
    elif views >= 1_000:
        return f"{views / 1_000:.1f}K"
    return str(views)

async def search_shorts(lang: str, category: str):
    """–ü–æ–∏—Å–∫ Shorts —Å —É—á—ë—Ç–æ–º —è–∑—ã–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    log.info(f"üîç [{lang.upper()}] –ü–æ–∏—Å–∫ Shorts, –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    
    recent_channels = get_recent_channels(12, lang)
    all_shorts = []
    
    # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —è–∑—ã–∫—É
    cat_data = CONTENT_CATEGORIES.get(category, CONTENT_CATEGORIES["news"])
    queries = cat_data["queries_ru"] if lang == "ru" else cat_data["queries_en"]
    
    random.shuffle(queries)
    
    for query in queries[:5]:
        try:
            url = "https://www.googleapis.com/youtube/v3/search"
            params = {
                "part": "id,snippet",
                "q": query + " shorts",
                "type": "video",
                "maxResults": 50,
                "order": "date",
                "publishedAfter": (datetime.now() - timedelta(days=3)).isoformat() + "Z",
                "regionCode": "RU" if lang == "ru" else "US",
                "relevanceLanguage": lang,
                "key": YOUTUBE_API_KEY
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status != 200:
                        continue
                    
                    data = await response.json()
                    video_ids = [item["id"]["videoId"] for item in data.get("items", []) 
                                if item["id"].get("kind") == "youtube#video"]
                    
                    if not video_ids:
                        continue
                    
                    # –î–µ—Ç–∞–ª–∏ –≤–∏–¥–µ–æ
                    details_url = "https://www.googleapis.com/youtube/v3/videos"
                    details_params = {
                        "part": "snippet,statistics,contentDetails",
                        "id": ",".join(video_ids[:50]),
                        "key": YOUTUBE_API_KEY
                    }
                    
                    async with session.get(details_url, params=details_params, timeout=15) as resp:
                        if resp.status != 200:
                            continue
                        
                        details_data = await resp.json()
                        
                        for item in details_data.get("items", []):
                            try:
                                duration = item["contentDetails"]["duration"]
                                total_sec = parse_duration_to_seconds(duration)
                                
                                if not (8 <= total_sec <= 65):
                                    continue
                                
                                snippet = item["snippet"]
                                stats = item["statistics"]
                                
                                title = snippet.get("title", "")
                                channel = snippet.get("channelTitle", "")
                                description = snippet.get("description", "")
                                
                                # –ü—Ä–æ–≤–µ—Ä–∫–∏
                                if is_blacklisted(title, channel):
                                    continue
                                
                                if channel.lower() in recent_channels:
                                    continue
                                
                                if get_channel_usage_count(channel, 24, lang) >= 2:
                                    continue
                                
                                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–∑—ã–∫–∞
                                if lang == "ru" and not is_russian_content(title, channel, description):
                                    continue
                                if lang == "en" and not is_english_content(title, channel, description):
                                    continue
                                
                                views = int(stats.get("viewCount", 0))
                                min_views = 1000 if is_trusted_channel(channel, lang) else 3000
                                if views < min_views:
                                    continue
                                
                                all_shorts.append({
                                    "id": item["id"],
                                    "title": title,
                                    "channel": channel,
                                    "views": views,
                                    "likes": int(stats.get("likeCount", 0)),
                                    "duration_sec": total_sec,
                                    "is_trusted": is_trusted_channel(channel, lang),
                                    "category": category
                                })
                                
                            except:
                                continue
            
            await asyncio.sleep(0.4)
            
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            continue
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    seen_ids = set()
    unique = []
    for s in all_shorts:
        if s["id"] not in seen_ids:
            seen_ids.add(s["id"])
            unique.append(s)
    
    unique.sort(key=lambda x: (get_channel_usage_count(x["channel"], 48, lang), 
                               not x["is_trusted"], -x["views"]))
    
    log.info(f"‚úÖ [{lang.upper()}] –ù–∞–π–¥–µ–Ω–æ {len(unique)} Shorts")
    return unique

async def download_shorts_video(video_id: str):
    output_file = os.path.join(TEMP_DIR, f"shorts_{video_id}.mp4")
    
    try:
        url = f"https://www.youtube.com/watch?v={video_id}"
        
        cmd = [
            sys.executable, "-m", "yt_dlp",
            "-f", "bv*+ba/b",
            "-o", output_file,
            "--no-playlist",
            "--merge-output-format", "mp4",
            "--extractor-args", "youtube:player_client=android",
            "--no-check-certificate",
            "--socket-timeout", "30",
            url
        ]
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        await asyncio.wait_for(process.communicate(), timeout=90)
        
        if process.returncode == 0 and os.path.exists(output_file):
            return output_file
            
    except Exception as e:
        log.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
    
    if os.path.exists(output_file):
        os.remove(output_file)
    return None

async def post_youtube_shorts(lang: str):
    """–ü–æ—Å—Ç–∏–Ω–≥ Shorts –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞"""
    stats = get_today_stats(lang)
    if stats["shorts"] >= 12:  # –£–≤–µ–ª–∏—á–∏–ª –ª–∏–º–∏—Ç
        log.info(f"[{lang.upper()}] –õ–∏–º–∏—Ç 12 shorts")
        return
    
    channel = CHANNEL_RU if lang == "ru" else CHANNEL_EN
    category = select_category_by_time()
    
    log.info(f"üé¨ [{lang.upper()}] –ó–∞–ø—É—Å–∫ Shorts, –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    
    shorts = await search_shorts(lang, category)
    
    if not shorts:
        log.warning(f"[{lang.upper()}] Shorts –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    for i, short in enumerate(shorts[:15], 1):
        if is_youtube_posted(short["id"], lang):
            continue
        
        log.info(f"[{lang.upper()}] [{i}/15] {short['title'][:50]}...")
        
        video_path = await download_shorts_video(short["id"])
        
        if not video_path:
            continue
        
        try:
            # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
            clean_title = short['title']
            clean_title = re.sub(r'#\S+', '', clean_title).strip()
            if '//' in clean_title:
                clean_title = clean_title.split('//')[0].strip()
            if '|' in clean_title:
                clean_title = clean_title.split('|')[0].strip()
            
            if lang == "ru":
                caption = (
                    f"‚ùó {clean_title}\n\n"
                    f"üì∫ {short['channel']}\n"
                    f"üëÄ {format_views(short['views'])} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤\n\n"
                    f"#shorts #{category}"
                )
            else:
                caption = (
                    f"‚ùó {clean_title}\n\n"
                    f"üì∫ {short['channel']}\n"
                    f"üëÄ {format_views(short['views'])} views\n\n"
                    f"#shorts #{category}"
                )
            
            with open(video_path, 'rb') as f:
                video_data = f.read()
            
            video_file = BufferedInputFile(video_data, filename=f"{short['id']}.mp4")
            
            await bot.send_video(
                channel,
                video=video_file,
                caption=caption,
                parse_mode=ParseMode.MARKDOWN,
                supports_streaming=True,
                width=1080,
                height=1920
            )
            
            save_youtube_posted(short['id'], 'shorts', category, lang)
            track_youtube_channel(short['channel'], lang)
            increment_stat(lang, "shorts")
            log_analytics(lang, "shorts", category, short['title'], 
                         short['channel'], short['views'], short['likes'], True)
            
            log.info(f"‚úÖ [{lang.upper()}] Shorts –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
            
            os.remove(video_path)
            return True
            
        except Exception as e:
            log.error(f"‚ùå [{lang.upper()}] –û—à–∏–±–∫–∞: {e}")
            log_analytics(lang, "shorts", category, short['title'], 
                         short['channel'], short['views'], short['likes'], False)
            
            if os.path.exists(video_path):
                os.remove(video_path)
            continue
    
    return False

# ================== –¶–ò–ö–õ–´ ==================
async def news_loop_ru():
    """–¶–∏–∫–ª –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è RU"""
    log.info("‚è∞ [RU] –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 5 —Å–µ–∫...")
    await asyncio.sleep(5)
    
    while True:
        await check_news("ru")
        interval = random.randint(20, 70)
        log.info(f"‚è∞ [RU] –°–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)

async def news_loop_en():
    """–¶–∏–∫–ª –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è EN"""
    log.info("‚è∞ [EN] –ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç —á–µ—Ä–µ–∑ 30 —Å–µ–∫...")
    await asyncio.sleep(30)
    
    while True:
        await check_news("en")
        interval = random.randint(25, 80)
        log.info(f"‚è∞ [EN] –°–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)

async def shorts_loop_ru():
    """–¶–∏–∫–ª Shorts –¥–ª—è RU - –∫–∞–∂–¥—ã–µ 1.5-2.5 —á–∞—Å–∞"""
    log.info("‚è∞ [RU] –ü–µ—Ä–≤—ã–π Shorts —á–µ—Ä–µ–∑ 2 –º–∏–Ω...")
    await asyncio.sleep(120)
    
    while True:
        await post_youtube_shorts("ru")
        interval = random.randint(90, 150)  # 1.5-2.5 —á–∞—Å–∞
        log.info(f"‚è∞ [RU] –°–ª–µ–¥—É—é—â–∏–π Shorts —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)

async def shorts_loop_en():
    """–¶–∏–∫–ª Shorts –¥–ª—è EN - –∫–∞–∂–¥—ã–µ 1.5-2.5 —á–∞—Å–∞"""
    log.info("‚è∞ [EN] –ü–µ—Ä–≤—ã–π Shorts —á–µ—Ä–µ–∑ 3 –º–∏–Ω...")
    await asyncio.sleep(180)
    
    while True:
        await post_youtube_shorts("en")
        interval = random.randint(90, 150)  # 1.5-2.5 —á–∞—Å–∞
        log.info(f"‚è∞ [EN] –°–ª–µ–¥—É—é—â–∏–π Shorts —á–µ—Ä–µ–∑ {interval} –º–∏–Ω")
        await asyncio.sleep(interval * 60)

def cleanup_old_files():
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        now = datetime.now().timestamp()
        for filename in os.listdir(TEMP_DIR):
            filepath = os.path.join(TEMP_DIR, filename)
            if os.path.isfile(filepath):
                if now - os.path.getmtime(filepath) > 86400:
                    os.remove(filepath)
                    log.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω: {filename}")
    except Exception as e:
        log.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

async def daily_analytics():
    """–ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞"""
    summary = get_analytics_summary(1)
    log.info("=" * 50)
    log.info("üìä –ê–ù–ê–õ–ò–¢–ò–ö–ê –ó–ê –î–ï–ù–¨:")
    log.info(f"–ü–æ —è–∑—ã–∫–∞–º: {summary['by_lang']}")
    log.info(f"–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {summary['by_category']}")
    log.info(f"–¢–æ–ø –∫–∞–Ω–∞–ª—ã: {summary['top_channels'][:5]}")
    log.info("=" * 50)

# ================== MAIN ==================
async def main():
    scheduler = AsyncIOScheduler(timezone=TIMEZONE)
    
    # –û—á–∏—Å—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    scheduler.add_job(cleanup_old_files, "cron", hour=3, minute=0)
    scheduler.add_job(daily_analytics, "cron", hour=23, minute=55)
    
    scheduler.start()
    
    log.info("=" * 70)
    log.info("ü§ñ –ù–û–í–û–°–¢–ù–û–ô –ë–û–¢ v3.0 - DUAL LANGUAGE")
    log.info("=" * 70)
    log.info(f"üì∞ RU –∫–∞–Ω–∞–ª: {CHANNEL_RU}")
    log.info(f"üåç EN –∫–∞–Ω–∞–ª: {CHANNEL_EN}")
    log.info("")
    log.info("üì∞ –ù–æ–≤–æ—Å—Ç–∏: –∫–∞–∂–¥—ã–µ 20-80 –º–∏–Ω (–º–∞–∫—Å 25/–¥–µ–Ω—å/–∫–∞–Ω–∞–ª)")
    log.info("üé¨ Shorts: –∫–∞–∂–¥—ã–µ 1.5-2.5 —á–∞—Å–∞ (–º–∞–∫—Å 12/–¥–µ–Ω—å/–∫–∞–Ω–∞–ª)")
    log.info("")
    log.info("üÜï –ß–¢–û –ù–û–í–û–ì–û:")
    log.info("   ‚úÖ –î–≤–∞ –∫–∞–Ω–∞–ª–∞ (RU + EN)")
    log.info("   ‚úÖ –ë–æ–ª—å—à–µ Shorts (–¥–æ 12/–¥–µ–Ω—å)")
    log.info("   ‚úÖ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    log.info("   ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫")
    log.info("   ‚úÖ –ß—ë—Ä–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∫–∞–Ω–∞–ª–æ–≤/—Ç–µ–º")
    log.info("   ‚úÖ –ê–Ω–∞–ª–∏—Ç–∏–∫–∞")
    log.info("=" * 70)
    
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    await asyncio.gather(
        news_loop_ru(),
        news_loop_en(),
        shorts_loop_ru(),
        shorts_loop_en(),
    )

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("üëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        conn.close()
    except Exception as e:
        log.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        conn.close()
